#!/bin/bash
export LD_LIBRARY_PATH=/usr/local/pgsql/lib

#--------------------------------------------------------------------
# sql_gen
# Syntax: ./sql_gen <server configuration file> <node> <new subscription, true/false> <subscription information file>
#--------------------------------------------------------------------

#--------------------------------------------------------------------
# 3-31-10 Fixed race condition, now updates the slony_parser.cfg file 
# right after running slony1_dump in the background.
#--------------------------------------------------------------------

#-------------------------------------------------------------------------
# syntax check
#-------------------------------------------------------------------------
if [ $# -eq 6 ]
then
#        config_file=$1
	node=$1
	new_site=$2
	archive=$3
	retention=$4
	tapegroup=$5
	input_file=$6
else
        #error="ERROR: Usage: $0 <server configuration file> <node> <new subscription, true/false> <subscription information file>"
        error="ERROR: Usage: $0 <node> <new subscription, true/false> <subscription information file>"
        echo $error
	exit 1
fi


logwrite "Starting $0 $1" nl

#------------------------------------------------------------------------
# Checks to see if the schema and tables exist on the webdb
#-------------------------------------------------------------------------
failure=0
logwrite "Setting failure to 0"

# Check to see if the schema "admin" and the table "admin.ns" exist
# check=`psql -p $SLAVEPORT -h $SLAVEHOST -U $REPUSER -c '\dt admin.*' $SLAVEDBNAME| grep admin.ns`	
	
#if [ "$check" == "" ]
#then
#	logwrite "Checking for drms_series in $pg_schema failed!" nl
#	failure=1
#	logwrite "Setting failure to 1"
#fi
#unset check

# remove all unsubscribe requests from input_file
cat $input_file | grep -v "unsubscribe" > $SMworkDir/new_input_file.txt
mv -f $SMworkDir/new_input_file.txt $input_file
## 
## see that input file now contains the PID of the process dealing with it
## i.e. #UPDATE PID:<pid>
## basically we are allowing comment lines in the file
##
exec < $input_file
	while read line
	do
		# skips blank lines
                if [ ${#line} -eq 0 ]
                then
                	continue
                fi

                # skips comment lines
                if [ -z "${line%%#*}" ]
                then
			continue
                fi

		set - $line

		#echo "line is [$line]" #remove
		oldIFS="$IFS"
		IFS="${IFS}."
		set - $1
		#echo "[$1] [$2] [$3]" #remove
		pg_schema=$1
		#echo "Schema is $pg_schema" #remove
		
		IFS="$oldIFS"
		logwrite "pg_schema is [$pg_schema]"
		logwrite "Executing [psql -p $SLAVEPORT -h $SLAVEHOST -U $REPUSER -c '\dt '$pg_schema'.*' $SLAVEDBNAME| grep drms_series]"
		check=`psql -p $SLAVEPORT -h $SLAVEHOST -U $REPUSER -c '\dt '$pg_schema'.*' $SLAVEDBNAME| grep drms_series`	
		logwrite "Result is [$check]" nl

		if [ "$check" == "" ]
		then
			logwrite "Checking for drms_series in $pg_schema failed!" nl
			failure=1
			logwrite "Setting failure to 1"
		fi

		unset check
		unset schema
	done

logwrite "Current failure status is [$failure]" nl
if [ $failure -ne 0 ]
then
	# there was a failure, report failure, cleanup and exit
	logwrite "Failure detected!"
	logwrite "Failure process: Removing $input_file"
	rm -f $input_file
	logwrite "Failure process: Removing $triggerdir/$node.subscribe_series.cfg"
	rm -f $triggerdir/$node.subscribe_series.cfg
	logwrite "Failure process: removing $subscribers_dir/$node.new" 
	rm -rf $subscribers_dir/$node.new
	logwrite "Failure process: removing $tables_dir/$node.new" 
	rm -f $tables_dir/$node.new.lst
	logwrite "Failure process: removing new entry from $parser_config"
	cat $parser_config | grep -v "$node.new.lst" > $parser_config.new
	mv -f $parser_config.new $parser_config
	logwrite "Failure process: Writing [failure] to $triggerdir/$node.subscribe_series.sqldone" 
	echo "failure in sqlgen. Couldn't connect to db or series doesn't exists in DB" > $triggerdir/$node.subscribe_series.sqldone
	exit 1
fi

#-------------------------------------------------------------------------
# Reads input file into a few arrays
# Array: schema_table includes <schema>.<name> (for use with createtabstrut script)
# Array: schema includes the <schema> (for use with createns script)
# Array: table includes the <table> (for use with the slony1_dump script)
# All arrays will match eachother via the counter number
#-------------------------------------------------------------------------
counter=0
exec < $input_file
	while read line
	do
		# skips blank lines
                if [ ${#line} -eq 0 ]
                then
                	continue
                fi

                # skips comment lines
                if [ -z "${line%%#*}" ]
                then
			continue
                fi

		set - $line
		
		sub=$2		
		#schema_table[$counter]=$1
		schema_table[$counter]=`echo $1 | tr '[A-Z]' '[a-z]'`
		oldIFS="$IFS"
		IFS="${IFS}."
		set - ${schema_table[$counter]}

		schema[$counter]=$1
		table[$counter]=$2

		IFS="$oldIFS"
                counter=$(( $counter + 1 ))
		unset sub
	done
logwrite "Schema list is [${schema[@]}]"
logwrite "Table list is [${table[@]}]"
#cp $input_file inputfilecheck.tmp #remove

#-------------------------------------------------------------------------
# Checks if there are any new subscriptions, if not, then skip createns, createtabstructure, and sdo_slony1_dump
#-------------------------------------------------------------------------
subcheck=`cat $input_file | grep -v "unsubscribe" | grep -vE "^#"`; ## remove unsubscribe lines and comment lines
if [ "$subcheck" == "" ]
then
	logwrite "No new subscriptions, skipping createns, createtabstructure and sdo_slony1_dump" nl
	logwrite "Creating dummy sql files"
	echo > $triggerdir/$node.subscribe_series.sql
else

#-------------------------------------------------------------------------
# Execute createns for each schema
#-------------------------------------------------------------------------
counter=0
while [ "$counter" -lt ${#schema[@]} ]
do
	nextcounter=$(( $counter + 1 ))
        
        # This will ensure that there are no duplicate schemas for which createns is called
	check=`echo "${schema[@]:0:$counter}" | grep ${schema[$counter]}`
	
	if [ "$check" == "" ]
	then
		# If first time finding this schema, execute ./createns
		logwrite "Executing [$kModDir/createns ns=${schema[$counter]} nsgroup=user dbusr=$REPUSER >> $triggerdir/$node.${schema[$counter]}.createns.sql]"
		#echo "Createns >>" #remove

		$kModDir/createns ns=${schema[$counter]} nsgroup=user dbusr=$REPUSER > $triggerdir/$node.${schema[$counter]}.createns.sql
		#echo "<<Createns" #remove
		# echo ${schema[$counter]} >> $triggerdir/$node.createns.sql
	fi
	counter=$(( $counter + 1 ))
done

#-------------------------------------------------------------------------
# Execute createtabstruct and slony1_dump
#-------------------------------------------------------------------------
counter=0
echo "BEGIN;" > $triggerdir/$node.subscribe_series.sql
logwrite "Schema_table array is [${schema_table[@]}] and has [${#schema_table[@]}] elements"
while [ "$counter" -lt ${#schema_table[@]} ]
do
	nextcounter=$(( $counter + 1 ))
	check=`echo "${schema_table[@]:0:$counter}" | grep ${schema_table[$counter]}`
	
	if [ "$check" == "" ]
	then
		# If first time finding this schema_table, execute ./createtabstruct
		logwrite "Executing [$kModDir/createtabstructure in=${schema_table[$counter]} out=${schema_table[$counter]} archive=$archive retention=$retention tapegroup=$tapegroup owner=slony >> $triggerdir/$node.subscribe_series.sql]"
		$kModDir/createtabstructure in=${schema_table[$counter]} out=${schema_table[$counter]} archive=$archive retention=$retention tapegroup=$tapegroup owner=slony >> $triggerdir/$node.subscribe_series.sql

    resp=$?

    if [ "$resp" == "5" ]
    then
        logwrite "ERROR: Unknown series ${schema_table[$counter]}"
        rm -f $input_file
        exit 1
    fi

    if [ "$resp" != "0" ]
        logwrite "ERROR: createtabstructure returned $resp"
        rm -f $input_file
        exit 1
	fi

	counter=$(( $counter + 1 ))
done
echo "new_site is $new_site"

# Set $new_site to binary boolean
if [ $new_site == "true" ]
then
	new_site_bool=1
else
	new_site_bool=0
fi

#-------------------------------------------------------------------------
# Execute sdo_slony1_dump.sh
#-------------------------------------------------------------------------
logwrite "Executing: [. $kRepDir/subscribe_manage/sdo_slony1_dump.sh $SLAVEDBNAME jsoc $SLAVEPORT $new_site_bool \"$SMworkDir/slon_counter.$node.txt\" ${schema_table[@]} >> $triggerdir/$node.subscribe_series.sql 2> $SMworkDir/slony1_dump.$node.log"

. $kRepDir/subscribe_manage/sdo_slony1_dump.sh $SLAVEDBNAME $CLUSTERNAME $SLAVEPORT $new_site_bool "$SMworkDir/slon_counter.$node.txt" ${schema_table[@]} >> $triggerdir/$node.subscribe_series.sql 2> $SMworkDir/slony1_dump.$node.log

#-------------------------------------------------------------------------
# Checking the status of slony1 dump
#-------------------------------------------------------------------------
logwrite "Checking [$triggerdir/$node.subscribe_series.sql for the [-- dump complete] flag"
unset check
check=`tail -5 $triggerdir/$node.subscribe_series.sql | grep "\-\- dump complete"`
#if dump complete doesn't exist then it means it failed.
if [ -n "$check" ]
then
  logwrite "Dump complete"
  echo "COMMIT;" >> $triggerdir/$node.subscribe_series.sql
else
  logwrite "ERROR: There was an error found from sdo_slony1_dump.sh!"
  logwrite "Review [$SMworkDir/slony1_dump.$node.log]"
  logwrite "Error is [$check]"
  logwrite "Exiting"
  rm -f $input_file
  exit 1
fi

rm -f $SMworkDir/slony1_dump.$node.log

#-------------------------------------------------------------------------
# End of check for new subscriptions
#-------------------------------------------------------------------------
fi

#-------------------------------------------------------------------------
# Tar up the sql files then remove them
#-------------------------------------------------------------------------
counter=0
filelist="$node.subscribe_series.sql"
while [ "$counter" -lt ${#schema[@]} ]
do
    filelist="$filelist $node.${schema[$counter]}.createns.sql"
    counter=$(( $counter + 1 ))
done

logwrite "Executing: tar -C $triggerdir/ -czvf $triggerdir/$node.sql.tar.gz $filelist"

# Don't put $filelist in dquotes! tar needs the file names separate
tar -C "$triggerdir/" -czvf "$triggerdir/$node.sql.tar.gz" $filelist

logwrite "Removing $node.*.createns.sql and $node.subscribe_series.sql"
rm -f "$triggerdir/$node."*".createns.sql" "$triggerdir/$node.subscribe_series.sql"

#-------------------------------------------------------------------------
# Renaming the subupdate file to sqldone file in trigger directory for subscriber to find
#-------------------------------------------------------------------------
logwrite "Renaming sqlgen file $input_file to $triggerdir/$node.subscribe_series.sqldone" nl

mv -f $input_file $triggerdir/$node.subscribe_series.sqldone

logwrite "Finished running $0"
