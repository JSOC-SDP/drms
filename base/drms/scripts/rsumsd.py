#!/usr/bin/env python3

import sys

if sys.version_info < (3, 2):
    raise Exception('You must run the 3.2 release, or a more recent release, of Python.')

import re
import os
import stat
import filecmp
import logging
import psycopg2
import threading
import fcntl
from datetime import datetime, timedelta, timezone
import urllib.request
import json
import signal
import time
from copy import deepcopy
import shutil
import psycopg2
import random
import argparse
import inspect
sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../../include'))
from drmsparams import DRMSParams
sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../../base/libs/py'))
from drmsCmdl import CmdlParser
from drmsLock import DrmsLock
from subprocess import check_output, check_call, CalledProcessError, Popen, PIPE, TimeoutExpired

# This script runs as a daemon at the site that has requested an SU that does not belong to the site. It is responsible for contacting
# the owning site and requesting the path to the desired SUs. The owning site must be running the rs.sh CGI to respond to the requesting
# site's request.

# There are three database tables: 1., a DRMS site table, 2., a request table, and 3., an SU table (sunum, starttime, status, errmsg)
# The site table (sitename, sitecode, baseurl) provides
# the information needed to query the providing site for the information needed to scp the requested SUs to the requesting site. The URL to the cgi
# program that provides scp information is formed by appending "rs.py" to baseurl. There are two parameters for this cgi: 1., "requestid", and 2., "sunums".
# During the initial request to the providing site, the requesting site will provide a requestid of "none", and a comma-separated list of SUNUMs
# in the sunums argument. Should the providing site have all the requested SUs online, then it will return the scp information needed to access those
# SUs, and a status of "complete". If, however, the SUs are not all online, the initial request will start an asynchronous tape-read of the offline SUs,
# returning a request ID that identifies the initial request, and a status of "pending". The requesting site must then poll for completion by
# periodically calling the cgi with a status request. To make a status request, the requestid argument contains the requestid returned by the
# inital CGI call, and the sunums argument contains "none". While the data are not ready, the status request returns a status of "pending". When
# the data are online, the status request returns a status of "complete".
#
# The request table (requestid, sunums, status) is populated by drms_storageunit.c. For all SUs that are offline and are not owned by the running
# DRMS/SUMS, the code inserts a record into the request table. The requestid is a UUID (within the running DRMS/SUMS) generated by a sequence table.
# The list of SUNUMs is put in sunums, and the initial status is set to 'N' (New request). drms_storageunit.c chunks such SUNUMs into
# manageable-sized requests. After inserting one or more such records into the request table, drms_storageunit.c then polls these records,
# waiting for the status to become 'C' (Complete request). After that happens, drms_storageunit.c then calls SUM_get() again on these SUs
# to obtain their paths. This daemon, rsumsd.py, periodically and reads all records in the request table. For each SU in each 'N' record
# (a single request, which could be requesting multiple SUs) the daemon first checks if the SU is already being processed. If so, then
# the SU table is not modified. The status of the record in the request table is set to 'P'. If the SU is not in the SU table, then
# this can mean one of two things. The daemon has never processed this SU, or the daemon has already processed this SU. When the daemon has completed
# processing an SU, it deletes the record for the SU from the SU table. If the latter is true, the daemon should not re-process the same SU. To distinguish
# between these two possibilities, the daemon first checks to see if the SU is already present in SUMS (it calls show_info -o sunum=SUNUM).
# If the SU is online, then the daemon does nothing. But if it is offline, then the daemon inserts a record for the SU into the SU table, and starts
# processing that SU. The status of that SU-table record is set to 'P', as is the status of the request record containing that SUNUM.
#
# For each SU in each 'P' request record, the daemon searches for records in the SU table. If one or more such records exist in the SU table, then
# the request record is left in the pending state. The next time the daemon scans the request records, it will again check the SU table looking
# for completion of all SUs. When that occurs, the status of the request record is set to 'C', indicating that the request is complete. The
# drms_storageunit.c code will then call SUMS to get the newly created paths to the requested SUs.

# Locks and thread synchronization
# The main thread looks for new and pending requests in the requests table. Since there is only a single thread accessing the requests
# table, there is no need to lock the requests table or its individual requests.
#
# The Storage Unit status, on the other hand, is accessed by both the Downloader threads and the ScpWorker threads. The latter queries the
# former for 'working' SUs - these are SUs that a Downloader thread has requested that an ScpWorker thread download. Each SU is assigned to a 
# single ScpWorker. Care must be taken so that two or more ScpWorkers do not operate on the same SU. The change of an SU's status from
# W to D by the ScpWorker thread must not be interrupted by another ScpWorker's call to getNextNWorking(). If an ScpWorker calls
# getNextNWorking() (which returns SUs with a status of W), but does not change the SUs statuses to D before a second ScpWorker
# calls getNextNWorking(), then both ScpWorkers could get assigned the same SU. The solution is for each ScpWorker to lock each SU
# before it checks its status for W, then change the status to D, then release the SU lock. To facilitate this locking, each ScpWorker
# thread first locks the SU Table, then it iterates through all SUs, locking each one, then checking its status. Once it collects
# all SUs with a status of W, it releases the SU Table lock, but it continues to hold on to the SU locks so it can change the statuses
# from W to D without interruption by the Downloader thread (see next paragraph).

# Both the Downloader and ScpWorker can change the status of an SU. Each thread must ensure that it does not overwrite a status change made
# by the other thread. If an ScpWorker sees an SU with a status of W, it will set the status to D before starting an scp. In the meantime, 
# the Downloader could set the status to S to signify a that the worker should be stopped. Without locks, the change to S by the 
# Downloader could be overwritten by the change to D by the ScpWorker. The solution is for both threads to acquire the SU lock, then 
# read the status before changing the status. So the ScpWorker thread will acquire the lock, see the W, set the status to D and then 
# start the scp and release the SU lock. The Downloader will acquire the SU lock before changing the status to S. In this manner, 
# the Downloader's change to S cannot be overwritten by the ScpWorker's change of status from W to D.

# SU status life cycle: 
#   main thread sets to 'P' (in processSUs()) - pending
#   Downloader sets to 'W' - waiting (to be assigned to an ScpWorker)
#   ScpWorker sets to 'D' - downloading
#   Scpworker sets to 'P' - done downloading, back to Downloader
#   Downloader sets to 'C' - complete
#   If an error or time-out happens anywhere in this life cycle, then the status is set to 'E'

RET_SUCCESS = 0
RET_UNKNOWN_ERROR = 1
RET_UNKOWN_DRMSPARAMETER = 2
RET_INVALID_REMOTE_SUMS_ARGUMENT = 3
RET_INVALID_ARGUMENT = 4
RET_SU_NOT_REFERENCED = 5
RET_WORKER_REFERENCE_ERROR = 6
RET_UNABLE_TO_ACQUIRE_LOCK = 7
RET_UNABLE_TO_LOCK_OBJECT = 8
RET_UNABLE_TO_READ_SUTABLE = 9
RET_UNABLE_TO_WRITE_SUTABLE = 10
RET_UNABLE_TO_READ_REQTABLE = 11
RET_UNABLE_TO_WRITE_REQTABLE = 12
RET_UNABLE_TO_READ_SITETABLE = 13
RET_DUPLICATE_SUNUM = 14
RET_UNKNOWN_SUNUM = 15
RET_ERROR_CALLING_SUMS_API = 16
RET_UNKNOWN_REQUESTID = 17
RET_UNABLE_TO_GET_RETENTION = 18
RET_INVALID_SUNUM = 19
RET_UNKNOWN_SITE_CODE = 20
RET_UNABLE_TO_DOWNLOAD_SU = 21
RET_SHUTDOWN_REQUESTED = 22
RET_NO_SUS_TO_DOWNLOAD = 23
RET_DOWNLOADER = 24
RET_SUPATH_CGI = 25
RET_PROVIDER_POLLER = 26
RET_UNABLE_TO_START_THREAD = 27
RET_TOO_MANY_THREADS = 28
RET_UNABLE_TO_CONNECT_TO_DB = 29
RET_USER_TERMINATED = 30

LOG_FILE_BASE_NAME = 'rslog'

SUM_MAIN = 'public.sum_main'
SUM_PARTN_ALLOC = 'public.sum_partn_alloc'

def terminator(signo, frame):
    # Raise the SystemExit exception (which will be caught by the __exit__() method below).
    sys.exit(0)

class TerminationHandler(object):
    def __new__(cls, thContainer):
        return super(TerminationHandler, cls).__new__(cls)

    def __init__(self, thContainer):
        self.container = thContainer
        arguments = thContainer[0]
        self.pidStr = thContainer[1]
        self.log = thContainer[2]
        
        self.lockFile = arguments.lockfile
        self.dbname = arguments.dbname
        self.dbuser = arguments.dbuser
        self.dbhost = arguments.dbhost
        self.dbport = arguments.dbport
        self.conn = None
        
        self.sumsdbname = arguments.sumsdbname
        self.sumsdbuser = arguments.sumsdbuser
        self.sumsdbhost = arguments.sumsdbhost
        self.sumsdbport = arguments.sumsdbport
        self.sumsconn = None
        
        self.savedSignals = None

        super(TerminationHandler, self).__init__()
        
    def __enter__(self):
        self.enableInterrupts()

        # Acquire locks.
        self.rsLock = DrmsLock(self.lockFile, self.pidStr)
        self.rsLock.acquireLock()
        
        # Make main DB connection to RS database. We also have to connect to the SUMS database, so connect to that too.
        # The connections are NOT in autocommit mode. If changes need to be saved, then conn.commit() must be called.
        # Do this instead of using BEGIN and END/COMMIT statements, cuz I don't know if the psycopg2/libpq interaction
        # supports this properly.
        try:
            self.conn = psycopg2.connect(database=self.dbname, user=self.dbuser, host=self.dbhost, port=self.dbport)
            rslog.writeInfo([ 'Connected to DRMS database ' + self.dbname + ' on ' + self.dbhost + ':' + str(self.dbport) + ' as user ' + self.dbuser + '.' ])

            self.sumsconn = psycopg2.connect(database=self.sumsdbname, user=self.sumsdbuser, host=self.sumsdbhost, port=self.sumsdbport)
            rslog.writeInfo([ 'Connected to SUMS database ' + self.sumsdbname + ' on ' + self.sumsdbhost + ':' + str(self.sumsdbport) + ' as user ' + self.sumsdbuser + '.' ])            
        except psycopg2.DatabaseError as exc:
            self.__exit__(*sys.exc_info()) # clean up
            raise DBConnectionException('Unable to connect to a database.')
        except psycopg2.Error as exc:
            self.__exit__(*sys.exc_info()) # clean up
            raise DBConnectionException('Unable to connect to a database.')

        return self

    # Normally, __exit__ is called if an exception occurs inside the with block. And since SIGINT is converted
    # into a KeyboardInterrupt exception, it will be handled by __exit__(). However, SIGTERM will not - 
    # __exit__() will be bypassed if a SIGTERM signal is received. Use the signal handler installed in the
    # __enter__() call to handle SIGTERM.
    def __exit__(self, etype, value, traceback):
        self.log.writeDebug([ 'TerminationHandler.__exit__() called' ])
        if etype is not None:
            # If the context manager was exited without an exception, then etype is None
            import traceback
            self.log.writeDebug([ traceback.format_exc(5) ])

        print('Remote SUMS shutting down...')
        self.finalStuff()
        
        # Clean up lock
        try:     
            self.rsLock.releaseLock()   
            self.rsLock.close()
            self.rsLock = None
        except IOError:
            pass
            
        self.log.writeDebug([ 'Exiting TerminationHandler.' ])
        
        if etype == SystemExit:
            print('and done')
            raise TerminationException('Termination signal handler called.')
            
    def saveSignal(self, signo, frame):
        if self.savedSignals == None:
            self.savedSignals = []

        self.savedSignals.append((signo, frame))
        self.log.writeDebug([ 'saved signal ' +  str(signo) ])

    def disableInterrupts(self):
        signal.signal(signal.SIGINT, self.saveSignal)
        signal.signal(signal.SIGTERM, self.saveSignal)
        signal.signal(signal.SIGHUP, self.saveSignal)
        
    def enableInterrupts(self):
        signal.signal(signal.SIGINT, terminator)
        signal.signal(signal.SIGTERM, terminator)
        signal.signal(signal.SIGHUP, terminator)
        
        if type(self.savedSignals) is list:
            for signalReceived in self.savedSignals:
                terminator(*signalReceived)
        
        self.savedSignals = None

    def finalStuff(self):
        self.log.writeInfo([ 'Halting threads.' ])

        # Shut-down ScpWorker threads. Send the sdEvent to all threads first, then wait after they have ALL received the message.
        # Otherwise, later threads could try to download SUs whose download was aborted by earlier threads.
        gotLock = False
        try:
            gotLock = ScpWorker.lock.acquire()
            if gotLock:
                for scpWorker in ScpWorker.tList:
                    self.log.writeInfo([ 'telling Scp Worker (ID ' + str(scpWorker.id) + ') to halt' ])
                    scpWorker.stop()
        finally:
            if gotLock:
                ScpWorker.lock.release()

        while True:
            gotLock = False
            scpWorker = None
            try:
                gotLock = ScpWorker.lock.acquire()
                # got lock - can't get here otherwise

                if len(ScpWorker.tList) > 0:
                    scpWorker = ScpWorker.tList[0]
                else:
                    break
            except:
                break
            finally:
                if gotLock:
                    ScpWorker.lock.release()

            if scpWorker and isinstance(scpWorker, (ScpWorker)) and scpWorker.isAlive():
                # can't hold worker lock here - when the worker terminates, it acquires the same lock
                self.log.writeInfo([ 'waiting for Scp Worker (ID ' + str(scpWorker.id) + ') to halt' ])
                scpWorker.join() # will block, possibly for ever
        
        # Shut-down Downloader threads.
        gotLock = False
        try:
            gotLock = Downloader.lock.acquire()
            if gotLock:
                for downloader in Downloader.tList:
                    self.log.writeInfo([ 'Waiting for Downloader (SUNUM ' + str(downloader.su.sunum) + ') to halt.' ])
                    downloader.stop()
        finally:
            if gotLock:
                Downloader.lock.release() 

        while True:
            gotLock = False
            try:
                gotLock = Downloader.lock.acquire()
                # got lock - can't get here otherwise

                if len(Downloader.tList) > 0:
                    downloader = Downloader.tList[0]
                else:
                    break 
            finally:
                if gotLock:
                    Downloader.lock.release()
                    
            if downloader and isinstance(downloader, (Downloader)) and downloader.isAlive():
                downloader.join()                                    
            
        # Shut-down ProviderPoller threads.
        gotLock = False
        try:
            gotLock = ProviderPoller.lock.acquire()
            if gotLock:
                for poller in ProviderPoller.tList:
                    self.log.writeInfo([ 'Waiting for poller (request ID ' + str(poller.requestID) + ') to halt.' ])
                    poller.stop()
        finally:
            if gotLock:
                ProviderPoller.lock.release()

        while True:
            gotLock = False
            try:
                gotLock = ProviderPoller.lock.acquire()
                # got lock - can't get here otherwise
                
                if len(ProviderPoller.tList) > 0:
                    poller = ProviderPoller.tList[0]
                else:
                    break 
            finally:
                if gotLock:
                    ProviderPoller.lock.release()

            if poller and isinstance(poller, (ProviderPoller)) and poller.isAlive():
                poller.join()
        
        if self.sumsconn:
            self.sumsconn.close()
            self.sumsconn = None
                
        if self.conn:
            self.conn.close()
            self.conn = None
    
        self.log.flush()
        

    def rsConn(self):
        return self.conn

    def sumsConn(self):
        return self.sumsconn


class SumsDrmsParams(DRMSParams):
    def __init__(self):
        super(SumsDrmsParams, self).__init__()

    def get(self, name):
        val = super(SumsDrmsParams, self).get(name)

        if val is None:
            raise DrmsParamsException('Unknown DRMS parameter: ' + name + '.')
        return val


class Arguments(object):

    def __init__(self, parser):
        # This could raise in a few places. Let the caller handle these exceptions.
        self.parser = parser
        
        # Parse the arguments.
        self.parse()
        
        # Set all args.
        self.setAllArgs()
        
    def parse(self):
        try:
            self.parsedArgs = self.parser.parse_args()      
        except Exception as exc:
            if len(exc.args) == 2:
                type, msg = exc
                  
                if type != 'CmdlParser-ArgUnrecognized' and type != 'CmdlParser-ArgBadformat':
                    raise # Re-raise

                raise RSArgsException(msg)
            else:
                raise # Re-raise

    def setArg(self, name, value):
        if not hasattr(self, name):
            # Since Arguments is a new-style class, it has a __dict__, so we can
            # set attributes directly in the Arguments instance.
            setattr(self, name, value)
        else:
            raise RSArgsException('Attempt to set an argument that already exists: ' + name + '.')

    def setAllArgs(self):
        for key,val in list(vars(self.parsedArgs).items()):
            self.setArg(key, val)
        
    def getArg(self, name):
        try:
            return getattr(self, name)
        except AttributeError as exc:
            raise RSArgsException('Unknown argument: ' + name + '.')
            
    def dump(self, log):
        attrList = []
        for attr in sorted(vars(self)):
            attrList.append('  ' + attr + ':' + str(getattr(self, attr)))
        log.writeDebug([ '\n'.join(attrList) ])
        
    @classmethod
    def checkArg(cls, argName, exc, default, **kwargs):
        val = None
        if argName in kwargs:
            val = kwargs[argName]
        elif exc is not None:
            raise exc
        else:
            val = default
        return val


class Log(object):
    """Manage a logfile."""
    def __init__(self, file, level, formatter):
        self.fileName = file
        self.log = logging.getLogger()
        self.log.setLevel(level)
        self.fileHandler = logging.FileHandler(file)
        self.fileHandler.setLevel(level)
        self.fileHandler.setFormatter(formatter)
        self.log.addHandler(self.fileHandler)
        
    def close(self):
        if self.log:
            if self.fileHandler:
                self.log.removeHandler(self.fileHandler)
                self.fileHandler.flush()
                self.fileHandler.close()
                self.fileHandler = None
            self.log = None
            
    def flush(self):
        if self.log and self.fileHandler:
            self.fileHandler.flush()
            
    def getLevel(self):
        # Hacky way to get the level - make a dummy LogRecord
        logRecord = self.log.makeRecord(self.log.name, self.log.getEffectiveLevel(), None, '', '', None, None)
        return logRecord.levelname
        
    def __prependFrameInfo(self, msg):
        frame, fileName, lineNo, fxn, context, index = inspect.stack()[2]
        return os.path.basename(fileName) + ':' + str(lineNo) + ': ' + msg

    def writeDebug(self, text):
        if self.log:
            for line in text:                
                self.log.debug(self.__prependFrameInfo(line))
            self.fileHandler.flush()
            
    def writeInfo(self, text):
        if self.log:
            for line in text:
                self.log.info(self.__prependFrameInfo(line))
        self.fileHandler.flush()
    
    def writeWarning(self, text):
        if self.log:
            for line in text:
                self.log.warning(self.__prependFrameInfo(line))
            self.fileHandler.flush()
    
    def writeError(self, text):
        if self.log:
            for line in text:
                self.log.error(self.__prependFrameInfo(line))
            self.fileHandler.flush()
            
    def writeCritical(self, text):
        if self.log:
            for line in text:
                self.log.critical(self.__prependFrameInfo(line))
            self.fileHandler.flush()


class RemoteSumsException(Exception):

    def __init__(self, msg):
        super(RemoteSumsException, self).__init__(msg)
        self.msg = msg # also in args[0] due to base-class constructor
        self.retcode = RET_UNKNOWN_ERROR

class DrmsParamsException(RemoteSumsException):

    def __init__(self, msg):
        super(DrmsParamsException, self).__init__(msg)
        self.retcode = RET_UNKOWN_DRMSPARAMETER
        
class RSArgsException(RemoteSumsException):

    def __init__(self, msg):
        super(RSArgsException, self).__init__(msg)
        self.retcode = RET_INVALID_REMOTE_SUMS_ARGUMENT

class InvalidArgumentException(RemoteSumsException):

    def __init__(self, msg):
        super(InvalidArgumentException, self).__init__(msg)
        self.retcode = RET_INVALID_ARGUMENT
        
class NoSUReferenceException(RemoteSumsException):

    def __init__(self, msg):
        super(NoSUReferenceException, self).__init__(msg)
        self.retcode = RET_SU_NOT_REFERENCED
        
class WorkerReferenceException(RemoteSumsException):

    def __init__(self, msg):
        super(WorkerReferenceException, self).__init__(msg)
        self.retcode = RET_WORKER_REFERENCE_ERROR

class LockException(RemoteSumsException):

    def __init__(self, msg):
        super(LockException, self).__init__(msg)
        self.retcode = RET_UNABLE_TO_ACQUIRE_LOCK
        
class LockAndHoldException(RemoteSumsException):

    def __init__(self, msg):
        super(LockAndHoldException, self).__init__(msg)
        self.retcode = RET_UNABLE_TO_LOCK_OBJECT

class SutableReadException(RemoteSumsException):

    def __init__(self, msg):
        super(SutableReadException, self).__init__(msg)
        self.retcode = RET_UNABLE_TO_READ_SUTABLE

class SutableWriteException(RemoteSumsException):

    def __init__(self, msg):
        super(SutableWriteException, self).__init__(msg)
        self.retcode = RET_UNABLE_TO_WRITE_SUTABLE
        
class ReqtableReadException(RemoteSumsException):

    def __init__(self, msg):
        super(ReqtableReadException, self).__init__(msg)
        self.retcode = RET_UNABLE_TO_READ_REQTABLE
        
class ReqtableWriteException(RemoteSumsException):

    def __init__(self, msg):
        super(ReqtableWriteException, self).__init__(msg)
        self.retcode = RET_UNABLE_TO_WRITE_REQTABLE
        
class SitetableReadException(RemoteSumsException):

    def __init__(self, msg):
        super(SitetableReadException, self).__init__(msg)
        self.retcode = RET_UNABLE_TO_READ_SITETABLE
        
class DuplicateSUException(RemoteSumsException):

    def __init__(self, msg):
        super(DuplicateSUException, self).__init__(msg)
        self.retcode = RET_DUPLICATE_SUNUM

class UnknownSunumException(RemoteSumsException):

    def __init__(self, msg):
        super(UnknownSunumException, self).__init__(msg)
        self.retcode = RET_UNKNOWN_SUNUM
        
class SumsAPIException(RemoteSumsException):

    def __init__(self, msg):
        super(SumsAPIException, self).__init__(msg)
        self.retcode = RET_ERROR_CALLING_SUMS_API
        
class UnknownRequestidException(RemoteSumsException):

    def __init__(self, msg):
        super(UnknownRequestidException, self).__init__(msg)
        self.retcode = RET_UNKNOWN_REQUESTID

class GetRetentionException(RemoteSumsException):

    def __init__(self, msg):
        super(GetRetentionException, self).__init__(msg)
        self.retcode = RET_UNABLE_TO_GET_RETENTION
        
class InvalidSunumException(RemoteSumsException):

    def __init__(self, msg):
        super(InvalidSunumException, self).__init__(msg)
        self.retcode = RET_INVALID_SUNUM

class UnknownSitecodeException(RemoteSumsException):

    def __init__(self, msg):
        super(UnknownSitecodeException, self).__init__(msg)
        self.retcode = RET_UNKNOWN_SITE_CODE
        
class ScpSUException(RemoteSumsException):

    def __init__(self, msg):
        super(ScpSUException, self).__init__(msg)
        self.retcode = RET_UNABLE_TO_DOWNLOAD_SU
        
class ShutDownException(RemoteSumsException):

    def __init__(self, msg):
        super(ShutDownException, self).__init__(msg)
        self.retcode = RET_SHUTDOWN_REQUESTED
        
class NoSusForDlException(RemoteSumsException):

    def __init__(self, msg):
        super(NoSusForDlException, self).__init__(msg)
        self.retcode = RET_NO_SUS_TO_DOWNLOAD
        
class DownloaderException(RemoteSumsException):

    def __init__(self, msg):
        super(DownloaderException, self).__init__(msg)
        self.retcode = RET_DOWNLOADER

class SUPathCGIException(RemoteSumsException):

    def __init__(self, msg):
        super(SUPathCGIException, self).__init__(msg)
        self.retcode = RET_SUPATH_CGI
        
class ProviderPollerException(RemoteSumsException):

    def __init__(self, msg):
        super(ProviderPollerException, self).__init__(msg)
        self.retcode = RET_PROVIDER_POLLER
        
class RSIOException(RemoteSumsException):

    def __init__(self, msg):
        super(RSIOException, self).__init__(msg)
        self.retcode = RET_IO_ERROR
        
class StartThreadException(RemoteSumsException):

    def __init__(self, msg):
        super(StartThreadException, self).__init__(msg)
        self.retcode = RET_UNABLE_TO_START_THREAD
        
class MaxThreadsException(RemoteSumsException):

    def __init__(self, msg):
        super(MaxThreadsException, self).__init__(msg)
        self.retcode = RET_TOO_MANY_THREADS
        
class DBConnectionException(RemoteSumsException):

    def __init__(self, msg):
        super(DBConnectionException, self).__init__(msg)
        self.retcode = RET_UNABLE_TO_CONNECT_TO_DB
        
class TimeoutException(RemoteSumsException):

    def __init__(self, msg):
        super(TimeoutException, self).__init__(msg)
        
        # do not set retcode - this error is never fatal

class TerminationException(RemoteSumsException):

    def __init__(self, msg):
        super(TerminationException, self).__init__(msg)
        self.retcode = RET_USER_TERMINATED

        
class StorageUnit(object):
    def __init__(self, sunum, series, retention, starttime, refcount, status, errmsg):
        self.sunum = sunum
        self.series = series
        self.retention = retention
        self.starttime = starttime
        self.refcount = refcount
        self.status = status
        self.errmsg = errmsg
        
        # not saved in the DB
        self.giveUpTheGhost = False
        self.dirty = False
        self.new = False
        self.polling = False
        self.path = None
        self.suSize = None
        self.worker = None
        
        self.lock = threading.Lock()
        
    def acquireLock(self):
        return self.lock.acquire()
    
    def releaseLock(self):
        self.lock.release()
        
    def acquireLockWithoutBlocking(self):
        return self.lock.acquire(blocking=False)
        
    def setDirty(self, value):
        if not isinstance(value, (bool)):
            raise InvalidArgumentException('setDirty(): argument must be a bool')
        
        self.dirty = value
        
    def getSunum(self):
        return self.sunum
        
    def getSeries(self):
        return self.series
            
    def setSeries(self, value):
        if not isinstance(value, (str)):
            raise InvalidArgumentException('setSeries(): argument must be a str')

        self.series = value
        self.setDirty(True)
        
    def getRetention(self):
        return self.retention
        
    def setRetention(self, value):
        if not isinstance(value, (int)):
            raise InvalidArgumentException('setRetention(): argument must be an integer')
            
        self.retention = value
        self.setDirty(True)
        
    def getStarttime(self):
        return self.starttime
    
    def setStarttime(self, value):
        if not isinstance(value, (datetime)):
            raise InvalidArgumentException('setStarttime(): argument must be a datetime')
            
        self.starttime = value
        self.setDirty(True)
        
    def touch(self):
        self.setStarttime(datetime.now(timezone.utc))
        
    def incrementRefcount(self):
        if self.giveUpTheGhost:
            # cannot increment refcount on an SU that has already been marked for deletion; it is as if
            # the SU does not exist any more
            raise NoSUReferenceException('cannot increment refcount on unreferenced SU record ' + str(self.sunum))

        self.refcount += 1
        self.setDirty(True)
        
        return self.refcount
        
    def decrementRefcount(self):
        if self.giveUpTheGhost:
            raise NoSUReferenceException('cannot decrement refcount on unreferenced SU record ' + str(self.sunum))
                    
        self.refcount -= 1
        if self.refcount == 0:
            self.giveUpTheGhost = True

        self.setDirty(True)
        
        return self.refcount
    
    def getStatus(self):
        return self.status
    
    # Set properties that are saved to the DB.
    def setStatus(self, codeValue, msgValue):
        if not isinstance(codeValue, (str)):
            raise InvalidArgumentException('setStatus(): fist argument must be a str.')
            
        if not isinstance(msgValue, (str)) and msgValue is not None:
            raise InvalidArgumentException('setStatus(): second argument must be a str or None.')
            
        self.status = codeValue
        if msgValue is not None:
            self.errmsg = msgValue
        else:
            self.errmsg = ''

        self.setDirty(True)
        
    # properties that are NOT saved to the DB
    def getNew(self):
        return self.new
    
    def setNew(self, value):
        if not isinstance(value, (bool)):
            raise InvalidArgumentException('setNew(): argument must be a bool')
            
        self.new = value
        
    def getPolling(self):
        return self.polling
        
    def setPolling(self, value):
        if not isinstance(value, (bool)):
            raise InvalidArgumentException('setPolling(): argument must be a bool')
            
        self.polling = value
        
    def getPath(self):
        path = None
        if hasattr(self, 'path'):
            path = self.path
        return path
        
    def setPath(self, value):
        if not isinstance(value, (str)) and value is not None:
            raise InvalidArgumentException('setPath(): argument must be a str or None')
            
        self.path = value
        
    def getSize(self):
        return self.suSize
            
    def setSize(self, value):
        if not isinstance(value, (int)):
            raise InvalidArgumentException('setSize(): argument must be an int')
            
        self.suSize = value
    
    def getWorker(self):
        return self.worker
        
    def setWorker(self, value):
        if not isinstance(value, (Downloader)):
            raise InvalidArgumentException('setWorker(): argument must be a Downloader')
            
        if self.worker:
            raise WorkerReferenceException('cannot set worker for SU ' + str(self.sunum) + '; worker already exists')
            
        self.worker = value
        
    def stopWorker(self):
        if hasattr(self, 'worker') and self.worker and isInstance(self.worker, (Downloader)) and self.worker.isAlive():
            self.worker.stop()
            if self.worker.isAlive():
                # Give the worker 15 seconds to self-terminate.
                self.worker.join(15)
            if self.worker.isAlive():
                # Apparently, there is no way to kill a thread from another thread. So, we are just going to
                # orphan the thread (so it doesn't use up our maxThreads quota).
                try:
                    Downloader.lock.acquire()
                    if self.worker in Downloader.tList:
                        Downloader.tList.remove(self.worker) # This thread is no longer one of the running threads.
                        if len(Downloader.tList) <= Downloader.maxThreads - 1:
                            # Fire event so that main thread can add new SUs to the download queue.
                            Downloader.eventMaxThreads.set()
                            # Clear event so that main will block the next time it calls wait.
                            Downloader.eventMaxThreads.clear()
                finally:
                    Downloader.lock.release()

            self.removeWorker()        
        
    def removeWorker(self):
        self.worker = None


class SuTable:
    rsConn = None
    rsDbLock = None # There is one global lock for the rs connection. Since the main thread and the Downloader and ScpWorker threads
                    # all share the rs connection, they all need to use the same lock.

    sumsConn = None
    sumsDbLock = None # Since all Downloader threads and the main thread share the same connection, their cursors 
                      # on these connections are not isolated. Use a lock to ensure a consistent view in the
                      # offline() method.
    
    def __init__(self, tableName, timeOut, log):
        self.tableName = tableName
        self.timeOut = timeOut # A timedelta object - the length of time to wait for a download to complete.
        self.lock = threading.Lock()
        self.log = log
        self.suDict = {}
    
    def read(self):
        # sus(sunum, starttime, refcount, status, errmsg)
        cmd = 'SELECT sunum, series, retention, starttime, refcount, status, errmsg FROM ' + self.tableName
    
        try:
            gotRsDbLock = SuTable.rsDbLock.acquire()
            if gotRsDbLock:
                with SuTable.rsConn.cursor() as cursor:
                    cursor.execute(cmd)
            
                    for record in cursor:
                        sunumStr = str(record[0])

                        self.suDict[sunumStr] = {}
                        sunum = record[0]         # integer
                        series = record[1]        # text
                        retention = record[2]     # integer
                        # Whoa! pyscopg returns timestamps as datetime.datetime objects already!
                        starttime = record[3]     # datetime.datetime
                        refcount = record[4]      # integer
                        status = record[5]        # text
                        errmsg = record[6]        # text

                        su = StorageUnit(sunum, series, retention, starttime, refcount, status, errmsg)

                        # Not read from or saved to database.
                        su.setDirty(False)
                        su.setNew(False)
                        su.setPolling(False)
                        su.setPath(None)              # The server path of the SU to be downloaded.
                        # worker is already None
                        self.suDict[sunumStr] = su
        except psycopg2.Error as exc:
            raise SutableReadException(exc.diag.message_primary)
        finally:
            SuTable.rsConn.rollback() # We read from the DB only, so no need to commit anything.
            if gotRsDbLock:
                SuTable.rsDbLock.release()

    def tryRead(self):
        nAtts = 0
        while True:
            try:
                self.read()
                break
            except SutableReadException:                
                if nAtts > 10:
                    raise # Re-raise

            nAtts += 1
            time.sleep(1)
            
    def getUpdateDBSql(self, sunums=None):
        sql = []

        sus, missingSunums = self.getAndLockSUs(sunums=sunums, filter=None) # want to get giveUpTheGhost sus
        try:
            if len(missingSunums) > 0:
                # the caller provided the sunums argument and it was not empty, and some of the provided sunums were invalid
                sunumStr = ','.join([ str(sunum) for sunum in missingSunums] )
                self.log.writeWarning([ 'SuTable.getUpdateDBSql() called with invalid SUNUMs: ' + sunumStr + '; skipping' ])

            for su in sus:
                if su.giveUpTheGhost:
                    sql.append('DELETE FROM ' + self.tableName + ' WHERE sunum = ' + str(su.sunum))
                    del self.suDict[ str(su.sunum) ]
                elif su.dirty:
                    if su.new:
                        sql.append('INSERT INTO ' + self.tableName + '(sunum, series, retention, starttime, refcount, status, errmsg) VALUES(' + str(su.sunum) + ",'" + su.series + "', " + str(su.retention) + ", '" + su.starttime.strftime('%Y-%m-%d %T%z') + "', " + str(su.refcount) + ", '" + su.status + "', '" + su.errmsg + "')")
                    else:
                        sql.append('UPDATE ' + self.tableName + " SET series='" + su.series + "', retention=" + str(su.retention) + ", starttime='" + su.starttime.strftime('%Y-%m-%d %T%z') + "', refcount=" + str(su.refcount) + ", status='" + su.status + "', errmsg='" + su.errmsg + "' WHERE sunum=" + str(su.sunum))

                    su.setDirty(False)
                    su.setNew(False)
        finally:
            for su in sus:
                su.releaseLock()
                
        return sql
    
    # This will NOT commit database changes. You are generally going to want to do this as part of a larger db change. Commit the changes
    # after all changes that form the atomic set of changes. Do not call rsConn.commit().
    #
    # ACQUIRES THE SU TABLE LOCK.
    # returns True if the DB was changed (in a uncommitted transaction)
    def updateDB(self, sql):
        if len(sql) > 0:
            # execute the SQL
            try:
                cmd = ';\n'.join(sql)

                self.log.writeDebug([ 'executing DB command: ' + cmd ])
                with SuTable.rsConn.cursor() as cursor:
                    cursor.execute(cmd)
                # The cursor has been closed, but the transaction has not been committed, as designed.
                self.log.writeDebug([ 'DB command succeeded: ' + cmd ])
            except psycopg2.Error as exc:
                import traceback
            
                SuTable.rsConn.rollback()
                self.log.writeError([ traceback.format_exc(5) ])
                raise SutableWriteException(exc.diag.message_primary)
            
            return True
        else:
            return False
        
    # This WILL commit changes to the db.
    def updateDbAndCommit(self, sunums=None):
        sql = self.getUpdateDBSql(sunums)

        SuTable.rsDbLock.acquire()
        try:
            if self.updateDB(sql):
                SuTable.rsConn.commit()
                self.log.writeDebug([ 'committed RS DB changes' ])
            else:
                self.log.writeDebug([ 'no SU changes made to DB' ])
        except:
            SuTable.rsConn.rollback()
            self.log.writeDebug([ 'rolled-back RS DB changes' ])
        finally:
            SuTable.rsDbLock.release()

    def acquireLock(self):
        return self.lock.acquire()
        # self.log.writeDebug(['Acquired SU-Table lock.'])
    
    def releaseLock(self):
        self.lock.release()
        # self.log.writeDebug(['Released SU-Table lock.'])
    
    # Main thread only.
    def insert(self, **kwargs):
        sunums = Arguments.checkArg('sunums', None, None, **kwargs)
        lockTable = Arguments.checkArg('lockTable', None, True, **kwargs)
    
        gotTableLock = False
        try:
            # Get lock because we are modifying self.suDict.
            if lockTable:
                gotTableLock = self.acquireLock()
 
            for asunum in sunums:
                sunumStr = str(asunum)
        
                if sunumStr in self.suDict:
                    raise DuplicateSUException('SU-table record already exists for SU ' + sunumStr + '.')
                
                su = StorageUnit(asunum, '', -1, datetime.now(timezone.utc), 1, 'P', '')

                su.setDirty(True)
                # Set the new flag (so that the record will be INSERTed into the SU database table instead of UPDATEd).
                su.setNew(True)
                # The polling flag is set only while we are waiting for a providing site to give us paths for SUs.
                su.setPolling(False)
        
                self.suDict[sunumStr] = su
        finally:
            if lockTable and gotTableLock:
                self.releaseLock()

    def setStatus(self, sunums, code, msg=None):
        sus, missingSunums = self.getAndLockSUs(sunums=sunums, filter=SuTable.removeGhosts)
        try:
            for su in sus:
                su.setStatus(code, msg)
        finally:
            for su in sus:
                su.releaseLock()

    def setSeries(self, sunums, series):
        sus, missingSunums = self.getAndLockSUs(sunums=sunums, filter=SuTable.removeGhosts)
        try:
            for su in sus:
                su.setSeries(series)
        finally:
            for su in sus:
                su.releaseLock()

    def setRetention(self, sunums, retention):    
        sus, missingSunums = self.getAndLockSUs(sunums=sunums, filter=SuTable.removeGhosts)
        try:
            for su in sus:
                su.setRetention(retention)
        finally:
            for su in sus:
                su.releaseLock()

    def setStarttime(self, sunums, starttime):
        sus, missingSunums = self.getAndLockSUs(sunums=sunums, filter=SuTable.removeGhosts)
        try:
            for su in sus:
                su.setStarttime(starttime)
        finally:
            for su in sus:
                su.releaseLock()

    def incrementRefcount(self, sunums):
        sus, missingSunums = self.getAndLockSUs(sunums=sunums)
        try:
            for su in sus:
                refCount = su.incrementRefcount()
                self.log.writeDebug([ 'incremented refcount for ' + str(su.sunum) + '; refcount is now ' + str(refCount) ])
        finally:
            for su in sus:
                su.releaseLock()

    def decrementRefcount(self, sunums):
        sus, missingSunums = self.getAndLockSUs(sunums=sunums)
        try:
            for su in sus:
                refCount = su.decrementRefcount()
                self.log.writeDebug([ 'decremented refcount for ' + str(su.sunum) + '; refcount is now ' + str(refCount) ])
        finally:
            for su in sus:
                su.releaseLock()
        
    def setWorker(self, sunum, worker):
        sunumStr = str(sunum)

        su = self.getAndLockSU(sunum=sunum, filter=SuTable.removeGhosts)
        
        if su:
            try:
                su.setWorker(worker)
            finally:
                su.releaseLock()

    # class private, to prevent code outside this class from calling this method without first locking the table;
    # we need to lock the table since it reads self.suDict
    def __get(self, sunums=None):
        # DOES NOT acquire table lock; this method is called by SUTable.getAndLockSU(), which acquires the
        # table lock; does not filter-out SUs with the giveUpTheGhost attribute since callers of this
        # function may need to act on those SUs
        foundSUs = []
        notfoundSunums = []
        
        if sunums is None:
            for sunumStr, su in self.suDict.items():
                foundSUs.append(su)
        else:
            for asunum in sunums:
                sunumStr = str(asunum)
    
                try:
                    if not sunumStr in self.suDict or not self.suDict[sunumStr]:
                        raise UnknownSunumException('(__get) no SU-table record exists for SU ' + sunumStr)
                    foundSUs.append(self.suDict[sunumStr])
                except:
                    notfoundSunums.append(asunum)

        return (foundSUs, notfoundSunums)
        
    # This will acquire the SU lock. The caller must release it!! Because the SU lock is acquired while the 
    # SU-table lock is held, nobody can modify the SU after this method returns and before the caller
    # releases the SU lock.
    def getAndLockSU(self, **kwargs):
        lockTable = Arguments.checkArg('lockTable', None, True, **kwargs)
        sunum = Arguments.checkArg('sunum', InvalidArgumentException('getAndLockSU(): missing sunums argument'), None, **kwargs)
        filter = Arguments.checkArg('filter', None, None, **kwargs)

        gotTableLock = False
        gotSULock = False
        su = None
        removedSus = []

        try:
            if lockTable:
                gotTableLock = self.acquireLock()

            # lock acquired (can't get here otherwise)
            sus, missingSunums = self.__get([ sunum ])
            
            if len(sus) == 1:
                su = sus[0]
                gotSULock = su.acquireLock()

                if filter:
                    filteredSus, removedSus = filter(sus)
                    if len(removedSus) > 0:
                        raise UnknownSunumException('SU ' + str(sunum) + ' has been deleted')
            else:
                raise UnknownSunumException('SU ' + str(sunum) + ' has been deleted')
        except UnknownSunumException:
            if gotSULock:
                su.releaseLock()
            su = None
        except:
            if gotSULock:
                su.releaseLock()
            su = None
            raise
        finally:
            # Always release lock.
            if lockTable and gotTableLock:
                self.releaseLock()

        return su

    # some SUs may not exist; return a tuple - the first element is a list of existing and locked SUs, and the second element is
    # a list of invalid SUNUMs
    def getAndLockSUs(self, **kwargs):
        lockTable = Arguments.checkArg('lockTable', None, True, **kwargs)
        releaseTableLock = Arguments.checkArg('releaseTableLock', None, True, **kwargs)
        sunums = Arguments.checkArg('sunums', None, None, **kwargs)
        filter = Arguments.checkArg('filter', None, None, **kwargs)

        gotTableLock = False
        lockedSUs = []
        unlockedSunums = []
        success = False

        try:
            if lockTable:
                gotTableLock = self.acquireLock()

            # lock acquired (can't get here otherwise)
            sus, missingSunums = self.__get(sunums)
            
            unlockedSunums.extend(missingSunums)

            for su in sus:
                gotSULock = False
                try:
                    gotSULock = su.acquireLock()
                    if filter:
                        filteredSus, removedSus = filter([ su ])
                        if len(removedSus) > 0:
                            raise UnknownSunumException('SU ' + str(su.sunum) + ' has been deleted')
                except UnknownSunumException:
                    import traceback
                    
                    self.log.writeWarning([ '(getAndLockSUs) unable to acquire lock for SU ' + str(su.sunum) ])
                    self.log.writeDebug([ traceback.format_exc(5) ])
                    unlockedSunums.append(su.sunum)
                    if gotSULock:
                        su.releaseLock()
                        gotSULock = False
                except:
                    unlockedSunums.append(su.sunum)
                    if gotSULock:
                        su.releaseLock()
                        gotSULock = False

                    raise
                finally:
                    if gotSULock:
                        lockedSUs.append(su)

            success = True
        except:
            # if an error happens, we need to release locks
            for su in lockedSUs:
                su.releaseLock()
                
            if gotTableLock:
                self.releaseLock()
            
            lockedSUs = []
            unlockedSunums = []
        finally:
            # Always release lock.
            if gotTableLock and releaseTableLock:
                self.releaseLock()
                
        return (lockedSUs, unlockedSunums)

    # Returns a list of SU objects.
    # NOT THREAD SAFE! This method is called by the main thread on start-up, before any other threads have been started.
    def getPending(self):
        pending = []
        
        for sunumStr, su in self.suDict.items():                
            if su.status == 'P':
                su.touch()
                pending.append(su)

        # Sorts in place - and returns None. The SUs will be sorted by starttime later, before Downloader threads are created
        # for them.
        pending.sort(key=lambda ansu : ansu.sunum)

        return pending
        
    # Returns a list of SU objects.
    # NOT THREAD SAFE! This method is called by the main thread on start-up, before any other threads have been started.
    def getWorking(self):
        working = []
        
        for sunumStr, su in self.suDict.items():                
            if su.status == 'W':
                su.touch()
                working.append(su)

        # Sorts in place - and returns None. The SUs will be sorted by starttime later, before Downloader threads are created
        # for them.
        working.sort(key=lambda ansu : ansu.sunum)

        return working
    
    # Returns num SU entries whose status is 'W'. If fewer than num W entries exist, then all W SU entries are returned.
    # Must acquire SU table lock before calling this method. There is no need to lock the individual SUs because the ScpWorker
    # threads must acquire the SU Table lock before calling getNextNWorking(). No other ScpWorker thread can 
    # change a W to a D while this ScpWorker is collecting SUs with a W status.
    #
    # Sort by start time - process the oldest first.
    #
    # All SUs returned MUST have the same scpUser, scpHost, and scpPort.
    def getAndLockNextNWorkingSUs(self, id, num):
        nworking = []
        scpUser = None
        scpHost = None
        scpPort = None
        scpInfoSet = False

        # get lock because we are reading self.suDict
        self.acquireLock()
        try:

            sortedSUs = sorted(list(self.suDict.values()), key=lambda su : su.starttime.strftime('%Y-%m-%d %T'))
            it = iter(sortedSUs)
            try:
                while num > 0:
                    su = next(it)
                    
                    try:
                        # do not block - just skip all SUs that are being processed somewhere else; we want to 
                        # keep the scp threads running at all times
                        gotSULock = su.acquireLockWithoutBlocking()
                        
                        if gotSULock:
                            if su.status == 'W':
                                su.touch()
                                if su.worker is None:
                                    raise WorkerReferenceException('No worker thread assigned to SU ' + str(su.sunum) + '.')
                                if scpInfoSet == False:
                                    scpUser = su.worker.scpUser
                                    scpHost = su.worker.scpHost
                                    scpPort = su.worker.scpPort
                                    scpInfoSet = True

                                if su.worker.scpUser == scpUser and su.worker.scpHost == scpHost and su.worker.scpPort == scpPort:
                                    nworking.append(su)
                                    num -= 1
                                else:
                                    if gotSULock:
                                        su.releaseLock()
                                        gotSULock = False
                    except:
                        # remove su from nworking (if we appended it)
                        if su in nworking:
                            nworking.pop()
                        if gotSULock:
                            su.releaseLock()
                            gotSULock = False
                    finally:
                        if su not in nworking and gotSULock:
                            su.releaseLock()
            except StopIteration:
                pass
        except:
            for su in nworking:
                su.releaseLock()
            nworking = []
        finally:
            self.releaseLock()

        return (nworking, (scpUser, scpHost, scpPort))      

    def getTimeout(self):
        return self.timeOut

    @classmethod
    def offline(cls, sunums, binPath, log):
        # There is not an efficient way to check for the SU being on/offline. But we can use jsoc_fetch (vs. show_info - jsoc_fetch returns
        # JSON, which is handy). And it also can be called in a mode where it does not trigger a SUM_get() - it uses SUM_infoAns():
        #   op=exp_su requestid=NOASYNCREQUEST sunum=123456789 format=json formatvar=dataobj method=url_quick protocol=as-is
        rv = []        

        if len(sunums) > 0:
            # query the SUMS db and check to see which of the SUs in sunums are present in the sum_main/sum_partn_alloc table
            cmd = "SELECT T1.ds_index, T1.online_loc, T1.online_status, T1.archive_status, T1.offsite_ack, T1.history_comment, T1.owning_series, T1.storage_group, T1.bytes, T1.create_sumid, T1.creat_date, T1.username, COALESCE(T1.arch_tape, 'N/A'), COALESCE(T1.arch_tape_fn, 0), COALESCE(T1.arch_tape_date, '1958-01-01 00:00:00'), COALESCE(T1.safe_tape, 'N/A'), COALESCE(T1.safe_tape_fn, 0), COALESCE(T1.safe_tape_date, '1958-01-01 00:00:00'), COALESCE(T2.effective_date, '195801010000'), coalesce(T2.status, 0), coalesce(T2.archive_substatus, 0) FROM " + SUM_MAIN + " AS T1 LEFT OUTER JOIN " + SUM_PARTN_ALLOC + " AS T2 ON (T1.ds_index = T2.ds_index) WHERE T1.ds_index IN (" + ','.join([ str(asunum) for asunum in sunums ]) + ')'

            try:
                gotSumsDbLock = SuTable.sumsDbLock.acquire()
                if gotSumsDbLock:
                    with SuTable.sumsConn.cursor() as cursor:
                        knownSUs = {} # bitmap of SUs known to the SUMS db.
                        try:                    
                            cursor.execute(cmd)
                            log.writeDebug([ 'Successfully queried SUMS db for known SUs.' ])
                        
                            # Put each SU in the result into the bitmap.
                            for row in cursor:
                                knownSUs[str(row[0])] = True
                        
                            # Determine which SUs for which we are going to initiate Downloaders that are not already in SUMS.
                            for ansunum in sunums:                        
                                if str(ansunum) not in knownSUs:
                                    rv.append(ansunum)
                        except psycopg2.Error as exc:
                            # Handle database-command errors. These are all due to problems communicating with the SUMS db.
                            raise SumsAPIException(exc.diag.message_primary + ': ' + cmd + '.') 
                        finally:
                            # Not making changes, so rollback always.
                            SuTable.sumsConn.rollback()
            finally:
                if gotSumsDbLock:
                    SuTable.sumsDbLock.release()

        return rv
    
    # must call from a getAndLockSU...() method
    @classmethod
    def removeGhosts(cls, sus):
        retKept = []
        retRemoved = []

        for su in sus:
            if not su.giveUpTheGhost:
                retKept.append(su)
            else:
                retRemoved.append(su)

        return (retKept, retRemoved)


class ReqTable:
    rsConn = None
    rsDbLock = None # There is one global lock for the rs connection. Since the main thread and the Downloader and ScpWorker threads
                    # all share the rs connection, they all need to use the same lock.

    def __init__(self, tableName, timeOut, log):
        self.tableName = tableName
        self.timeOut = timeOut
        self.log = log
        self.reqDict = {}
    
    def read(self):
        # requests(requestid, starttime, sunums, status, errmsg)
        cmd = 'SELECT requestid, dbhost, dbport, dbname, starttime, sunums, status, errmsg FROM ' + self.tableName
        
        try:
            gotRsDbLock = ReqTable.rsDbLock.acquire()
            if gotRsDbLock:
                with ReqTable.rsConn.cursor() as cursor:
                    cursor.execute(cmd)
        
                    for record in cursor:
                        requestidStr = str(record[0])

                        self.reqDict[requestidStr] = {}
                        self.reqDict[requestidStr]['requestid'] = record[0] # integer
                        self.reqDict[requestidStr]['dbhost'] = record[1]    # text
                        self.reqDict[requestidStr]['dbport'] = record[2]    # integer
                        self.reqDict[requestidStr]['dbname'] = record[3]    # text
                        # Whoa! pyscopg returns timestamps as datetime.datetime objects already!
                        self.reqDict[requestidStr]['starttime'] = record[4] # datetime.datetime
                        self.reqDict[requestidStr]['sunums'] = [int(asunum) for asunum in record[5].split(',')] # text (originally)
                        self.reqDict[requestidStr]['status'] = record[6]    # text
                        self.reqDict[requestidStr]['errmsg'] = record[7]    # text
                        self.reqDict[requestidStr]['dirty'] = False
        except psycopg2.Error as exc:
            raise ReqtableReadException(exc.diag.message_primary, cmd)
        finally:
            ReqTable.rsConn.rollback()
            
            if gotRsDbLock:
                ReqTable.rsDbLock.release()

    def tryRead(self):
        nAtts = 0
        while True:
            try:
                self.read()
                break
            except ReqtableReadException:
                if nAtts > 10:
                    raise # Re-raise

            nAtts += 1
            time.sleep(1)

    # This method finds 'N' records inserted since the last time it was run (or since the table was first read). It ignores
    # all other changes to the database table (made from outside this program) that have happened. To read those changes,
    # shut down this program, then make the changes, then start this program again.
    def refresh(self):
        # Delete existing items from self.
        self.reqDict = {}
        
        # Read the table from the database anew.
        self.tryRead()

    def getUpdateDBSql(self, requestids=None):
        sql = []

        if requestids:
            # Update the specified records.
            for arequestid in requestids:
                requestidStr = str(arequestid)
            
                if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                    self.log.writeWarning([ 'no request-table record exists for ID ' + requestidStr + '; skipping' ])
                
                if self.reqDict[requestidStr]['dirty']:
                    self.log.writeDebug([ 'updating req table DB for request ' + requestidStr ])
                    # The only columns that this daemon will modify are status and errmsg.
                    sql.append('UPDATE ' + self.tableName + " SET status='" + self.reqDict[requestidStr]['status'] + "', errmsg='" + self.reqDict[requestidStr]['errmsg'] + "' WHERE requestid=" + requestidStr)
                    
                    self.reqDict[requestidStr]['dirty'] = False
        else:
            for requestidStr in self.reqDict:
                if self.reqDict[requestidStr]['dirty']:
                    self.log.writeDebug([ 'updating req table DB for request ' + requestidStr ])
                    # The only columns that this daemon will modify are status and errmsg.
                    sql.append('UPDATE ' + self.tableName + " SET status='" + self.reqDict[requestidStr]['status'] + "', errmsg='" + self.reqDict[requestidStr]['errmsg'] + "' WHERE requestid='" + requestidStr + "'")
                    
                    self.reqDict[requestidStr]['dirty'] = False
                    
        return sql

    # This will NOT commit database changes. You are generally going to want to do this as part of a larger db change. Commit the changes
    # after all changes that form the atomic set of changes.
    # no lock needed for the requests table (only the main thread access it)
    # returns True if the DB was changed (inside the current, uncommitted transaction), False otherwise
    def updateDB(self, sql):        
        if len(sql) > 0:
            try:
                cmd = ';\n'.join(sql)
                self.log.writeDebug([ 'executing DB command: ' + cmd])
                with ReqTable.rsConn.cursor() as cursor:
                    cursor.execute(cmd)
                # The cursor has been closed, but the transaction has not been committed, as designed.
            except psycopg2.Error as exc:
                import traceback
                
                self.log.writeError([ traceback.format_exc(5) ])
                ReqTable.rsConn.rollback()
                self.log.writeDebug([ 'rolling-back req table DB update' ])
                raise ReqtableWriteException(exc.diag.message_primary)
                
            return True
        else:
            return False
                    
    # This WILL commit changes to the db.
    def updateDbAndCommit(self, requestids=None):
        sql = self.getUpdateDBSql(requestids)
        
        ReqTable.rsDbLock.acquire()
        try:
            if self.updateDB(sql):
                ReqTable.rsConn.commit()
                self.log.writeDebug([ 'committed RS DB changes' ])
            else:
                self.log.writeDebug([ 'no request changes made to DB' ])
        except:
            self.log.writeDebug([ 'rolled-back RS DB changes' ])
            ReqTable.rsConn.rollback()
        finally:
            ReqTable.rsDbLock.release()

    # This will NOT commit database changes. You are generally going to want to do this as part of a larger db change. Commit the changes
    # after all changes that form the atomic set of changes. Do not call rsConn.commit().
    def deleteDB(self, requestids):
        if len(requestids) > 0:
            for arequestid in requestids:
                requestidStr = str(requestid)
                
                if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                    raise UnknownRequestidException('No request-table record exists for ID ' + requestidStr + '.')

                del self.reqDict[requestidStr]
            
            reqidLstStr = ','.join(requestids)
            
            cmd = 'DELETE FROM ' + self.tableName + ' WHERE requestid=' + reqidLstStr
            
            needsRollback = True
            try:
                gotRsDbLock = ReqTable.rsDbLock.acquire()
                if gotRsDbLock:
                    with ReqTable.rsConn.cursor() as cursor:
                        cursor.execute(cmd)
                    needsRollback = False
                    # The cursor has been closed, but the transaction has not been committed, as designed.
            except psycopg2.Error as exc:
                raise ReqtableWriteException(exc.diag.message_primary + ': ' + cmd)
            finally:
                if needsRollback:
                    ReqTable.rsConn.rollback()
                if gotRsDbLock:
                    ReqTable.rsDbLock.release()

    def setStatus(self, requestids, code, msg=None):
        for arequestid in requestids:
            requestidStr = str(arequestid)
        
            if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                raise UnknownRequestidException('No request-table record exists for ID ' + requestidStr + '.')
            
            self.reqDict[requestidStr]['status'] = code
            if msg:
                self.reqDict[requestidStr]['errmsg'] = msg
            else:
                self.reqDict[requestidStr]['errmsg'] = ''
            
            # Set dirty flag
            self.reqDict[requestidStr]['dirty'] = True

    def get(self, requestids=None):
        toRet = []
    
        if not requestids:
            return [ self.reqDict[key] for (key, val) in self.reqDict.items() ]
        
        for arequestid in requestids:
            requestidStr = str(arequestid)
            
            if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                raise UnknownRequestidException('No request-table record exists for ID ' + requestidStr + '.')
    
            toRet.append(self.reqDict[requestidStr])
    
        return toRet        
    
    def getPending(self):
        pendLst = []

        for requestidStr, reqObj in self.reqDict.items():
            if reqObj['status'] == 'P':
                pendLst.append(reqObj)

        # Sort by start time. Sorts in place - and returns None.
        pendLst.sort(key=lambda dict : dict['starttime'].strftime('%Y-%m-%d %T'))
    
        return pendLst
    
    def getNew(self):
        newLst = []                
                
        for requestidStr, reqObj in self.reqDict.items():
            if reqObj['status'] == 'N':
                newLst.append(reqObj)

        # Sort by start time. Sorts in place - and returns None.
        newLst.sort(key=lambda dict : dict['starttime'].strftime('%Y-%m-%d %T'))

        return newLst

    def getDelete(self):
        deleteLst = []
                
        for requestidStr, reqObj in self.reqDict.items():
            if reqObj['status'] == 'D':
                deleteLst.append(reqObj)
                
        deleteLst.sort(key=lambda dict : dict['requestid'])

        return deleteLst

    def getTimeout(self):
        return self.timeOut

    # series (text) - Name of series whose retention is wanted.
    # request (ReqTable::reqDict[requestidStr] object) - Contains the dbhost, dbport, dbname that identifies the
    #    database that contains the series.
    @staticmethod
    def getRetention(series, request, dbuser, log):
        # Who you gonna call...jsoc_info! A la:
        # jsoc_info op=series_struct ds=hmi.v_avg120
        # Ack - there is Stanford-specific stuff in jsoc_info. Instead, just get the retention from the db. We have the
        # host/port/dbname information from request.
        newSuRetention = -1
        ns, tab = series.split('.')

        try:
            # We need to get information from the DRMS database, which might not be the same database as the remote SUMS database.
            # The class variable ReqTable.rsConn is the connection to the remote SUMS database.
            with psycopg2.connect(database=request['dbname'], user=dbuser, host=request['dbhost'], port=request['dbport']) as conn:
                with conn.cursor() as cursor:
                    # We want the new-SU retention too, not the staging retention. So extract the bottom 15 bits.
                    cmd = "SELECT retention & x'00007FFF'::int AS retention FROM " + ns + ".drms_series WHERE seriesname ILIKE '" + series + "'"
                    log.writeInfo(['Obtaining new-SU retention for series ' + series + ': ' + cmd])

                    try:
                        cursor.execute(cmd)
                        newSuRetention = cursor.fetchone()[0]
                        log.writeInfo(['Retention is ' + str(newSuRetention) + '.'])
                    except psycopg2.Error as exc:
                        # Handle database-command errors.
                        raise GetRetentionException(exc.diag.message_primary)
            # The connection is read-only, so there is not need to commit a transaction.
        except psycopg2.DatabaseError as exc:
            # Closes the cursor and connection

            # Man, there is no way to get an error message from any exception object that will provide any information why
            # the connection failed.
            msg = 'Unable to connect to the database (no, I do not know why).'
            raise GetRetentionException(msg)

        return newSuRetention

# The site information is stored in a public database table at Stanford. It is accessible by all remote sites
# with the rssites.sh cgi. To obtain information about all sites, the rssites.sh cgi is called with no parameters.
# Otherwise, information about a single site can be obtained by by providing the site name to the 'site' argument.
# The information is stored in drms.rs_sites on hmidb.
class SiteTable:
    def __init__(self, url, log):
        self.url = url # rssites.sh URL
        self.log = log
        self.siteDict = {} # Keyed by name.
        self.siteMap = {} # Map from str(code) to name.

    def read(self):
        natt = 0
        while True:
            try:
                self.log.writeDebug([ 'opening URL ' + self.url ])
                with urllib.request.urlopen(self.url) as response:
                    siteInfoStr = response.read().decode('UTF-8')
                    natt = 0
                break
            except urllib.error.URLError as exc:
                # we want to try again, until we time-out
                natt += 1
                if natt > 2:
                    raise SitetableReadException("unable to access URL " + self.url)
                if type(exc.response) is str:
                    msg = exc.response
                else:
                    msg = ''
                log.writeWarning([ 'failed to access site-table URL (' + self.url + '); trying again.' ])
                time.sleep(1)

        # siteInfoStr is a string, that happens to be json.
        siteInfo = json.loads(siteInfoStr)
        
        if siteInfo['status'] != 'success':
            raise SitetableReadException("Failure calling cgi '" + self.url + "'.")

        # siteInfo is a dictionary, keyed by site name. Each dictionary entry is a dictionay, with two keys: code and baseurl.
        for asite, info in siteInfo.items():
            if asite == 'status':
                # Skip status.
                continue
            self.siteDict[asite] = {}
            self.siteDict[asite]['name'] = asite
            self.siteDict[asite]['code'] = info['code']
            self.siteDict[asite]['baseurl'] = info['baseurl']
            self.siteDict[asite]['cgi-supath'] = info['cgi-supath']
            self.siteMap[str(self.siteDict[asite]['code'])] = asite

            self.log.writeInfo([ 'Reading site info for ' + asite + ': code => ' + str(self.siteDict[asite]['code']) + ', baseurl => ' + self.siteDict[asite]['baseurl'] + ', cgi-supath => ' + self.siteDict[asite]['cgi-supath'] ])

    def tryRead(self):
        nAtts = 0
        while True:
            try:
                self.read()
                break
            except SitetableReadException:
                if nAtts > 10:
                    raise # Re-raise

            nAtts += 1
            time.sleep(1)

    @staticmethod
    def getCode(sunum):
        code = sunum >> 48
        if code & 0xC000 != 0:
            raise InvalidSunumException('The site-code value of SUNUM ' + sunum + ' is out of range (valid range is 0 to 16383).')

        return code

    def getBaseURL(self, sunum):
        code = SiteTable.getCode(sunum)
        
        if not str(code) in self.siteMap or not self.siteMap[str(code)]:
            raise UnknownSitecodeException('There is no site in the site table for code ' + str(code) + '.')
        
        name = self.siteMap[str(code)]
        url = self.siteDict[name]['baseurl']
        return url
        
    def getSuPathCGI(self, sunum):
        code = SiteTable.getCode(sunum)
        
        if not str(code) in self.siteMap or not self.siteMap[str(code)]:
            raise UnknownSitecodeException('There is no site in the site table for code ' + str(code) + '.')

        name = self.siteMap[str(code)]
        url = urllib.parse.urljoin(self.siteDict[name]['baseurl'], self.siteDict[name]['cgi-supath'])
        return url
    
    
class Chunker(object):
    def __init__(self, list, chSize):
        self.chunks = []
        iChunk = -1
        nElem = 1
        
        for elem in list:
            if iChunk == -1 or nElem % chSize == 0:
                iChunk += 1
                self.chunks.append([])
    
            self.chunks[iChunk].append(elem)
            nElem += 1
    
    def __iter__(self):
        return self.iterate()
    
    # Iterate through chunks.
    def iterate(self):
        i = 0
        while i < len(self.chunks):
            yield self.chunks[i]
            i += 1
            
class ScpWorker(threading.Thread):
    tList = [] # A list of running thread IDs.
    maxThreads = 64
    scpNeeded = threading.Event() # Event fired by main when a Downloader thread has changed the state of an SU from 'P' to 'W'
    scpCompleted = threading.Event()  # Event fired by ScpWorker when an ScpWorker thread had completed an SU download
    lock = threading.Lock() # Guard tList.
    
    def __init__(self, id, suTable, arguments, log):
        threading.Thread.__init__(self)
        self.id = id
        self.suTable = suTable
        self.tmpdir = arguments.tmpdir
        self.arguments = arguments
        self.log = log
        self.sdEvent = threading.Event()

    def run(self):
        gotSULock = False
        maxNumSUs = self.arguments.scpMaxNumSUs
        maxPayloadSUs = self.arguments.scpMaxPayload
        scpTimeOut = self.arguments.scpTimeOut
        doDownload = False
        lastDownloadTime = None
        
        self.log.writeDebug([ 'running ScpWorker ' + str(self.id) ])

        try:
            # the finally clause ensures that this thread will remove itself from the static list of threads
            # before it terminates
            while not self.sdEvent.isSet():
                self.sunums = []
                paths = []

                # Look for an SU whose status is 'W' (which means that a Downloader thread is requesting an ScpWorker thread perform a 
                # download for it). If we find one, set status to 'D' and download the SU. When the download is complete, set the status
                # to 'P' so the requesting Downloader thread can clean-up and set the status to 'C' for the main thread to handle.
                # acquired SU table lock
                (workingSUs, (user, host, port)) = self.suTable.getAndLockNextNWorkingSUs(self.id, maxNumSUs)
                try:
                    # no SU in workingSUs can have errored out at this point
                    susToDownload = []
                
                    doDownload = False
                
                    numSUs = len(workingSUs)
                    # No need to lock su for suSize - that gets set before ScpWorker sees the SU.
                    payload = sum([ su.suSize for su in workingSUs ])
                    currentTime = datetime.now(timezone.utc)

                    if numSUs > 0:
                        self.log.writeDebug([ 'examining ' + str(numSUs) + ' working SUs' ])
                        doDownload = payload > maxPayloadSUs or numSUs == maxNumSUs
                    
                        # Now we need to modify workingSUs if the payload would be too large, or there would be too many SUs for this scp.
                        payload = 0
                        numSUs = 0
                        for su in workingSUs:
                            if (payload + su.suSize > maxPayloadSUs and numSUs > 0) or numSUs + 1 > maxNumSUs:
                                break
                        
                            susToDownload.append(su)
                            payload += su.suSize
                            numSUs += 1                    
                    
                        self.log.writeDebug([ 'doDownload calc (payload, maxPayLoadSUs, numSUs, maxNumSUs, currentTime, lastDownloadTime, scpTimeOut): ' + str(payload) + ',' + str(maxPayloadSUs) + ',' + str(numSUs) + ',' + str(maxNumSUs) + ',' + str(currentTime) + ',' + str(lastDownloadTime) + ',' + str(scpTimeOut) + ')' ])

                        if doDownload:                            
                            # Set lastDownloadTime. We will keep setting this, until we complete the scp - that is the best place to
                            # set it, but errors before that could cause us to not set it.
                            lastDownloadTime = datetime.now(timezone.utc)
                        else:
                            # Now it could be that at least one SU has been waiting a long time. If that is true, then 
                            # do a download.
                            if lastDownloadTime is None:
                                lastDownloadTime = datetime.now(timezone.utc)
                            
                            if currentTime - lastDownloadTime > scpTimeOut:
                                doDownload = True
                    else:
                        doDownload = False
                
                    if doDownload:
                        self.log.writeInfo([ 'ScpWorker ' + str(self.id) + ' - Time for a download! Collected ' + str(len(susToDownload)) + ' for download. Payload is ' + str(payload) + '.' ])
                    
                        # all SUs are locked so no other thread modifies the SUs while we are processing the download
                        for su in susToDownload:                        
                            self.sunums.append(su.sunum)                            
                            serverPath = su.worker.path
                            if not serverPath:
                                raise ScpSUException('Server SU path is not known.')
                            paths.append(serverPath)
                    
                            # Set status to D to prevent another ScpWorker from processing the download. Must do this
                            # before the SU Table lock is released, otherwise another ScpWorker could grab the same
                            # SU that this ScpWorker just grabbed.
                            self.log.writeInfo([ 'ScpWorker setting SU ' + str(su.sunum) + ' status to D.' ])
                            su.setStatus('D', None)
                                
                        if len(self.sunums) == 0:
                            doDownload = False
                finally:
                    # release SU locks
                    for su in workingSUs:
                        su.releaseLock()

                if doDownload:
                    try:
                        self.log.writeInfo([ 'ScpWorker ' + str(self.id) + ' downloading SUs ' + ','.join([ str(asunum) for asunum in self.sunums ]) + '.' ])                        

                        # Don't forget to make the temporary directory first.
                        if not os.path.exists(self.tmpdir):
                            self.log.writeInfo([ 'Creating temporary download directory ' + self.tmpdir + '.' ])
                            os.mkdir(self.tmpdir)

                        lastDownloadTime = datetime.now(timezone.utc)
                    
                        cmd = 'scp -r -P ' + port + ' ' + user + '@' + host + ':"' + ' '.join(paths) + '" ' + self.tmpdir
                        self.log.writeInfo([ 'ScpWorker ' + str(self.id) + ' running ' + cmd + '.' ])
                        try:
                            # check_call(cmdList)
                            # The scp process will inherit stdin, stdout, and stderr from this script.
                            # proc = Popen(cmdList, stdout=PIPE, stderr=PIPE)
                            proc = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE, start_new_session=True)
                        except OSError as exc:
                            import traceback
                            self.log.writeError([ traceback.format_exc(5) ])
                            raise ScpSUException('Cannot run scp command.')
                        except ValueError as exc:
                            import traceback
                            self.log.writeError([ traceback.format_exc(5) ])
                            raise ScpSUException('scp command called with invalid arguments.')

                        # Poll for completion
                        while True:
                            # The Python documentation is confusing at best. I think we have to look at the proc.returncode attribute
                            # to determine if the child process has completed. None means it hasn't. If the value is not None, then 
                            # the child process has terminated, and the value is the child process's return code.
                            lastDownloadTime = datetime.now(timezone.utc)
                        
                            proc.poll()
                            if proc.returncode is not None:                        
                                # The scp has completed.
                                out, err = proc.communicate()
                                self.log.writeInfo([ 'ScpWorker ' + str(self.id) + ' - scp process exited with return code ' + str(proc.returncode) + '.' ])
                                lastDownloadTime = datetime.now(timezone.utc)
                                if proc.returncode != 0:
                                    msg = 'Command "' + cmd + '" returned non-zero status code ' + str(proc.returncode) + '.'
                                    if err is not None:
                                        self.log.writeError([ 'scp stderr msg: ' + err.decode('UTF8') ])
                                    raise ScpSUException(msg)
                                break

                            atLeastOneGoodSU = False

                            sus, missingSunums = self.suTable.getAndLockSUs(sunums=self.sunums, filter=SuTable.removeGhosts)
                            try:
                                for su in sus:
                                    if su.status == 'D':                                
                                        # Check for SU download time-out. We keep doing the download, unless all SUs have timed out.
                                        timeNow = datetime.now(su.starttime.tzinfo)
                                        if timeNow > su.starttime + self.suTable.getTimeout():
                                            self.log.writeInfo([ 'download of SUNUM ' + str(su.sunum) + ' timed-out' ])
                                            self.log.writeInfo([ 'ScpWorker setting SU ' + str(su.sunum) + ' status to E (for time-out)' ])
                                            su.setStatus('E', 'download timed-out')
                                        else:
                                            atLeastOneGoodSU = True
                            finally:
                                for su in sus:
                                    su.releaseLock()

                            if self.sdEvent.isSet():
                                # Kill the download and also exit the ScpWorker thread.
                                self.log.writeDebug([ 'ScpWorker ' + str(self.id) + ' received sdEvent. Killing scp.'])
                                proc.kill()

                                try:
                                    proc.communicate(timeout=4)
                                    self.log.writeInfo([ 'Successfully killed ScpWorker ' + str(self.id) + ' download.' ])
                                except TimeoutExpired:
                                    self.log.writeWarning([ 'Unable to kill scp for ScpWorker ' + str(self.id) + '.' ])
                                
                                raise ShutDownException('ScpWorker ' + str(self.id) + ' is observing the global shutdown and exiting now.')
                        
                            if not atLeastOneGoodSU:
                                # Go on to the next set of requested SUs. Don't exit the ScpWorker thread.
                                proc.kill()
                                raise NoSusForDlException('The downloads of all SUs in the payload have been either canceled or have timed-out.')

                            time.sleep(1) # In scp process poll loop.
                            # end proc-wait loop

                        sus, missingSunums = self.suTable.getAndLockSUs(sunums=self.sunums, filter=SuTable.removeGhosts)
                        try:
                            for su in sus:
                                if su.status == 'D':
                                    # the download for this SU has not been canceled
                                    self.log.writeInfo([ 'ScpWorker setting SU ' + str(su.sunum) + ' status to P' ])
                                    su.setStatus('P', None)
                        finally:
                            for su in sus:
                                su.releaseLock()

                        # Flush the change to disk.
                        self.log.writeDebug([ 'ScpWorker ' + str(self.id) + ' updating SU table' ])
                        self.suTable.updateDbAndCommit(self.sunums)

                        self.log.writeInfo([ 'ScpWorker ' + str(self.id) + ' - scp command succeeded for SUs ' + ','.join([ str(asunum) for asunum in self.sunums ]) ])
                    except ScpSUException as exc:                        
                        # These errors should not cause the ScpWorker thread to exit. Set status to 'E'.
                        self.log.writeError([ exc.args[0] ])
                        
                        sus, missingSunums = self.suTable.getAndLockSUs(sunums=self.sunums, filter=SuTable.removeGhosts)
                        try:
                            for su in sus:
                                if su.getStatus() != 'E':
                                    self.log.writeInfo([ 'ScpWorker setting SU ' + str(su.sunum) + ' status to E' ])
                                    su.setStatus('E', exc.args[0])
                        finally:
                            for su in sus:
                                su.releaseLock()
                    except NoSusForDlException as exc:
                        # The SU statuses have already been updated with a non-P status.
                        self.log.writeInfo([ exc.args[0] ])
                    except ShutDownException as exc:
                        self.log.writeInfo([ exc.args[0] ])

                        sus, missingSunums = self.suTable.getAndLockSUs(sunums=self.sunums, filter=SuTable.removeGhosts)
                        try:
                            for su in sus:
                                if su.status == 'D':
                                    # The ScpWorker was in the middle of downloading the SU when rsumds.py was shut-down.
                                    # Set the status back to 'W' - we basically want to pretend that the download
                                    # never started.                                
                                    self.log.writeInfo([ 'Shutting down - ScpWorker ' + str(self.id) +' setting SU ' + str(su.sunum) + ' status to W. Daemon will initiate SU download upon restart.' ])
                                    su.setStatus('W', None)
                                else:
                                    # If the status was P, then the download for this SU already succeeded. Or the Downloader
                                    # saw the shut-down signal, and set that status back to P. If it was W, then 
                                    # let the Downloader handle cleaning up - it will set the status back to P so the download
                                    # will be attempted again during the next run of rsumsd.py.
                                    pass
                        finally:
                            for su in sus:
                                su.releaseLock()

                    # Move on to the next scp without sleeping. Wake up a Downloader.
                    ScpWorker.scpCompleted.set()
                    # Clear event so that main will block the next time it calls wait.
                    ScpWorker.scpCompleted.clear()
                else:
                    # There were fewer than scpBatchSize requests for an SU download. Wait for more with ScpWorker.scpNeeded.wait().
                    # Set a timeout so we can gracefully exit if the shutdown event has been triggered (just in case this thread
                    # blocks on wait - when the shutdown happens, the scpNeeded event will be triggered, however).
                
                    # It could be the case that the last Downloader has fired scpNeeded, but there still aren't enough SUs
                    # to trigger a download. If that is the case, then we must let scpNeeded time-out before we do check to see
                    # if it is time to do a download. If the scpNeeded.wait() timeout is long, then we won't check for the 
                    # ScpWorker timeout for a long time, even though the ScpWorker timeout might be short.
                    timeOutToUse = min(scpTimeOut, timedelta(seconds=10))
                
                    try:
                        ScpWorker.scpNeeded.wait(timeOutToUse.total_seconds())
                    except RuntimeError:
                        pass
        finally:
            # This thread is about to terminate. 
            # We need to check the class tList variable to update it, so we need to acquire the lock.
            try:
                ScpWorker.lock.acquire()
                ScpWorker.tList.remove(self) # This thread is no longer one of the running threads.
                self.log.writeInfo([ 'Scp Worker (ID ' +  str(self.id) + ') halted.' ])            
            finally:
                ScpWorker.lock.release()        
        
    # Called from main thread
    def stop(self):
        self.log.writeInfo([ 'Stopping ScpWorker ' + str(self.id) + ' - (it may take 10 seconds for the ScpWorker to stop).' ])
        
        # Fire event to stop thread.
        self.sdEvent.set()
        self.log.writeDebug([ 'Set sdEvent in ScpWorker ' + str(self.id) + '.' ])
        
        # Fire scpNeeded event so that the ScpWorker thread will not block on wait.
        ScpWorker.scpNeeded.set()
    
    @staticmethod
    def newThread(id, suTable, arguments, log):
        worker = ScpWorker(id, suTable, arguments, log)
        ScpWorker.tList.append(worker)
        worker.start()

# Downloads a single SU. Ingests it into SUMs (SUMS allows to ingestion of a single SU at a time only.). Updates
# the SU table status for that SU.
# The main thread sets the SU status to 'P'. The Downloader thread sets that status to 'W' to request a ScpWorker
# thread to download the SU. The ScpWorker thread sets the status to 'D' while it is processing the download. When
# the download is complete, the ScpWorker thread sets the status back to 'P'.
class Downloader(threading.Thread):
    sumsConn = None
    sumsDbLock = None # Since all Downloader threads share the same connection, their cursors on these connections
                      # are not isolated. Use a lock to ensure that only one Download is modifying SUMS at one time.

    tList = [] # A list of running thread IDs.
    maxThreads = 16 # Default. Can be overriden with the Downloader.setMaxThreads() method.
    eventMaxThreads = threading.Event() # Event fired when the number of threads decreases.
    lock = threading.Lock() # Guard tList.

    def __init__(self, su, path, sus, scpUser, scpHost, scpPort, binPath, arguments, log):
        threading.Thread.__init__(self)
        self.su = su
        self.sunum = su.sunum
        self.path = path
        self.suTable = sus
        self.scpUser = scpUser # the linux user that has access to the SU
        self.scpHost = scpHost # the machine hosting the SU
        self.scpPort = scpPort # the port on the machine hosting the SU
        self.binPath = binPath
        self.tmpdir = arguments.tmpdir
        self.arguments = arguments
        self.log = log
        self.sdEvent = threading.Event()

    def run(self):
        # Sub-out the download to an ScpWorker instance. To do that, set the SU status to 'W'. The ScpWorker
        # that is used will set the status to 'D' so that no other ScpWorker attempt to download the SU 
        # as well. When the ScpWorker completes the download, it will set the status to 'P' again.        
        suDlPath = None
        sudir = None
        
        setErrorStatus = False
        cleanUpSUDir = False
        gotSULock = False
        
        retentionCached = None
        seriesCached = None
        
        try:
            # the finally clause will remove this thread from tList; that must happen, else we could 
            # hang on shut-down
            try:
                # this SU cannot have been removed from the SU Table at this point; that can only happen
                # after this thread sets the SU status to C or E; so, there is no chance that the SU has been orphaned and
                # there is no need to check for that
                self.su.acquireLock()
                try:                    
                    suStatus = self.su.getStatus()
                    retentionCached = self.su.getRetention()
                    seriesCached = self.su.getSeries()
                    
                    self.log.writeInfo([ 'Downloader is running for SU ' + str(self.sunum) ])

                    if suStatus != 'P' and suStatus != 'W':
                        raise DownloaderException('SU ' + str(self.sunum) + 'is not pending')
                
                    if suStatus != 'W':
                        # Let an ScpWorker thread handle the download. We do that by setting the status to 'W'.
                        # Upon recovery from a daemon shutdown, we may start certain SUs in the W state, in which
                        # case we do not need to set the status to W.
                        self.log.writeInfo([ 'Setting SU ' + str(self.sunum) + " status to W." ])
                
                        # The ScpWorker learns of the source path, the SU size, the scp user, etc., from su.worker.
                        self.su.setStatus('W', None)
                
                    suDlPath = os.path.join(self.tmpdir, 'D' + str(self.sunum))
                finally:
                    self.su.releaseLock()

                # wake up an ScpWorker
                ScpWorker.scpNeeded.set()
                # clear event so that main will block the next time it calls wait
                ScpWorker.scpNeeded.clear()

                # wait for the ScpWorker thread to finish downloading the SU (look for a 'P' status)
                while not self.sdEvent.isSet():
                    self.su.acquireLock()
                    try:
                        suStatus = self.su.getStatus()

                        # Check for download error or completion. Call get() again, since the SU record could have been deleted (due to
                        # abnormal execution).
                        if suStatus == 'W' or suStatus == 'D':
                            # ScpWorking is still performing the download; or the ScpWorker died; check for a time-out for this
                            # SU, and if a time-out has happened, then raise.
                            timeNow = datetime.now(self.su.starttime.tzinfo)
                            if timeNow > self.su.starttime + self.suTable.getTimeout():
                                self.log.writeInfo([ 'Download of SU ' + str(self.sunum) + ' timed-out.' ])
                                self.log.writeInfo([ 'Downloader setting SU ' + str(self.sunum) + ' status to E (for time-out).' ])
                                self.su.setStatus('E', 'Download timed-out.')
                                raise DownloaderException('The download of SU ' + str(self.sunum) + ' timed-out.')
                        elif suStatus == 'P':
                            # ScpDownloader is done performing the download.
                            break
                        else:
                            raise DownloaderException('The download of SU ' + str(self.sunum) + ' errored-out.')
                    finally:
                        self.su.releaseLock()

                    # Wait for ANY ScpWorker to complete a download. The downloaded SU might not be the one needed by
                    # this Downloader thread. If a shutdown is happening, then all ScpWorker threads should fire the 
                    # scpCompleted event, releasing all blocking Downloader threads. But just in case, set a 10-second 
                    # timeout - this raises if the timeout occurs. It is here that we will catch other errors too, like
                    # a download time-out for this SU.

                    # use a time-out; during shut-down, we shut-down the ScpWorkers first, so no ScpWorker will call set()
                    toDidNotOccur = ScpWorker.scpCompleted.wait(10)
            
                # The download has completed (it is in self.tmpdir), or a shut-down is being observed.
            
                # If a shut-down is being observed, then the SU status must be W (waiting for scp thread) or P (scp download complete) or 
                # E (error). If there is no shut-down happening, then the status must be P or E.
                if self.sdEvent.isSet():
                    self.log.writeInfo([ 'shutdown happening while downloading SU ' + str(self.sunum) ])
                    # If status == 'P', then go ahead and save the SU in SUMS. Below, set status to C. This SU was successfully downloaded.
                    # If status == 'E', the download failed, and if status == 'W', then we pretend the download never started (although
                    # it could have started, but got canceled by the shutdown).
                    self.su.acquireLock()
                    try:
                        if self.su.getStatus() != 'P':
                            # the download did not happen, so skip the SUMS ingestion
                            raise ShutDownException('downloader thread for SU ' + str(self.sunum) + ' is observing the global shutdown and exiting now')
                    finally:
                        self.su.releaseLock()

                # At this point, we need to allocate (mkdir) a new SU directory, move the downloaded SU content into this SUDIR, 
                # then commit the newly created SU into SUMS. The previous incarnations of remote-sums-type code all used the SUMS
                # API to achieve the first and last steps. To use the SUMS API, code need to be written in C and it needs to link
                # to the SUMS library. The first remote-sums code did this by running a DRMS module, wherein all three steps were
                # performed. The JMD uses vso_sum_alloc (a DRMS module with access to the SUMS API) to allocate the SU directory, 
                # then it copies the downloaded SU content into the directory, and then it calls vso_sum_put to commit the SU.
                # However, at a high rate of download, the SUMS API seems to have problems, resulting in the vso_sum_alloc and/or
                # vso_sum_put calls to hang for minutes. We have not been able to track down this issue, but it appears to have
                # something to do with either saturation of socket resources and/or RPC resources and/or SUMS queues.
            
                # This script by-passes SUMS altogether to avoid the issues with SUMS and/or DRMS modules hanging under higher load.
                # The first step in by-passing SUMS is to perform the equivalent of the SUM_open() API call. Then we can call the
                # equivalent of the SUM_alloc2() call to allocate a new SU directory, followed by the copying of the download SU
                # content into this new SU directory. Then we can call the equivalent of the SUM_put() API call to commit the 
                # SU, and then we can call the equivalent of the SUM_close() API call to end the SUMS session.
            
                # We need to connect to the SUMS database before we can modify SUMS objects.
                # The DB transaction is NOT in autocommit mode.
                #
                # Do not put a lock around these operations. It takes about 5 to 8 seconds to complete these SUMS DB 
                # operations. If every thread has to acquire and hold this lock for 5 to 8 seconds, throughput will
                # suffer. 
                #
                # And there is no reason to lock these operations. Each Downloader thread
                # operates on a unique set of rows in the SUMS DB tables being modified. The original
                # SUMS maintained a single connection to the SUMS DB. In response to each API function call,
                # SUMS would manipulate the SUMS DB and then it would commit the change. Since each
                # SUMS client makes multiple SUMS API function calls during its run, and SUMS responds to each
                # request as it arrives in its 'inbox', without any regard to sorting by client,
                # the DB manipulations made by clients are interleaved. Therefore, there is no need
                # for one Downloader thread to perform all DB manipulations without interruption from
                # other Downloader threads.
                #
                # The various cursors held by different Downloaders are not isolated (they operate within the same transaction
                # across all Downloader threads), so the following DB manipulations can be interrupted. If an error 
                # happens somewhere in this chain of events, we need to undo the manipulations performed before the
                # error occurred.
                #
                # PG and psycopg2 do not allow multiple concurrent transactions in a single connection. So, I guess
                # we have to serialize each manipulation (by putting a lock around each one).
                with Downloader.sumsConn.cursor() as cursor:
                    allOK = False
                    openDone = False
                    alloc2Done = False
                    putDone = False
                    closeDone = False
                
                    try: 
                        # Put all of this in one transaction. If everything is good, commit the transaction. If an 
                        # exception occurs, roll back.
            
                        ##### SUM_open() port #####
                        try:
                            gotSumsDbLock = Downloader.sumsDbLock.acquire()
                        
                            # This increments the sequence that supplies the sumid and inserts that sumid into the sum_open table.
                            cmd = "SELECT NEXTVAL('public.sum_seq')"
                            cursor.execute(cmd)
                            records = cursor.fetchall()
                            if len(records) != 1:
                                raise SumsAPIException('Unexpected response when fetching sumid from sequence.')
                
                            sumid = records[0][0]

                            currentTimeStr = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            cmd = 'INSERT INTO public.sum_open(sumid, open_date) VALUES (' + str(sumid) + ", '" + currentTimeStr + "')"
                            cursor.execute(cmd)
                            Downloader.sumsConn.commit()
                        finally:
                            if gotSumsDbLock:
                                Downloader.sumsDbLock.release()
                        ##### SUM_open() port - end #####
                        openDone = True
                        self.log.writeInfo([ 'Successfully called SUM_open() port for SU ' +  str(self.sunum) ])
                        self.log.writeInfo([ 'sumid is ' + str(sumid) + '.' ])
    
                        ##### SUM_alloc2() port #####
                        try:
                            gotSumsDbLock = Downloader.sumsDbLock.acquire()

                            #   First, find a partition that has enough available space for the size of the SU to be downloaded.
                            cmd = 'SELECT PARTN_NAME FROM public.sum_partn_avail WHERE AVAIL_BYTES >= 1024 AND PDS_SET_NUM = 0'
                            cursor.execute(cmd)
                            records = cursor.fetchall()
                            if len(records) < 1:
                                raise SumsAPIException('Cannot allocate a new Storage Unit in SUMS - out of space.')
            
                            partitions = []
                            for rec in records:
                                partitions.append(rec[0])
            
                            #   Second, randomly choose one of the partitions to put the new SU into. We want to spread the write load over available 
                            #   partitions.
                            randIndex = random.randint(0, len(partitions) - 1)
                            partition = partitions[randIndex]
                            sudir = os.path.join(partition, 'D' + str(self.sunum))
                            os.mkdir(sudir)
                            os.chmod(sudir, 0O2755)

                            #   Third, insert a record into the sum_partn_alloc table for this SU. status is DARW, which is 1. effective_date is "0".
                            cmd = "INSERT INTO public.sum_partn_alloc(wd, sumid, status, bytes, effective_date, archive_substatus, group_id, safe_id, ds_index) VALUES ('" + sudir + "', '" + str(sumid) + "', 1, 1024, '0', 0, 0, 0, 0)"
                            cursor.execute(cmd)
                            Downloader.sumsConn.commit()
                        finally:
                            if gotSumsDbLock:
                                Downloader.sumsDbLock.release()
                        ##### SUM_alloc2() port - end #####
                        alloc2Done = True
                        self.log.writeInfo([ 'Successfully called SUM_alloc2() for SU ' +  str(self.sunum) ])
            
                        #    Fourth, move the downloaded SU files into the chosen SUMS partition. SUM_alloc2() code calls mkdir, so we cannot
                        #    move the top-level D___ directory into the SUMS partition. Instead we have to move, recursively,  all files and directories in 
                        #    the downloaded D___ directory into the SUMS D___ directory.
                        files = os.listdir(suDlPath)
                        self.log.writeInfo([ 'Moving downloaded SU content from ' + suDlPath + ' into allocated SU (' + sudir  + ').' ])

                        try:
                            for afile in files:
                                src = os.path.join(suDlPath, afile)
                                self.log.writeDebug([ 'moving ' + src + ' to ' + sudir ])
                                shutil.move(src, sudir)
                        except shutil.Error as exc: 
                            import traceback
                            self.log.writeError([ traceback.format_exc(5) ])
                            raise RSIOException('Unable to move SU file ' + afile + ' into SUdir ' + sudir + '.')

                        self.log.writeInfo([ 'Move of SU ' + str(self.sunum) + ' content succeeded.' ])
            
                        ##### SUM_put() port #####
                        try:
                            gotSumsDbLock = Downloader.sumsDbLock.acquire()

                            # The original SUM_put() call called "chmown" to change the ownership of the
                            # files in the SU dir to the SUM_MANAGER. However, this is not necessary since rsumsd.py is run by the 
                            # SUM_MANAGER.
            
                            #   First, chmod all directories to 0755. All regular files get their user/group/other read enabled, and their
                            #   user write enabled, and their group and other write disabled.
                            for root, dirs, files in os.walk(sudir):
                                for adir in dirs:
                                    fullPath = os.path.join(root, adir)
                                    os.chmod(fullPath, 0O0755)
                                for afile in files:
                                    fullPath = os.path.join(root, afile)
                                    st = os.stat(fullPath)
                                    newMod = st.st_mode | stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH | stat.S_IWUSR & ~stat.S_IWGRP & ~stat.S_IWOTH
                                    os.chmod(fullPath, newMod)
                    
                            #   Second, update SUMS sum_main database table - Calculate SU dir number of bytes, set online status to 'Y', set archstatus to 'N', 
                            #   set offsiteack to 'N', set dsname to seriesname, set storagegroup to tapegroup, set storageset to tapegroup / 10000,
                            #   set username to getenv('USER') or nouser if no USER env, set mode to TEMP + TOUCH, set apstatus: if SUMS_TAPE_AVAILABLE ==>
                            #   DAAP (4), else DADP (2), set archsub ==> DAAEDDP (32), set effective_date to tdays in the future (with format "%04d%02d%02d%02d%02d").
                            #   Insert all of this into sum_main.
                            numBytes = os.path.getsize(sudir) + sum([ os.path.getsize(fullPath) for fullPath in [ os.path.join(root, afile) for root, dirs, files in os.walk(sudir) for afile in files ] ]) + sum([ os.path.getsize(fullPath) for fullPath in [ os.path.join(root, adir) for root, dirs, files in os.walk(sudir) for adir in dirs ] ])
                            if self.arguments.tapesysexists:
                                apStatus = 4 # DAAP
                            else:
                                apStatus = 2 # DADP

                            createDate = datetime.now()
                            createDateStr = createDate.strftime('%Y-%m-%d %H:%M:%S')
                            expDate = createDate + timedelta(days=retentionCached)
                            effDate = expDate.strftime('%Y%m%d%H%M')

                            # storage_group is the tape group. It should come from the series definition, but remote sites have been using 0 for years.            
                            cmd = "INSERT INTO public.sum_main(online_loc, online_status, archive_status, offsite_ack, history_comment, owning_series, storage_group, storage_set, bytes, ds_index, create_sumid, creat_date, access_date, username) VALUES ('" + sudir + "', 'Y', 'N', 'N', '', '" + seriesCached + "', 0, 0, " + str(numBytes) + ', ' + str(self.sunum) + ', ' + str(sumid) + ", '" + createDateStr + "', '" + createDateStr + "', '" + os.getenv('USER', 'nouser') + "')"
                            cursor.execute(cmd)
            
                            self.log.writeInfo([ 'Successfully inserted record into sum_main for SU ' + str(self.sunum) + '.' ])

                            #    Third, update SUMS sum_partn_alloc table - Insert a new row into sum_partn_alloc for this SU. The SUM_alloc2() port will result in
                            #    a row in sum_partn_alloc with a ds_index of 0, which does not make sense to me. But the SUM_close() port will delete
                            #    that row. By the time this thread terminates, there will be only a single row for this SU in sum_partn_alloc. substatus is DAAEDDP (32).
                            #    But first, delete any existing DADP (delete pending) rows for this sunum if the status of the SU for the new row is DADP.
                            if apStatus == 2:
                                # We do this simply to ensure that we do not have two sum_partn_alloc records with status DADP (delete pending).
                                cmd = 'DELETE FROM public.sum_partn_alloc WHERE ds_index = ' + str(self.sunum) + ' AND STATUS = 2'
                                cursor.execute(cmd)
                                self.log.writeInfo([ 'Successfully deleted old DADP record from sum_partn_alloc for SU ' + str(self.sunum) + '.' ])
            
                            cmd = "INSERT INTO public.sum_partn_alloc(wd, sumid, status, bytes, effective_date, archive_substatus, group_id, safe_id, ds_index) VALUES ('" + sudir + "', " + str(sumid) + ', ' + str(apStatus) + ', ' + str(numBytes) + ", '" + effDate + "', 32, 0, 0, " + str(self.sunum) + ')'
                            cursor.execute(cmd)
                            self.log.writeInfo([ 'Successfully inserted record into sum_partn_alloc for SU ' + str(self.sunum) + '.' ])
                            Downloader.sumsConn.commit()
                        finally:
                            if gotSumsDbLock:
                                Downloader.sumsDbLock.release()
                        ##### SUM_put() port - end #####
                        putDone = True
                        self.log.writeInfo([ 'Successfully called SUM_put() for SU ' + str(self.sunum) ])
            
                        ##### SUM_close() port #####
                        try:
                            gotSumsDbLock = Downloader.sumsDbLock.acquire()

                            # Delete sum_partn_alloc records for read-only partitions (status == 8) and read-write partitions (status == 1).
                            cmd = 'DELETE FROM public.sum_partn_alloc WHERE sumid = ' + str(sumid) + ' AND (status = 8 OR status = 1)'
                            cursor.execute(cmd)
                            self.log.writeInfo([ 'Successfully deleted read-only and read-write records from sum_partn_alloc for SU ' + str(self.sunum) + '.' ])
            
                            # Delete the temporary ds_index = 0 records created during the SUM_put() port. I still do not know why this record
                            # was created in the first place.
                            cmd = 'DELETE FROM public.sum_open WHERE sumid = ' + str(sumid)
                            cursor.execute(cmd)
                            Downloader.sumsConn.commit()
                        finally:
                            if gotSumsDbLock:
                                Downloader.sumsDbLock.release()

                        ##### SUM_close() port - end #####
                        closeDone = True
                        self.log.writeInfo([ 'Successfully called SUM_close() for SU ' + str(self.sunum) ])
                        self.log.writeInfo([ 'Successfully deleted temporary (ds_index == 0) records from sum_open for SU ' + str(self.sunum) + '.' ])
                    
                        allOK = True
                    except psycopg2.Error as exc:
                        # Handle database-command errors. These are all due to problems communicating with the SUMS db.
            
                        # Clean-up
                        if os.path.exists(sudir):
                            shutil.rmtree(sudir)
                        if os.path.exists(suDlPath):
                            shutil.rmtree(suDlPath)
                        raise SumsAPIException(exc.diag.message_primary + ': ' + cmd + '.') 
                    except Exception as exc:
                        # Clean-up
                        if os.path.exists(sudir):
                            shutil.rmtree(sudir)
                        if os.path.exists(suDlPath):
                            shutil.rmtree(suDlPath)
                        raise
                    finally:
                        # the cursor still exists
                        if not allOK:
                            # undo manipulations that were successfully performed; these could raise, in which
                            # case we've done the best we can; let the enclosing exception handler set the
                            # download status for this SU to error
                            if openDone:
                                cmd = 'DELETE FROM public.sum_open WHERE sumid = ' + str(sumid)
                                cursor.execute(cmd)
                            if alloc2Done:
                                cmd = "DELETE FROM public.sum_partn_alloc WHERE wd = '" + sudir + "'"
                                cursor.execute(cmd)
                            if putDone:
                                # if putDone, then alloc2Done, so the record was already deleted from sum_partn_alloc; don't
                                # do that here.
                                cmd = 'DELETE FROM public.sum_main WHERE ds_index = ' + str(self.sunum)
                                cursor.execute(cmd)
                            if closeDone:
                                # nothing to clean up if close succeeds
                                pass

                # Remove temporary directory.
                self.log.writeInfo([ 'Removing empty temporary download directory ' + suDlPath + '.' ])
                try:
                    if os.path.exists(suDlPath):
                        shutil.rmtree(suDlPath)
                except OSError as exc:
                    raise RSIOException(exc.strerror)
           
                self.log.writeInfo([ 'Removal of temporary directory ' + suDlPath + ' succeeded.' ])
 
                self.su.acquireLock()
                try:                    
                    self.log.writeDebug([ 'downloader for SU ' + str(self.sunum) + ' terminating; unsetting worker' ])
                    self.su.removeWorker()
                    self.log.writeInfo([ 'Setting SU ' + str(self.sunum) + ' status to complete.' ])
                    self.su.setStatus('C', None)
                finally:
                    self.su.releaseLock()
            except ShutDownException:
                # If the status is W, then either the download never happened, or it got canceled part-way through and
                # the ScpWorker thread set its status back to W. The daemon will resume upon restart, starting
                # Downloader threads for each SU that has status W.
                msg = None
                setErrorStatus = False
                cleanUpSUDir = True
            except RemoteSumsException as exc:
                import traceback
            
                msg = [ exc.args[0], traceback.format_exc(5) ]
                setErrorStatus = True
                cleanUpSUDir = True
            except Exception as exc:
                # catch all remaining exceptions
                import traceback

                msg = [ 'Unknown exception.', traceback.format_exc(5) ]
                setErrorStatus = True
                cleanUpSUDir = True

            # if this code raises, the outer-most finally clause will execute, removing this thread from the
            # tList
            if setErrorStatus:
                # set SU status to E
                self.su.acquireLock()
                try:
                    self.log.writeError([ 'Setting SU ' + str(self.sunum) + ' status to error.' ])
                    self.log.writeError(msg)
                    self.log.writeDebug([ 'downloader for SU ' + str(self.sunum) + ' terminating; unsetting worker' ])
                    self.su.removeWorker()
                    self.su.setStatus('E', msg[0]) # do not put traceback strings into DB (weird chars in tracebacks mess things up)
                finally:
                    self.su.releaseLock()

            if cleanUpSUDir:                
                # Must clean-up SU dir and the downloaded files.
                if sudir and os.path.exists(sudir):
                    shutil.rmtree(sudir)
                if suDlPath and os.path.exists(suDlPath):
                    shutil.rmtree(suDlPath)
            
            # Update SU table (write-out status, error or success, to the DB).
            # This is a no-op if no SUs were actually modified.
            self.suTable.updateDbAndCommit([ self.sunum ])
        finally:
            # This thread is about to terminate. 
            # We need to check the class tList variable to update it, so we need to acquire the lock.
            try:
                Downloader.lock.acquire()
                Downloader.tList.remove(self) # This thread is no longer one of the running threads.
                self.log.writeInfo([ 'Downloader (SUNUM ' + str(self.sunum) + ') halted.' ])     
                # Use <= because we don't know if we were able to reach Downloader.maxThreads thread running due 
                # to system-resource limitations.
                if len(Downloader.tList) <= Downloader.maxThreads - 1:
                    # Fire event so that main thread can add new SUs to the download queue.
                    self.log.writeDebug([ 'OK to start new download threads.' ])
                    Downloader.eventMaxThreads.set()
                    # Clear event so that main will block the next time it calls wait.
                    Downloader.eventMaxThreads.clear()
            finally:
                Downloader.lock.release()

    # called from main thread only
    def stop(self):
        self.log.writeInfo([ 'Stopping Downloader (SUNUM ' + str(self.sunum) + '). It may take 10 seconds for Downloader to stop.' ])
        
        # Fire event to stop thread.
        self.sdEvent.set()
        
        # Fire scpCompleted event so that the Downloader thread will not block on wait.
        ScpWorker.scpCompleted.set()

    # the main thread is holding the SU lock
    # must acquire Downloader lock BEFORE calling newThread() since newThread() will append to tList (the Downloader threads will delete from tList as they complete).
    @staticmethod
    def newThread(su, path, suTable, scpUser, scpHost, scpPort, binPath, arguments, log):
        dl = Downloader(su, path, suTable, scpUser, scpHost, scpPort, binPath, arguments, log)
        su.setWorker(dl)
        dl.tList.append(dl)

        try:
            dl.start()
        except RuntimeError:
            # cannot start a new thread, so rollback and re-raise so the calling thread can handle the error
            Downloader.tList.remove(dl)
            su.removeWorker()

            raise StartThreadException('cannot start a new Downloader thread due to system resource limitations')

    @classmethod
    def setMaxThreads(cls, maxThreads):
        cls.maxThreads = maxThreads

class ProviderPoller(threading.Thread):
    tList = [] # A list of running thread IDs.
    maxThreads = 32 
    eventMaxThreads = threading.Event() # Event fired when the number of threads decreases.
    lock = threading.Lock() # Guard tList.
    
    def __init__(self, cgi, requestID, sunums, sus, reqTable, request, dbUser, binPath, arguments, log):
        threading.Thread.__init__(self)
        self.cgi = cgi
        self.requestID = requestID # The provider request ID.
        self.sunums = sunums # The sunums requested from provider (under request ID self.requestID).
        self.suTable = sus
        self.reqTable = reqTable
        self.request = request # The ReqTable::reqdict[requestidStr] object (the row in the request table)
        self.dbUser = dbUser
        self.binPath = binPath
        self.arguments = arguments
        self.log = log
        self.startTime = datetime.now(timezone.utc) # Cool bug. This used to be self.start. But the parent object has a method named 'start' The effect was to override the method with an attribute.
        self.timeOut = sus.getTimeout()
        self.sdEvent = threading.Event()

    def run(self):
        values = {'requestid' : self.requestID, 'sunums' : 'none'}
        data = urllib.parse.urlencode(values)
        errMsg = None
        timeToLog = True
        loopN = 0
        
        try:
            # Set the in-memory ProviderPoller flag for all sunums in this request.
            for asunum in self.sunums:
                su = self.suTable.getAndLockSU(sunum=asunum, filter=SuTable.removeGhosts)

                if su:
                    try:
                        su.setPolling(True)
                    finally:
                        su.releaseLock()
                else:
                    # the sunum might no longer be in the SU table, just skip it for the rest of this function
                    self.log.writeInfo([ 'ProviderPoller unable to lock and get su ' + str(asunum) + ' to set polling flag; skipping' ])

            dlInfo = {}
            dlInfo['status'] = 'pending'
            url = self.cgi + '?' + data
            while not self.sdEvent.isSet():
                if datetime.now(self.startTime.tzinfo) > self.startTime + self.timeOut:
                    # the providing site has not completed the export, and the time-out has elapsed; give up
                    # the main thread will handle the time-out, but we need to exit this thread
                    raise TimeoutException('ProviderPoller: timed-out waiting for providing site to return paths to requested SUs (' + ','.join([ str(asunum) for asunum in self.sunums ]) + ') - provider request ' + self.requestID)

                if timeToLog:
                    self.log.writeInfo([ 'Checking on request to provider (provider request ' + self.requestID + ').' ])
                    self.log.writeInfo([ 'URL is ' + url ])

                natt = 0

                while True:
                    try:
                        with urllib.request.urlopen(url) as response:
                            dlInfoStr = response.read().decode('UTF-8')
                        break
                    except urllib.error.URLError as exc:
                        # we want to try again, until we time-out
                        natt += 1
                        if natt > 5:
                            raise ProviderPollerException('unable to access URL ' + url + '; skipping')
                        if type(exc.response) is str:
                            msg = exc.response
                        else:
                            msg = ''
                        log.writeWarning([ 'unable to check request status at provider (' + msg + '); trying again' ])
                        time.sleep(1)

                dlInfo = json.loads(dlInfoStr)

                if timeToLog:
                    self.log.writeInfo([ 'Provider returns status ' + dlInfo['status'] + '.' ])

                if dlInfo['status'] != 'pending':
                    break;

                time.sleep(1)
                loopN += 1
            
                # Log every 5 seconds.
                timeToLog = (loopN % 5 == 0)

            # We might not have printed to log.
            if not timeToLog:
                self.log.writeInfo([ 'Checking on request to provider (provider request ' + self.requestID + ').' ])
                self.log.writeInfo([ 'URL is ' + url ])
                self.log.writeInfo([ 'Provider returns status ' + dlInfo['status'] + '.' ])

            # We are done polling, remove the polling flag.
            for asunum in self.sunums:
                su = self.suTable.getAndLockSU(sunum=asunum, filter=SuTable.removeGhosts)

                if su:
                    try:
                        # do not worry about sus that are no longer in the SU table - the problem is SUs that are in the
                        # table and keep their polling flag True
                        su.setPolling(False)
                    finally:
                        su.releaseLock()

            if dlInfo['status'] != 'complete':
                # Set all SU records to 'E' (rsumds.py timed-out waiting for the SUs to be ready at the providing site).
                errMsg = 'The providing site failed to return paths to requests SUs.'

                # SuTable::setStatus() will acquire and release all SU locks.
                self.suTable.setStatus(self.sunums, 'E', errMsg)
                errMsg = None
            else:
                # now we have to get the SU info by calling the same CGI, but with different arguments
                sunumLst = ','.join(str(asunum) for asunum in self.sunums)
                values = { 'requestid' : 'none', 'sunums' : sunumLst }
                data = urllib.parse.urlencode(values)
                url = self.cgi + '?' + data
                self.log.writeInfo([ 'requesting paths for SUNUMs ' + sunumLst + '; URL is ' + url ])

                natt = 0
                while True:
                    try:
                        with urllib.request.urlopen(url) as response:    
                            dlInfoStr = response.read().decode('UTF-8')
                            natt = 0
                        break
                    except urllib.error.URLError as exc:
                        # we want to try again, until we time-out
                        natt += 1
                        if natt > 5:
                            raise ProviderPollerException('unable to access URL ' + url + '; skipping')
                        if type(exc.response) is str:
                            msg = exc.response
                        else:
                            msg = ''
                        log.writeWarning([ 'unable to obtain SU info from provider (' + msg + '); trying again' ])
                        time.sleep(1)

                dlInfo = json.loads(dlInfoStr)
                
                if dlInfo['status'] != 'complete':
                    errMsg = 'unable to obtain paths from providing site: ' + dlInfo['statusMsg']
                    self.suTable.setStatus(self.sunums, 'E', errMsg)
                    errMsg = None
                else:
                    # Start a download for each SU. If we cannot start the download for any reason, then set the SU status to 'E'.
                    self.log.writeInfo([ 'The SU paths are ready.' ])
                    paths = dlInfo['paths']

                    retentions = {}
                    for (asunum, path, series, suSize) in paths:
                        if path is None:
                            printPath = '<none>'
                        else:
                            printPath = path
                        if series is None:
                            printSeries = '<none>'
                        else:
                            printSeries = series
                        if suSize is None:
                            printSize = '<none>'
                        else:
                            printSize = str(suSize)

                        self.log.writeDebug([ 'ProviderPoller new Downloader loop: su ' + str(asunum) + ', path ' + printPath + ', series' + printSeries + ', size ' + printSize ])

                        su = self.suTable.getAndLockSU(sunum=asunum, filter=SuTable.removeGhosts)
                        
                        if su:
                            try:
                                if not path:
                                        # A path of None means that the SUNUM was invalid. We want to set the SU status to 'E'.
                                    errMsg = 'SU ' + str(asunum) + ' is not valid at the providing site'
                                    su.setStatus('E', errMsg)
                                    self.log.writeError(errMsg)
                                    errMsg = None
                                    continue
                                elif path == '':
                                    # An empty-string path means that the SUNUM was valid, but that the SU referred to was offline, and could not
                                    # be placed back online - it is not archived.
                                    # ART - I need to figure out how to place the SUNUM in SUMS so that its archive flag is N (not archived).
                                    su.setStatus('C', 'SU ' + str(asunum) + ' refers to an offline SU valid at the providing site that was not archived. It cannot be downloaded.')
                                    continue
                
                                if suSize is None:
                                    suSize = 0

                                if series in retentions:
                                    retention = retentions[series]
                                else:
                                    # request provides the host, port, and dbname to use with jsoc_info to fetch the retention value.
                                    retention = ReqTable.getRetention(series, self.request, self.dbUser, self.log)
                                    retentions[series] = retention
                
                                    # Save series and retention.
                                    su.setSeries(series)
                                    su.setRetention(retention)
                                    su.setSize(suSize)

                                while not self.sdEvent.isSet():
                                    Downloader.lock.acquire()
                                    try:
                                        if len(Downloader.tList) < Downloader.maxThreads:
                                            self.log.writeInfo([ 'Instantiating a Downloader for SU ' + str(su.sunum) + '.' ])
                                            Downloader.newThread(su, path, self.suTable, dlInfo['scpUser'], dlInfo['scpHost'], dlInfo['scpPort'], self.binPath, self.arguments, self.log)
                                            break # The finally clause will ensure the Downloader lock is released.
                                    except StartThreadException:
                                        # Ran out of system resources - could not start new thread. Just wait for a thread slot to become free.
                                        pass
                                    finally:
                                        Downloader.lock.release()

                                    Downloader.eventMaxThreads.wait()
                                    # We woke up, but we do not know if there are any open threads in the thread pool. Loop and check
                                    # tList again.
                            finally:
                                su.releaseLock()
                        else:
                            self.log.writeInfo([ 'ProviderPoller unable to lock and get su ' + str(asunum) + ' to initiate a download; skipping' ])                            
        except TimeoutException as exc:
            errMsg = exc.args[0]
        except RemoteSumsException as exc:
            import traceback
            
            errMsg = 'exc.args[0], traceback.format_exc(5)'
            self.suTable.setStatus(self.sunums, 'E', exc.args[0])
        except Exception as exc:
            # catch all remaining exceptions
            import traceback

            errMsg = 'unknown exception, traceback.format_exc(5)'
            self.suTable.setStatus(self.sunums, 'E', 'unknown exception')
            
        if errMsg:
            self.log.writeError([ errMsg ])
            
        for asunum in self.sunums:
            su = self.suTable.getAndLockSU(sunum=asunum, filter=SuTable.removeGhosts)
            
            if su:
                try:
                    if su.getPolling():
                        # do not worry about SUs that are no longer in the SU table
                        su.setPolling(False)
                finally:
                    su.releaseLock()
                    
        # Flush the change to disk.
        self.suTable.updateDbAndCommit(self.sunums)

        try:
            ProviderPoller.lock.acquire()
            ProviderPoller.tList.remove(self) # This thread is no longer one of the running threads.
            self.log.writeInfo([ 'Poller (request ID ' + str(self.requestID) + ') halted.' ])   
            
            # Use <= because we don't know if we were able to reach Downloader.maxThreads thread running due 
            # to system-resource limitations.
            if len(ProviderPoller.tList) <= ProviderPoller.maxThreads - 1:
                # Fire event so that main thread can add new sunums to the ProviderPoller queue.
                ProviderPoller.eventMaxThreads.set()
                # Clear event so that main will block the next time it calls wait.
                ProviderPoller.eventMaxThreads.clear()
        finally:
            ProviderPoller.lock.release()

    def stop(self):
        self.log.writeInfo([ 'Stopping ProviderPoller (requestID ' + str(self.requestID) + ').' ])
        self.sdEvent.set()

    # for all sus, the SU lock is held by the main thread
    @staticmethod
    def newThread(cgi, requestID, sunums, suTable, reqTable, request, dbUser, binPath, arguments, log):
        poller = ProviderPoller(cgi, requestID, sunums, suTable, reqTable, request, dbUser, binPath, arguments, log)
        poller.tList.append(poller)
        try:
            poller.start()
        except RuntimeError:
            poller.tList.remove(poller)
            del poller
            raise StartThreadException('Cannot start a new ProviderPoller thread due to system resource limitations.')
            
    @classmethod
    def isPollingForSU(cls, sunum):
        rv = False
        try:
            cls.lock.acquire()
            
            for poller in cls.tList:
                pollerSUs = poller.sunums
                
                # check for list membership
                if sunum in pollerSUs:
                    rv = True
                    break
        finally:
            cls.lock.release()
            
        return rv


def readTables(sus, requests, sites):
    if sus:
        sus.tryRead()

    if requests:
        requests.tryRead()
    
    if sites:
        sites.tryRead()

# Process the SUs for the source site represented by url.
# cgi - the cgi-supath URL (e.g., http://jsoc.stanford.edu/cgi-bin) from which SU paths can be obtained.
# sunums - a list of sorted SUNUMs to download.
# suTable - the SU table object that represents the SU database table.
# request - ReqTable::dict[requestidStr] object.
# binPath - the local path to the binaries needed to ingest the downloaded SU into SUMS. This is mostly likely the path to
#           the DRMS binaries (one binary needed is vso_sum_alloc)
# log - the log to write messages to.
# reprocess - the SUs identified are all being reprocessed. They all have a status of 'P' in the SU table. There was
#             some interruption that caused the download to be lost.
# reset - reset the processing start time for each SU. Ignored, unless reprocess is true
def processSUs(cgi, sunums, suTable, reqTable, request, dbUser, binPath, arguments, log, reprocess=False, reset=False):
    # get path to SUs by calling the rs.sh cgi at the owning remote site
    # Create the sunum= argument.

    # Skip any SUs that have already been processed. Pending SUs are OK to restart, however. It may be the case
    # that rsumds.py was interrupted during a download, in which case, we want to re-download the SU.
    workingSus = {}

    try:
        # must have locks
        sus, missingSunums = suTable.getAndLockSUs(releaseTableLock=False, sunums=sunums, filter=SuTable.removeGhosts)
        try:
            for su in sus:
                if su.status == 'P' or su.status == 'W':
                    if not reprocess:
                        # Accidental attempt to reprocess an SU whose processing has already started.
                        raise DuplicateSUException('An accidental attempt to reprocess pending SU ' + str(su.sunum) + ' occurred.')
                    workingSus[str(su.sunum)] = su
                    # Reset start time.
                    if reset:
                        # acquires the SU table lock 
                        su.setStarttime(datetime.now(timezone.utc))
                    
            for sunum in missingSunums:
                log.writeInfo([ 'inserting a new SU table record for ' + str(sunum) ])
                # create a new SU table record for this SU; will set status to P
                suTable.insert(sunums=[ sunum ], lockTable=False)
                log.writeDebug([ 'successfully inserted SU ' + str(sunum) ])
                su = suTable.getAndLockSU(lockTable=False, sunum=sunum, filter=SuTable.removeGhosts)
                workingSus[str(su.sunum)] = su
        finally:
            # no longer need SU table lock
            suTable.releaseLock()        

        sunumLst = ','.join(list(workingSus.keys()))
        values = {'requestid' : 'none', 'sunums' : sunumLst, 'N' : 1}
        data = urllib.parse.urlencode(values)
        url = cgi + '?' + data

        natt = 0
        while True:
            try:
                log.writeInfo([ 'requesting paths for SUNUMs ' + sunumLst + '; URL is ' + url ])
                with urllib.request.urlopen(url) as response:    
                    dlInfoStr = response.read().decode('UTF-8')
                    natt = 0
                    break
            except urllib.error.URLError as exc:
                # we want to try again, until we time-out
                natt += 1
                if natt > 10:
                    raise SUPathCGIException('unable to access URL ' + url + '; skipping')
                if type(exc.response) is str:
                    msg = exc.response
                else:
                    msg = ''
                log.writeWarning([ 'unable to obtain SU info from provider (' + msg + '); trying again' ])
                time.sleep(1)

        dlInfo = json.loads(dlInfoStr)
        
        if dlInfo['status'] == 'complete':
            # All of the requested SUs are online at the providing site.
            paths = dlInfo['paths']

            # Start a download for each SU. If we cannot start the download for any reason, then set the SU status to 'E'.
            retentions = {}
            for (sunum, path, series, suSize) in paths:
                try:
                    su = workingSus[str(sunum)]
                except KeyError:
                    log.writeWarning([ 'SUNUM ' + str(sunum) + ' returned by SU-path CGI is not recognized; skipping ' ])
                    continue
                    
                if path is None:
                    # A path of None means that the SUNUM was invalid. We want to set the SU status to 'E'.    
                    su.setStatus('E', 'SU ' + str(sunum) + ' is not valid at the providing site.')
                    continue
                elif path == '':
                    # An empty-string path means that the SUNUM was valid, but that the SU referred to was offline (it may or
                    # may be archived). Regardless, RS will not attempt to perform an export request to obtain the path.
                    # ART - I need to figure out how to place the SUNUM in SUMS so that its archive flag is N (not archived).
                    su.setStatus('C', 'SU ' + str(sunum) + ' refers to an SU that is valid at the providing site, but it is offline and cannot be downloaded.')
                    continue
                
                if suSize is None:
                    suSize = 0

                if series in retentions:
                    retention = retentions[series]
                else:
                    # request provides the host, port, and dbname to use with jsoc_info to fetch the retention value.
                    retention = ReqTable.getRetention(series, request, dbUser, log)
                    retentions[series] = retention
                
                # save series and retention
                su.setSeries(series)
                su.setRetention(retention)
                su.setSize(suSize)

                # ok to stop main thread here; if we are saturated with downloads, wait here while the Downloader threads complete;
                # ok to hold SU lock too - the only other threads trying to obtain the SU lock are the Scp Workers, and they
                # do not block on the lock (if they cannot acquire lock immediately, they skip the SU)
                while True:
                    Downloader.lock.acquire()
                    try:
                        if len(Downloader.tList) < Downloader.maxThreads:
                            log.writeInfo([ 'instantiating a Downloader for SU ' + str(su.sunum) ])
                            Downloader.newThread(su, path, suTable, dlInfo['scpUser'], dlInfo['scpHost'], dlInfo['scpPort'], binPath, arguments, log)
                            break # The finally clause will ensure the Downloader lock is released.
                    except StartThreadException as exc:
                        log.writeError([ 'cannot start Downloader thread. Out of system resources; will try again when existing Downloaders complete' ])
                        # Ran out of system resources - could not start new thread. Just wait for a thread slot to become free.
                    finally:
                        Downloader.lock.release()

                    log.writeDebug([ 'main thread waiting for thread slot for SU ' + str(su.sunum) ])
                    Downloader.eventMaxThreads.wait()
                    # We woke up, but we do not know if there are any open threads in the thread pool. Loop and check
                    # tList again.

            # for each SU that was requested, but for which no path was given in the response, update its SU-table record with an error status.
            pathInResp = set([ sunum for (sunum, path, series, suSize) in paths ])
            
            for sunumStr, su in workingSus.items():
                if su.sunum not in pathInResp:
                    su.setStatus('E', 'providing site cannot provide a path for SU ' + str(su.sunum))
        elif dlInfo['status'] == 'pending':
            # One or more of the requested SUs is offline. Poll until they are ready. Ideally we wouldn't block the main
            # thread here, waiting for the SUs to be available. We could spawn a thread to poll, freeing up the main
            # thread to continue with other requests. But in the interest of time, just poll for now. Must acquire
            # su-table lock when it is finally time to start downloads.
            # log.writeInfo([ 'Request includes one or more SUs that are offline at the providing site. Waiting for providing site to put them online.' ])
            # 
            #         while True:
            #             ProviderPoller.lock.acquire()
            #             try:
            #                 if len(ProviderPoller.tList) < ProviderPoller.maxThreads:
            #                     log.writeInfo([ 'Instantiating a ProviderPoller for sunums ' + ','.join([ str(asunum) for asunum in workingSunums ]) + '.' ])
            #                     ProviderPoller.newThread(url, dlInfo['requestid'], workingSunums, sus, reqTable, request, dbUser, binPath, arguments, log)
            #                     break
            #             except StartThreadException as exc:
            #                 log.writeError([ 'Cannot start ProviderPoller thread. Out of system resources. Will try again when existing ProviderPollers complete.' ])
            #             finally:
            #                 ProviderPoller.lock.release()
            # 
            #             ProviderPoller.eventMaxThreads.wait()
        
            # We started an export request to fetch offline SUs at the providing site. This can no longer happen (rs.sh is called
            # with the N=1 argument now)!! Do not perform an export request! There are a limited number ofexport-request 
            # slots (at the JSOC). Since a request for an offline SU entails an asynchronous tape read, performing these requests, 
            # making a request for a large number of SUs could saturate the export system for days.
            # Log an error if we get here.
            log.writeInfo([ 'request includes one or more SUs that are offline at the providing site; an export request was started to put them online' ])
            for sunumStr, su in workingSus.items():
                su.setStatus('E', 'an export request was started - this is no longer allowed')
        else:
            # Error of some kind.
            # Update the SU-table status of the SUs to 'E'.
            for sunumStr, su in workingSus.items():
                su.setStatus('E', 'unable to obtain paths from providing site; ' + dlInfo['statusMsg']) 
    finally:
        allLockedSUs = set()
        
        for su in sus:
            allLockedSUs.add(su)
            
        for sunumStr, su in workingSus.items():
            allLockedSUs.add(su)
        
        for su in list(allLockedSUs):
            su.releaseLock()


class LogLevelAction(argparse.Action):
    def __call__(self, parser, namespace, value, option_string=None):
        valueLower = value.lower()
        if valueLower == 'critical':
            level = logging.CRITICAL
        elif valueLower == 'error':
            level = logging.ERROR
        elif valueLower == 'warning':
            level = logging.WARNING
        elif valueLower == 'info':
            level = logging.INFO
        elif valueLower == 'debug':
            level = logging.DEBUG
        else:
            level = logging.ERROR

        setattr(namespace, self.dest, level)
        
        
def updateDbAndCommit(log, rsConn, rsDbLock, reqTable, suTable, reqIDs, sunums):
    # lock the SU table, the SUs, and access to the DB until the SU table SQL has been run so that nothing changes out from under us;
    # no need to lock anything having to do with the requests table - it is accessed by a single thread only
    # we acquire the DB Lock so that our transaction cannot get polluted - we do not want another thread committing the
    # transaction (all cursors share the same transaction)
    
    suSql = suTable.getUpdateDBSql(sunums)
    rqSql = reqTable.getUpdateDBSql(reqIDs)
    
    rsDbLock.acquire() # do not allow other threads to commit the DB changes in the middle of this
    try:
        # neither of these calls commits changes to the DB
        log.writeDebug([ 'updating the DB with SU table changes (SUs ' + ','.join([ str(sunum) for sunum in sunums ]) + ')' ])
        suTable.updateDB(suSql)
        log.writeDebug([ 'updating the DB with request table changes (request ' + ','.join(str(reqID) for reqID in reqIDs) + ')' ])
        reqTable.updateDB(rqSql)
        
        log.writeDebug([ 'committing Req-table and SU-table changes to DB' ])
        rsConn.commit()
        log.writeDebug([ 'successfully committed Req and SU changes' ])
    except:
        import traceback
        
        log.writeWarning([ 'failed to commit Req and SU changes; changed rolled back' ])
        log.writeWarning([ traceback.format_exc(5) ])
        rsConn.rollback() # it could be that the first call succeeds and the second fails - we need to rollback the first too
        # continue on with the next request (do not terminate)
    finally:
        rsDbLock.release()


if __name__ == "__main__":
    rv = RET_SUCCESS

    try:
        sumsDrmsParams = SumsDrmsParams()

        parser = CmdlParser(usage='%(prog)s [ -h ] [ sutable=<storage unit table> ] [ reqtable=<request table> ] [ --dbname=<db name> ] [ --dbhost=<db host> ] [ --dbport=<db port> ] [ --binpath=<executable path> ] [ --logfile=<base log-file name> ]')
    
        # Optional parameters - no default argument is provided, so the default is None, which will trigger the use of what exists in the configuration file
        # (which is drmsparams.py).
        parser.add_argument('r', '--reqtable', help='The database table that contains records of the SU-request being processed. If provided, overrides default specified in configuration file.', metavar='<request unit table>', dest='reqtable', default=sumsDrmsParams.get('RS_REQUEST_TABLE'))
        parser.add_argument('s', '--sutable', help='The database table that contains records of the storage units being processed. If provided, overrides default specified in configuration file.', metavar='<storage unit table>', dest='sutable', default=sumsDrmsParams.get('RS_SU_TABLE'))
        parser.add_argument('n', '--nworkers', help='The number of scp worker threads.', metavar='<number of worker threads>', dest='nWorkers', type=int, default=sumsDrmsParams.get('RS_N_WORKERS'))
        parser.add_argument('t', '--tmpdir', help='The temporary directory to use for scp downloads.', metavar='<temporary directory>', dest='tmpdir', default=sumsDrmsParams.get('RS_TMPDIR'))
        parser.add_argument('-N', '--dbname', help='The name of the database that contains the series table from which records are to be deleted.', metavar='<db name>', dest='dbname', default=sumsDrmsParams.get('RS_DBNAME'))
        parser.add_argument('-U', '--dbuser', help='The name of the database user account.', metavar='<db user>', dest='dbuser', default=sumsDrmsParams.get('RS_DBUSER'))
        parser.add_argument('-H', '--dbhost', help='The host machine of the database that contains the series table from which records are to be deleted.', metavar='<db host machine>', dest='dbhost', default=sumsDrmsParams.get('RS_DBHOST'))
        parser.add_argument('-P', '--dbport', help='The port on the host machine that is accepting connections for the database that contains the series table from which records are to be deleted.', metavar='<db host port>', dest='dbport', default=int(sumsDrmsParams.get('RS_DBPORT')))
        parser.add_argument('-b', '--binpath', help='The path to executables run by this daemon (e.g., vso_sum_alloc, vso_sum_put).', metavar='<executable path>', dest='binpath', default=sumsDrmsParams.get('RS_BINPATH'))
        parser.add_argument('-l', '--loglevel', help='Specifies the amount of logging to perform. In order of increasing verbosity: critical, error, warning, info, debug', dest='loglevel', action=LogLevelAction, default=logging.ERROR)
                
        arguments = Arguments(parser)
        
        arguments.setArg('lockfile', sumsDrmsParams.get('RS_LOCKFILE'))
        arguments.setArg('dltimeout', timedelta(seconds=int(sumsDrmsParams.get('RS_DLTIMEOUT'))))
        arguments.setArg('reqtimeout', timedelta(seconds=int(sumsDrmsParams.get('RS_REQTIMEOUT'))))
        arguments.setArg('maxthreads', int(sumsDrmsParams.get('RS_MAXTHREADS')))
        arguments.setArg('scpMaxNumSUs', int(sumsDrmsParams.get('RS_SCP_MAXSUS')))
        arguments.setArg('scpMaxPayload', 1024 * 1024 * int(sumsDrmsParams.get('RS_SCP_MAXPAYLOAD')))
        arguments.setArg('scpTimeOut', timedelta(seconds=int(sumsDrmsParams.get('RS_SCP_TIMEOUT'))))
        arguments.setArg('rsSiteInfoURL', sumsDrmsParams.get('RS_SITE_INFO_URL'))
        arguments.setArg('logdir', sumsDrmsParams.get('RS_LOGDIR'))
        
        arguments.setArg('sumsdbname', sumsDrmsParams.get('DBNAME') + '_sums')
        arguments.setArg('sumsdbuser', sumsDrmsParams.get('SUMS_MANAGER'))
        arguments.setArg('sumsdbhost', sumsDrmsParams.get('SUMS_DB_HOST'))
        arguments.setArg('sumsdbport', int(sumsDrmsParams.get('SUMPGPORT')))
        
        arguments.setArg('tapesysexists', int(sumsDrmsParams.get('SUMS_TAPE_AVAILABLE')) == 1)
        
        pid = os.getpid()

        # Create/Initialize the log file.
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
        rslog = Log(os.path.join(sumsDrmsParams.get('RS_LOGDIR'), LOG_FILE_BASE_NAME + '_' + datetime.now().strftime('%Y%m%d') + '.txt'), arguments.loglevel, formatter)
        rslog.writeCritical([ 'Starting up remote-SUMS daemon.' ])
        rslog.writeCritical([ 'Logging threshold level is ' + rslog.getLevel() + '.' ]) # Critical - always write the log level to the log.
        arguments.dump(rslog)
        
        rslog.writeInfo([ 'Setting download timeout to ' + str(arguments.dltimeout) + ' (the daemon must complete the download within this interval).' ])
        rslog.writeInfo([ 'Setting request timeout to ' + str(arguments.reqtimeout) + ' (the daemon must locate the request within this interval).' ])
        rslog.writeInfo([ 'Setting scp timeout to ' + str(arguments.scpTimeOut) + ' (each ScpWorker waits this long at most before initiating the next scp.).' ])

        thContainer = [ arguments, str(pid), rslog ]
        
        # TerminationHandler opens a DB connection to the RS database (which is the same as the DRMS database, most likely).
        with TerminationHandler(thContainer) as th:
            rsConn = th.rsConn()
            sumsConn = th.sumsConn()

            rsDbLock = threading.RLock() # global
            sumsDbLock = threading.Lock() # global
            
            rslog.writeInfo([ 'Obtained script file lock.' ])

            suTable = arguments.sutable
            reqTable = arguments.reqtable

            suTableObj = None
            reqTableObj = None
            sites = None

            # Read the storage-unit and request tables. Do this only once per daemon run. However, we save this table
            # information every iteration of the daemon loop, just in case a crash happens. After a crash, when
            # the daemon starts up, it will retrieve the latest saved information so the disruption will be minimal.
            # We will have to clean up any pending downloads since the threads managing those downloads will have
            # been lost, and we cannot trust that the downloads completed successfully (although they might have).
            # A fancier implementation would be some kind of download manager that can recover partially downloaded
            # storage units, but who has the time :)

            SuTable.rsConn = rsConn # Class variable
            SuTable.sumsConn = sumsConn # Class variable
            SuTable.rsDbLock = rsDbLock # Class variable
            SuTable.sumsDbLock = sumsDbLock # Class variable
            suTableObj = SuTable(suTable, arguments.dltimeout, rslog)

            ReqTable.rsConn = rsConn # Class variable
            ReqTable.rsDbLock = rsDbLock # Class variable
            reqTableObj = ReqTable(reqTable, arguments.reqtimeout, rslog)
            
            Downloader.sumsConn = sumsConn
            Downloader.sumsDbLock = sumsDbLock # Class variable

            sites = SiteTable(arguments.rsSiteInfoURL, rslog)

            # This function will try to read each table 10 times before giving up (and raising an exception).
            readTables(suTableObj, reqTableObj, sites)

            # Set max number of threads we can process at once.
            Downloader.setMaxThreads(arguments.maxthreads)
            
            ###########################
            # START SCPWORKER THREADS #
            ###########################
            
            # make N threads that handle scp commands; each of the worker threads will use one of these threads to perform
            # the actual scp command; MUST do this here, before 'P' Downloader threads have been
            # started. The ScpWorker threads need the parent Downloader threads to exist before it can process 'W' SUs, but if there
            # are more than maxThreads 'P' SUs, then the processSUs code will block until threads free up, and that cannot happen
            # if the ScpWorker threads are not running
            for nthread in range(1, arguments.nWorkers + 1):
                ScpWorker.lock.acquire()
                try:
                    if len(ScpWorker.tList) < ScpWorker.maxThreads:
                        rslog.writeInfo([ 'Instantiating ScpWorker ' + str(nthread) + '.' ])
                        ScpWorker.newThread(nthread, suTableObj, arguments, rslog)
                    else:
                        break # The finally clause will ensure the ScpWorker lock is released.
                finally:
                    ScpWorker.lock.release()
            
            ################################
            # RESTART INTERRUPTED REQUESTS #
            ################################
            
            # we must have already started ScpWorker threads, otherwise processSUs will block if we try to process too
            # many SUs in the following block of code - we'll use up all Downloader threads, and since there are no 
            # ScpWorker threads perform downloads, the Downloader pool will not get restored, and the whole
            # system will block on the Downloader.eventMaxThreads.wait() call
            th.disableInterrupts()
            
            reqsPending = reqTableObj.getPending()
            rslog.writeInfo([ 'there are ' + str(len(reqsPending)) + ' pending requests on start-up'])
                              
            # remove ALL SUs from the SU Table; as we iterate through the pending requests, we will start a Downloader
            # for each of the SUs that are part of a pending request and insert that SU into the SU Table with a status 
            # of P; if an SU exists in the SU Table and it is not part of a pending request, then it is orphaned and
            # should also be deleted
            sus, missingSunums = suTableObj.getAndLockSUs(sunums=None, filter=None)
            try:
                if sus and len(sus) > 0:
                    rslog.writeInfo([ 'deleting SUs (via decrementRefcount()) from previous run of Remote SUMS: ' + ','.join([ str(su.sunum) for su in sus ]) ])
                    for su in sus:
                        # the existing SUs should all have refcount 1 (since we are starting up Remote SUMS)
                        su.decrementRefcount()
            finally:
                for su in sus:
                    su.releaseLock()
            
            # gotta commit changes to the DB, else we'll get a duplicate SU exception when we attempt to initiate
            # the download of the SUs again
            suTableObj.updateDbAndCommit()

            # request is a dictionary
            for request in reqsPending:
                # use a set() to remove duplicates IN THE SAME REQUEST
                sunums = list(set(request['sunums']))
                rslog.writeInfo([ 'found an interrupted download request, id ' + str(request['requestid']) + ', for SUNUMs ' + ','.join([str(sunum) for sunum in sunums]) ])
                
                # Get all SU records for which a download is already in progress.
                unknown = [] # SUs that are not being processed (not in the SU Table)
                known = [] # SUs that are being processed (in the SU Table AND the SUs have a Worker - these have been
                           # 'restarted')
                toComplete = [] # unknown and online

                # if an SU exists in the SU Table, but it has not been 'restarted' yet (it was 'interrupted), then 
                # we want to remove that SU from the SU Table and put it in the unknown list; if an SU exists in the 
                # SU table, but it has already been restarted, then it is known
                sus, missingSunums = suTableObj.getAndLockSUs(sunums=sunums, filter=SuTable.removeGhosts)
                try:
                    for su in sus:
                        rslog.writeInfo([ 'a download for SU ' + str(su.sunum)+ ' is already in progress' ])
                        known.append(su.sunum)
                    
                    if len(missingSunums) > 0:
                        unknown.extend(missingSunums)
                        rslog.writeDebug([ 'request ' + str(request['requestid']) + ' contains unknown SUs: ' + ','.join([ str(sunum) for sunum in unknown ]) ])

                    offlineSunums = SuTable.offline(unknown, arguments.binpath, rslog)
                
                    dlsToStart = []

                    for sunum in unknown:
                        if sunum in offlineSunums:
                            rslog.writeInfo([ 'SU ' + str(sunum) + ' is offline - will start a download' ])
                            dlsToStart.append(sunum)
                        else:
                            rslog.writeInfo([ 'SU ' + str(sunum) + ' is online already - will NOT start a download.' ])
                            toComplete.append(sunum)
                    
                    # Start downloads for all unknown, offline SUs
                    siteSunums = {}
                    for asunum in dlsToStart:
                        try:
                            siteCGI = sites.getSuPathCGI(asunum)
                        except UnknownSitecodeException as exc:                        
                            # Skip this request - invalid SU.
                            msg = 'uknown site code for SU ' + str(asunum) + '; skipping SU ' + str(sunum) + ' for ' + str(request['requestid'])
                            rslog.writeWarning([ msg ])
                            continue
                    
                        if siteCGI not in siteSunums:
                            siteSunums[siteCGI] = []
                    
                        siteSunums[siteCGI].append(asunum)
                except:
                    # skip the current request, but don't allow rsumsd.py to terminate (if the exception is not handled here, then
                    # rsumsd.py will terminate)
                    import traceback
                    
                    rslog.writeWarning([ traceback.format_exc(5) ])
                    continue
                finally:
                    for su in sus:
                        su.releaseLock()

                for cgi, sunumList in siteSunums.items():
                    if len(sunumList) > 0:
                        # Chunk is a list of SUNUMs (up to 64 of them).
                        sunumList.sort()
                        chunker = Chunker(sunumList, 64)
                        for chunk in chunker:
                            # We want to always insert a record for each SU into the SU table. Do not provide the insertRec
                            # argument to do so. This call creates new SU-table records, so it modifies the sus object.
                            try:
                                # processSUs() does not modify the reqTableObj; but it can modify SU objects
                                processSUs(cgi, chunk, suTableObj, reqTableObj, request, arguments.dbuser, arguments.binpath, arguments, rslog, True, False)
                            except RemoteSumsException as exc:
                                # do not die - just reject the request's current chunk; we do not know exactly which SUs made it into the
                                # SU Table, but at least one either did not or it did, but with status E; the request will eventually 
                                # error-out when the other SUs complete (successfully or not)
                                if type(exc.response) is str:
                                    rslog.writeWarning([ exc.response ])
                                rslog.writeWarning([ 'failed to process SUs ' + ','.join([ str(sunum) for sunum in chunk ]), 'skipping this chunk' ])
                            except Exception:
                                import traceback

                                msg = traceback.format_exc(5)
                                rslog.writeWarning([ msg ])
                                rslog.writeWarning([ 'failed to process SUs ' + ','.join([ str(sunum) for sunum in chunk ]), 'skipping this chunk' ])
                                
                # increment the refcount on all SU records that were already being processed before the new request processed
                # the same SU records; this modifies the SU objects; it also acquires each SU's lock
                if len(known) > 0:
                    rslog.writeDebug([ 'incrementing refcount for ' + ','.join([ str(sunum) for sunum in known ]) ])
                    suTableObj.incrementRefcount(known)
                
                # insert a new SU record for all unknown SUs that are already online; these calls modify the sus object;
                # in this case, there was no SU inserted into the SU Table, but at least one request contained the SU; 
                # if a pending request contains an SU that cannot be found (i.e., it is not in the SU Table), the 
                # request will error-out; to avoid this, we need to insert the SU in the SU Table; an SU that is online 
                # is essentially an SU that has downloaded successfully, so as we insert the SU into the SU Table, we 
                # need to mark the SU complete
                suTableObj.insert(sunums=toComplete)
                
                # this also acquire each SU's lock
                suTableObj.setStatus(toComplete, 'C')

                # At this point, both the requests table and SU table have been modified, but have not been flushed to disk.
                # Flush them, but do this inside a transaction so that the first does not happen without the second.
                updateDbAndCommit(rslog, rsConn, rsDbLock, reqTableObj, suTableObj, [ request['requestid'] ], sunums)
                
            th.enableInterrupts()
            # END RESTART REQUESTS #

            #############
            # MAIN LOOP #
            #############
            loopN = 0
            while True:
                # For each 'P' request in the request table, check to see if the requested downloads have completed yet.

                # Log every 5 seconds.
                timeToLog = (loopN % 5 == 0)
                
                ############################
                # PROCESS PENDING REQUESTS #
                ############################
                th.disableInterrupts()
                            
                reqsPending = reqTableObj.getPending()

                for arequest in reqsPending:
                    done = True
                    reqError = False
                    sunums = list(set(arequest['sunums']))

                    sus, missingSunums = suTableObj.getAndLockSUs(sunums=sunums, filter=SuTable.removeGhosts)
                    try:
                        for asu in sus:
                            if asu.status == 'P' or asu.status == 'W' or asu.status == 'D':
                                rslog.writeInfo([ 'download of SU ' + str(asu.sunum)  + ' is pending' ])

                                # Check for dead Downloader thread. If so, error-out the SU.
                                if not ProviderPoller.isPollingForSU(asu.sunum):
                                    # a downloader thread has not yet been assigned to this SU
                                    if not hasattr(asu, 'worker') or not asu.worker or not isinstance(asu.worker, (Downloader)) or not asu.worker.isAlive():
                                        asu.setStatus('E', 'no worker for SU ' + str(asu.sunum))
                                done = False
                            elif asu.status == 'E':
                                rslog.writeInfo([ 'download of SU ' + str(asu.sunum)  + ' has errored-out; ' + asu.errmsg ])
                                reqError = True
                            elif asu.status == 'C':
                                rslog.writeInfo([ 'Download of SU ' + str(asu.sunum)  + ' has completed.' ])
                            else:
                                # Unknown status; set status to 'E' (next pass through this loop, this SU will be considered complete)
                                asu.setStatus('E', 'SU ' + str(asu.sunum) + ' has an unknown status of ' + asu.status + '.')
                                done = False
                                
                        if reqError:
                            errMsg = 'failed to download at least one SU in this request'
                                
                        # len(missingSunums) could be > 0 because at least one SU was skipped for some reason in processSUs() - 
                        # perhaps the path for the SU could not be obtained, or perhaps the SU was offline at the providing
                        # site; in this case, because we iterated above through only the SUs that are being processed, processing for this
                        # request will eventually stop; but we need to set the request's status to E
                        if len(missingSunums) > 0:
                            rslog.writeError([ 'request number ' + str(arequest['requestid']) + ' errored-out - could not process all SUs' ])
                            reqError = True
                            errMsg = 'could not process all SUs for this request'
                    finally:
                        for su in sus:
                            su.releaseLock()

                    if done:
                        # There are no pending downloads for this request. Set this request's status to 'C' or 'E', and decrement
                        # refcount on each SU.
                        if reqError:
                            rslog.writeInfo([ 'request number ' + str(arequest['requestid']) + ' for SUNUM(s) ' + ','.join([ str(sunum) for sunum in sunums ]) + ' errored-out' ])
                            reqTableObj.setStatus([ arequest['requestid'] ], 'E', errMsg)
                        else:
                            rslog.writeInfo([ 'request number ' + str(arequest['requestid']) + ' for SUNUM(s) ' + ','.join([ str(sunum) for sunum in sunums ]) + ' completed successfully' ])
                            reqTableObj.setStatus([ arequest['requestid'] ], 'C')
                        
                        # ART - Do not hold rsDbLock here! The functions called may acquire other locks, leading to deadlock.
                        # The rsDbLock should be for executing SQL only.
                        rslog.writeDebug([ 'decrementing refcount for SUs: ' + ','.join([ str(ansunum) for ansunum in sunums ]) ])
                        # Remove duplicates from list first. We do not need to preserve the order of the SUNUMs
                        # before calling decrementRefcount() since that function uses a hash lookup on the SUNUM
                        # to find the associated refcount. Does not modify SU db table.
                        suTableObj.decrementRefcount(sunums)

                        # commit status changes to the requests and su table (decrementRefcount modified SUs)
                        updateDbAndCommit(rslog, rsConn, rsDbLock, reqTableObj, suTableObj, [ arequest['requestid'] ], sunums)
                    else:
                        # if no SU status has changed, then this is a no-op, otherwise changes are saved to the DB
                        # no changes were made to the requests table
                        suTableObj.updateDbAndCommit()

                # For each 'N' request in the request table, start a new set of downloads (if there is no download currently running -
                # i.e., no SU record) or increment the refcounts on the downloads (if there are downloads currently running - i.e.,
                # an SU record exists). But Before starting a new download, make sure that requested SU is not already online.
                # Due to race conditions, a request could have caused a download to occur needed by another request whose state is 'N'.
                
                th.enableInterrupts()
                # END PENDING REQUESTS #

                ########################
                # PROCESS NEW REQUESTS #
                ########################
                th.disableInterrupts()
                            
                reqTableObj.refresh() # Clients may have added requests to the queue.
                reqsNew = reqTableObj.getNew()

                # arequest is a dictionary
                for arequest in reqsNew:
                    timeNow = datetime.now(arequest['starttime'].tzinfo)
                    if timeNow > arequest['starttime'] + reqTableObj.getTimeout():
                        rslog.writeInfo([ 'Request number ' + str(arequest['requestid']) + ' timed-out.' ])
                        reqTableObj.setStatus([ arequest['requestid'] ], 'E', 'Request timed-out.')
                        
                        try:
                            reqTableObj.updateDbAndCommit([ arequest['requestid'] ])
                        except:
                            import traceback
                            
                            rslog.writeWarning([ traceback.format_exc(5) ])
                            # swallow the exception and continue with the next request
                            pass
                        continue
                    
                    # use a set() to remove duplicates IN THE SAME REQUEST
                    sunums = list(set(arequest['sunums']))
                    rslog.writeInfo([ 'found a new download request, id ' + str(arequest['requestid']) + ', for SUNUMs ' + ','.join([str(sunum) for sunum in sunums]) ])
                    
                    # Get all SU records for which a download is already in progress.
                    unknown = [] # SUs that are not being processed
                    known = [] # SUs that are being processed
                    toComplete = [] # unknown and online

                    # split the request into duplicate 'known' SUs (another request is handling the SU download) and
                    # new 'unknown' SU
                    sus, missingSunums = suTableObj.getAndLockSUs(sunums=sunums, filter=SuTable.removeGhosts)
                    try:
                        for su in sus:
                            rslog.writeInfo([ 'a download for SU ' + str(su.sunum)+ ' is already in progress' ])
                            known.append(su.sunum)
                        
                        if len(missingSunums) > 0:
                            unknown.extend(missingSunums)
                            rslog.writeDebug([ 'request ' + str(arequest['requestid']) + ' contains unknown SUs: ' + ','.join([ str(sunum) for sunum in unknown ]) ])
                            
                        offlineSunums = SuTable.offline(unknown, arguments.binpath, rslog)
                    
                        dlsToStart = []

                        for asunum in unknown:
                            if asunum in offlineSunums:
                                rslog.writeInfo([ 'SU ' + str(asunum) + ' is offline - will start a download' ])
                                dlsToStart.append(asunum)
                            else:
                                rslog.writeInfo([ 'SU ' + str(asunum) + ' is online already - will NOT start a download.' ])
                                toComplete.append(asunum)
                        
                        # Start downloads for all unknown, offline SUs
                        siteSunums = {}
                        for asunum in dlsToStart:
                            try:
                                siteCGI = sites.getSuPathCGI(asunum)
                            except UnknownSitecodeException as exc:                        
                                # Skip this request - invalid SU.
                                msg = 'uknown site code for SU ' + str(asunum) + '; skipping SU ' + str(sunum) + ' for ' + str(arequest['requestid'])
                                rslog.writeWarning([ msg ])
                                continue
                        
                            if siteCGI not in siteSunums:
                                siteSunums[siteCGI] = []
                        
                            siteSunums[siteCGI].append(asunum)
                    except:
                        # skip the current request, but don't allow rsumsd.py to terminate (if the exception is not handled here, then
                        # rsumsd.py will terminate)
                        import traceback
                        
                        rslog.writeWarning([ traceback.format_exc(5) ])
                        continue
                    finally:
                        for su in sus:
                            su.releaseLock()

                    for cgi, sunumList in siteSunums.items():
                        if len(sunumList) > 0:
                            # Chunk is a list of SUNUMs (up to 64 of them).
                            sunumList.sort()
                            chunker = Chunker(sunumList, 64)
                            for chunk in chunker:
                                # We want to always insert a record for each SU into the SU table. Do not provide the insertRec
                                # argument to do so. This call creates new SU-table records, so it modifies the sus object.
                                try:
                                    # processSUs() does not modify the reqTableObj; but it can modify SU objects
                                    processSUs(cgi, chunk, suTableObj, reqTableObj, arequest, arguments.dbuser, arguments.binpath, arguments, rslog)
                                except RemoteSumsException as exc:
                                    # do not die - just reject the request's current chunk; we do not know exactly which SUs made it into the
                                    # SU Table, but at least one either did not or it did, but with status E; the request will eventually 
                                    # error-out when the other SUs complete (successfully or not)
                                    if type(exc.response) is str:
                                        rslog.writeWarning([ exc.response ])
                                    rslog.writeWarning([ 'failed to process SUs ' + ','.join([ str(asunum) for asunum in chunk ]), 'skipping this chunk' ])
                                    
                                except Exception:
                                    import traceback

                                    msg = traceback.format_exc(5)
                                    rslog.writeWarning([ msg ])
                                    rslog.writeWarning([ 'failed to process SUs ' + ','.join([ str(asunum) for asunum in chunk ]), 'skipping this chunk' ])

                    # REGARDLESS IF ANY SUS WERE INSERTED INTO THE SU TABLE (if none got inserted, then no downloads will happen), 
                    # a request was initiated; if any Downloader thread fails, or if at least one was never started, then
                    # the request will fail. So, execution always gets this far, and we should add to the SU table any SUs
                    # that were not 
                                    
                    # increment the refcount on all SU records that were already being processed before the new request processed
                    # the same SU records; this modifies the SU objects; it also acquires each SU's lock
                    if len(known) > 0:
                        rslog.writeDebug([ 'incrementing refcount for ' + ','.join([ str(sunum) for sunum in known ]) ])
                        suTableObj.incrementRefcount(known)
                    
                    # insert a new SU record for all unknown SUs that are already online; these calls modify the sus object;
                    # in this case, there was no SU inserted into the SU Table, but at least one request contained the SU; 
                    # if a pending request contains an SU that cannot be found (i.e., it is not in the SU Table), the 
                    # request will error-out; to avoid this, we need to insert the SU in the SU Table; an SU that is online 
                    # is essentially an SU that has downloaded successfully, so as we insert the SU into the SU Table, we 
                    # need to mark the SU complete
                    suTableObj.insert(sunums=toComplete)
                    
                    # this also acquires each SU's lock
                    suTableObj.setStatus(toComplete, 'C')
                    
                    # the new request has been fully processed; change its status from 'N' to 'P';
                    # this call modifies the requests object and it acquires each SU's lock
                    reqTableObj.setStatus([ arequest['requestid'] ], 'P')

                    # At this point, both the requests table and SU table have been modified, but have not been flushed to disk.
                    # Flush them, but do this inside a transaction so that the first does not happen without the second.
                    updateDbAndCommit(rslog, rsConn, rsDbLock, reqTableObj, suTableObj, [ arequest['requestid'] ], sunums)
                    
                th.enableInterrupts()
                # END PROCESS NEW REQUESTS #

                # Delete all request-table records whose state is 'D'. It doesn't matter if this operation gets interrupted. If
                # that happens, then these delete-pending records will be deleted the next time this code runs uninterrupted.
                # CLIENTS DO NOT APPEAR TO SET REQ STATUS TO D - THIS IS A NO-OP
                reqsToDelete = reqTableObj.getDelete()
                reqTableObj.deleteDB(reqsToDelete)
               
                # Must poll for new requests to appear in requests table.
                time.sleep(1)
                loopN += 1
                # I wonder if Python throws an exception if loopN rolls-over. Does it roll-over at the 32-bit or 64-bit boundary? 
                if loopN >= 0x7FFFFFFF:
                    loopN = 0
                # End of main loop.
            
            # Save the db state when exiting.
            rslog.writeInfo([ 'Remote-sums daemon is exiting. Saving database tables.' ])
            updateDbAndCommit(rslog, rsConn, rsDbLock, reqTableObj, suTableObj, None, None)

        # DB connection was terminated.            
        # Lock was released     
    except TerminationException as exc:
        msg = exc.args[0]
        if rslog:
            rslog.writeInfo([ msg ]) 
    except RemoteSumsException as exc:
        msg = exc.args[0]
        if rslog:
            rslog.writeError([ msg ])
            
        rv = exc.retcode
    except:
        import traceback
        
        msg = traceback.format_exc(5)
        if rslog:
            rslog.writeError([ msg ])
        else:
            print(msg, file=sys.stderr)
        rv = RET_UNKNOWN_ERROR

if rslog:
    rslog.writeInfo([ 'Exiting with return status ' + str(rv) + '.' ])
# Will not exit process if threads are still running. Set global shutdown flag that threads are monitoring. When they see the flag, they
# will terminate too.

logging.shutdown()
sys.exit(rv)
