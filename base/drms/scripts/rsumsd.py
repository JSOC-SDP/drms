#!/usr/bin/env python

from __future__ import print_function
import sys

if sys.version_info < (3, 2):
    raise Exception('You must run the 3.2 release, or a more recent release, of Python.')

import re
import os
import stat
import filecmp
import logging
import psycopg2
import threading
import fcntl
from datetime import datetime, timedelta, timezone
import urllib.request
import json
import signal
import time
from copy import deepcopy
import shutil
import psycopg2
import random
import argparse
sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../../include'))
from drmsparams import DRMSParams
sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../../base/libs/py'))
from drmsCmdl import CmdlParser
from drmsLock import DrmsLock
from subprocess import check_output, check_call, CalledProcessError, Popen, PIPE

# This script runs as a daemon at the site that has requested an SU that does not belong to the site. It is responsible for contacting
# the owning site and requesting the path to the desired SUs. The owning site must be running the rs.sh CGI to respond to the requesting
# site's request.

# There are three database tables: 1., a DRMS site table, 2., a request table, and 3., an SU table (sunum, starttime, status, errmsg)
# The site table (sitename, sitecode, baseurl) provides
# the information needed to query the providing site for the information needed to scp the requested SUs to the requesting site. The URL to the cgi
# program that provides scp information is formed by appending "rs.py" to baseurl. There are two parameters for this cgi: 1., "requestid", and 2., "sunums".
# During the initial request to the providing site, the requesting site will provide a requestid of "none", and a comma-separated list of SUNUMs
# in the sunums argument. Should the providing site have all the requested SUs online, then it will return the scp information needed to access those
# SUs, and a status of "complete". If, however, the SUs are not all online, the initial request will start an asynchronous tape-read of the offline SUs,
# returning a request ID that identifies the initial request, and a status of "pending". The requesting site must then poll for completion by
# periodically calling the cgi with a status request. To make a status request, the requestid argument contains the requestid returned by the
# inital CGI call, and the sunums argument contains "none". While the data are not ready, the status request returns a status of "pending". When
# the data are online, the status request returns a status of "complete".
#
# The request table (requestid, sunums, status) is populated by drms_storageunit.c. For all SUs that are offline and are not owned by the running
# DRMS/SUMS, the code inserts a record into the request table. The requestid is a UUID (within the running DRMS/SUMS) generated by a sequence table.
# The list of SUNUMs is put in sunums, and the initial status is set to 'N' (New request). drms_storageunit.c chunks such SUNUMs into
# manageable-sized requests. After inserting one or more such records into the request table, drms_storageunit.c then polls these records,
# waiting for the status to become 'C' (Complete request). After that happens, drms_storageunit.c then calls SUM_get() again on these SUs
# to obtain their paths. This daemon, rsumsd.py, periodically and reads all records in the request table. For each SU in each 'N' record
# (a single request, which could be requesting multiple SUs) the daemon first checks if the SU is already being processed. If so, then
# the SU table is not modified. The status of the record in the request table is set to 'P'. If the SU is not in the SU table, then
# this can mean one of two things. The daemon has never processed this SU, or the daemon has already processed this SU. When the daemon has completed
# processing an SU, it deletes the record for the SU from the SU table. If the latter is true, the daemon should not re-process the same SU. To distinguish
# between these two possibilities, the daemon first checks to see if the SU is already present in SUMS (it calls show_info -o sunum=SUNUM).
# If the SU is online, then the daemon does nothing. But if it is offline, then the daemon inserts a record for the SU into the SU table, and starts
# processing that SU. The status of that SU-table record is set to 'P', as is the status of the request record containing that SUNUM.
#
# For each SU in each 'P' request record, the daemon searches for records in the SU table. If one or more such records exist in the SU table, then
# the request record is left in the pending state. The next time the daemon scans the request records, it will again check the SU table looking
# for completion of all SUs. When that occurs, the status of the request record is set to 'C', indicating that the request is complete. The
# drms_storageunit.c code will then call SUMS to get the newly created paths to the requested SUs.

# Locks and thread synchronization
# The main thread looks for new and pending requests in the requests table. Since there is only a single thread accessing the requests
# table, there is no need to lock the requests table or its individual requests.
#
# The Storage Unit status, on the other hand, is accessed by both the Downloader threads and the ScpWorker threads. The latter queries the
# former for 'working' SUs - these are SUs that a Downloader thread has requested that an ScpWorker thread download. Each SU is assigned to a 
# single ScpWorker. Care must be taken so that two or more ScpWorkers do not operate on the same SU. The change of an SU's status from
# W to D by the ScpWorker thread must not be interrupted by another ScpWorker's call to getNextNWorking(). If an ScpWorker calls
# getNextNWorking() (which returns SUs with a status of W), but does not change the SUs statuses to D before a second ScpWorker
# calls getNextNWorking(), then both ScpWorkers could get assigned the same SU. The solution is for each ScpWorker to lock each SU
# before it checks its status for W, then change the status to D, then release the SU lock. To facilitate this locking, each ScpWorker
# thread first locks the SU Table, then it iterates through all SUs, locking each one, then checking its status. Once it collects
# all SUs with a status of W, it releases the SU Table lock, but it continues to hold on to the SU locks so it can change the statuses
# from W to D without interruption by the Downloader thread (see next paragraph).

# Both the Downloader and ScpWorker can change the status of an SU. Each thread must ensure that it does not overwrite a status change made
# by the other thread. If an ScpWorker sees an SU with a status of W, it will set the status to D before starting an scp. In the meantime, 
# the Downloader could set the status to S to signify a that the worker should be stopped. Without locks, the change to S by the 
# Downloader could be overwritten by the change to D by the ScpWorker. The solution is for both threads to acquire the SU lock, then 
# read the status before changing the status. So the ScpWorker thread will acquire the lock, see the W, set the status to D and then 
# start the scp and release the SU lock. The Downloader will acquire the SU lock before changing the status to S. In this manner, 
# the Downloader's change to S cannot be overwritten by the ScpWorker's change of status from W to D.

RET_SUCCESS = 0
RET_INVALIDARGS = 1
RET_LOCK = 2
RET_SUTABLE_READ = 3
RET_SUTABLE_WRITE = 4
RET_REQTABLE_READ = 5
RET_REQTABLE_WRITE = 6
RET_SITETABLE_LOAD = 7
RET_DBCOMMAND = 8
RET_LOGFILE = 9
RET_OFFLINE = 10
RET_GETRETENTION = 11
RET_UKNOWNREQUEST = 12
RET_UKNOWNSU = 13
RET_WORKERREF = 14
RET_UKNOWNSITECODE = 15
RET_DUPLICATESUNUM = 16
RET_DBUPDATE = 17
RET_SUMS = 18
RET_TERMINATED = 19
RET_TOOMANYTHREADS = 20
RET_UNKNOWNSTATUS = 21

LOG_FILE_BASE_NAME = 'rslog'

SUM_MAIN = 'public.sum_main'
SUM_PARTN_ALLOC = 'public.sum_partn_alloc'

def terminator(*args):
    # Raise the SystemExit exception (which will be caught by the __exit__() method below).
    sys.exit(0)

class TerminationHandler(object):
    def __new__(cls, thContainer):
        return super(TerminationHandler, cls).__new__(cls)

    def __init__(self, thContainer):
        self.container = thContainer
        arguments = thContainer[0]
        self.pidStr = thContainer[1]
        self.log = thContainer[2]
        
        self.lockFile = arguments.lockfile
        self.dbname = arguments.dbname
        self.dbuser = arguments.dbuser
        self.dbhost = arguments.dbhost
        self.dbport = arguments.dbport
        self.conn = None
        
        self.sumsdbname = arguments.sumsdbname
        self.sumsdbuser = arguments.sumsdbuser
        self.sumsdbhost = arguments.sumsdbhost
        self.sumsdbport = arguments.sumsdbport
        self.sumsconn = None

        super(TerminationHandler, self).__init__()
        
    def __enter__(self):
        signal.signal(signal.SIGINT, terminator)
        signal.signal(signal.SIGTERM, terminator)
        signal.signal(signal.SIGHUP, terminator)

        # Acquire locks.
        self.rsLock = DrmsLock(self.lockFile, self.pidStr)
        self.rsLock.acquireLock()
        
        # Make main DB connection to RS database. We also have to connect to the SUMS database, so connect to that too.
        # The connections are NOT in autocommit mode. If changes need to be saved, then conn.commit() must be called.
        # Do this instead of using BEGIN and END/COMMIT statements, cuz I don't know if the psycopg2/libpq interaction
        # supports this properly.
        try:
            self.conn = psycopg2.connect(database=self.dbname, user=self.dbuser, host=self.dbhost, port=self.dbport)
            rslog.writeInfo([ 'Connected to DRMS database ' + self.dbname + ' on ' + self.dbhost + ':' + str(self.dbport) + ' as user ' + self.dbuser + '.' ])

            self.sumsconn = psycopg2.connect(database=self.sumsdbname, user=self.sumsdbuser, host=self.sumsdbhost, port=self.sumsdbport)
            rslog.writeInfo([ 'Connected to SUMS database ' + self.sumsdbname + ' on ' + self.sumsdbhost + ':' + str(self.sumsdbport) + ' as user ' + self.sumsdbuser + '.' ])            
        except psycopg2.DatabaseError as exc:
            if self.sumsconn:
                self.sumsconn.close()
                self.sumsconn = None
    
            if self.conn:
                self.conn.close()
                self.conn = None

            self.log.writeError([ 'Unable to connect to a database (no, I do not know why).' ])

            # No need to close cursor - leaving the with block does that.
            self.container[3] = RET_DBCONNECT
        except psycopg2.Error as exc:
            if self.sumsconn:
                self.sumsconn.close()
                self.sumsconn = None
        
            if self.conn:
                self.conn.close()
                self.conn = None

            # Handle database-command errors.
            self.log.writeError([ exc.diag.message_primary ])
            self.container[3] = RET_DBCOMMAND

        return self

    # Normally, __exit__ is called if an exception occurs inside the with block. And since SIGINT is converted
    # into a KeyboardInterrupt exception, it will be handled by __exit__(). However, SIGTERM will not - 
    # __exit__() will be bypassed if a SIGTERM signal is received. Use the signal handler installed in the
    # __enter__() call to handle SIGTERM.
    def __exit__(self, etype, value, traceback):
        if etype is not None:
            # If the context manager was exited without an exception, then etype is None
            import traceback
            self.log.writeDebug([ traceback.format_exc(5) ])
                            
        if etype == SystemExit:
            self.log.writeInfo([ 'Termination signal handler called.' ])
            self.container[3] = RET_TERMINATED

        self.finalStuff()
        
        # Clean up lock
        try:     
            self.rsLock.releaseLock()   
            self.rsLock.close()
            self.rsLock = None
        except IOError:
            pass
            
        self.log.writeDebug([ 'Exiting TerminationHandler.' ])
        
    def finalStuff(self):
        self.log.writeInfo([ 'Halting threads.' ])

        # Shut-down ScpWorker threads. Send the sdEvent to all threads first, then wait after they have ALL received the message.
        # Otherwise, later threads could try to download SUs whose download was aborted by earlier threads.
        gotLock = False
        try:
            gotLock = ScpWorker.lock.acquire()
            if gotLock:
                for worker in ScpWorker.tList:
                    worker.stop()
        finally:
            if gotLock:
                ScpWorker.lock.release()

        while True:
            gotLock = False
            try:
                gotLock = ScpWorker.lock.acquire()
                if gotLock:
                    if len(ScpWorker.tList) > 0:
                        worker = ScpWorker.tList[0]
                    else:
                        break 
            finally:
                if gotLock:
                    ScpWorker.lock.release()
                    
            self.log.writeInfo([ 'Waiting for worker (ID ' + str(worker.id) + ') to halt.' ])
            worker.join()
            self.log.writeInfo([ 'Worker (ID ' +  str(worker.id) + ') halted.' ])   
        
        # Shut-down Downloader threads.
        gotLock = False
        try:
            gotLock = Downloader.lock.acquire()
            if gotLock:
                for downloader in Downloader.tList:
                    downloader.stop()
        finally:
            if gotLock:
                Downloader.lock.release() 

        while True:
            gotLock = False
            try:
                gotLock = Downloader.lock.acquire()
                if gotLock:
                    if len(Downloader.tList) > 0:
                        downloader = Downloader.tList[0]
                    else:
                        break 
            finally:
                if gotLock:
                    Downloader.lock.release()
                    
            self.log.writeInfo([ 'Waiting for downloader (SUNUM ' + str(downloader.sunum) + ') to halt.' ])
            downloader.join()
            self.log.writeInfo([ 'Downloader (SUNUM ' + str(downloader.sunum) + ') halted.' ])     
                                    
            
        # Shut-down ProviderPoller threads.
        gotLock = False
        try:
            gotLock = ProviderPoller.lock.acquire()
            if gotLock:
                for poller in ProviderPoller.tList:
                    poller.stop()
        finally:
            if gotLock:
                ProviderPoller.lock.release()

        while True:
            gotLock = False
            try:
                gotLock = ProviderPoller.lock.acquire()
                if gotLock:
                    if len(ProviderPoller.tList) > 0:
                        poller = ProviderPoller.tList[0]
                    else:
                        break 
            finally:
                if gotLock:
                    ProviderPoller.lock.release()

            self.log.writeInfo([ 'Waiting for poller (request ID ' + str(poller.requestID) + ') to halt.' ])
            poller.join()
            self.log.writeInfo([ 'Poller (request ID ' + str(poller.requestID) + ') halted.' ])   
        
        if self.sumsconn:
            self.sumsconn.close()
            self.sumsconn = None
                
        if self.conn:
            self.conn.close()
            self.conn = None
    
        self.log.flush()
        

    def rsConn(self):
        return self.conn

    def sumsConn(self):
        return self.sumsconn


class SumsDrmsParams(DRMSParams):
    def __init__(self):
        super(SumsDrmsParams, self).__init__()

    def get(self, name):
        val = super(SumsDrmsParams, self).get(name)

        if val is None:
            raise Exception('drmsParams', 'Unknown DRMS parameter: ' + name + '.')
        return val


class Arguments(object):

    def __init__(self, parser):
        # This could raise in a few places. Let the caller handle these exceptions.
        self.parser = parser
        
        # Parse the arguments.
        self.parse()
        
        # Set all args.
        self.setAllArgs()
        
    def parse(self):
        try:
            self.parsedArgs = self.parser.parse_args()      
        except Exception as exc:
            if len(exc.args) == 2:
                type, msg = exc
                  
                if type != 'CmdlParser-ArgUnrecognized' and type != 'CmdlParser-ArgBadformat':
                    raise # Re-raise

                raise Exception('args', msg)
            else:
                raise # Re-raise

    def setArg(self, name, value):
        if not hasattr(self, name):
            # Since Arguments is a new-style class, it has a __dict__, so we can
            # set attributes directly in the Arguments instance.
            setattr(self, name, value)
        else:
            raise Exception('args', 'Attempt to set an argument that already exists: ' + name + '.')

    def setAllArgs(self):
        for key,val in list(vars(self.parsedArgs).items()):
            self.setArg(key, val)
        
    def getArg(self, name):
        try:
            return getattr(self, name)
        except AttributeError as exc:
            raise Exception('args', 'Unknown argument: ' + name + '.')
            
    def dump(self, log):
        attrList = []
        for attr in sorted(vars(self)):
            attrList.append('  ' + attr + ':' + str(getattr(self, attr)))
        log.writeDebug([ '\n'.join(attrList) ])

class Log(object):
    """Manage a logfile."""
    def __init__(self, file, level, formatter):
        self.fileName = file
        self.log = logging.getLogger()
        self.log.setLevel(level)
        self.fileHandler = logging.FileHandler(file)
        self.fileHandler.setLevel(level)
        self.fileHandler.setFormatter(formatter)
        self.log.addHandler(self.fileHandler)
        
    def close(self):
        if self.log:
            if self.fileHandler:
                self.log.removeHandler(self.fileHandler)
                self.fileHandler.flush()
                self.fileHandler.close()
                self.fileHandler = None
            self.log = None
            
    def flush(self):
        if self.log and self.fileHandler:
            self.fileHandler.flush()
            
    def getLevel(self):
        # Hacky way to get the level - make a dummy LogRecord
        logRecord = self.log.makeRecord(self.log.name, self.log.getEffectiveLevel(), None, '', '', None, None)
        return logRecord.levelname

    def writeDebug(self, text):
        if self.log:
            for line in text:
                self.log.debug(line)
            self.fileHandler.flush()
            
    def writeInfo(self, text):
        if self.log:
            for line in text:
                self.log.info(line)
        self.fileHandler.flush()
    
    def writeWarning(self, text):
        if self.log:
            for line in text:
                self.log.warning(line)
            self.fileHandler.flush()
    
    def writeError(self, text):
        if self.log:
            for line in text:
                self.log.error(line)
            self.fileHandler.flush()
            
    def writeCritical(self, text):
        if self.log:
            for line in text:
                self.log.critical(line)
            self.fileHandler.flush()

class StorageUnit(object):
    def __init__(self, sunum, series, retention, starttime, refcount, status, errmsg):
        self.sunum = sunum
        self.series = series
        self.retention = retention
        self.starttime = starttime
        self.refcount = refcount
        self.status = status
        self.errmsg = errmsg
        
        # Not saved in the DB.
        self.giveUpTheGhost = False
        self.dirty = False
        self.new = False
        self.polling = False
        self.path = None
        self.suSize = None
        self.worker = None
        
        self.lock = threading.Lock()
        
    def acquireLock(self):
        return self.lock.acquire()
    
    def releaseLock(self):
        self.lock.release()
        
    def setDirty(self, value):
        if not isinstance(value, (bool)):
            raise Exception('invalidArg', 'setDirty(): argument must be a bool.')
        
        self.dirty = value
    
    # Set properties that are NOT saved to the DB.
    def setNew(self, value):
        if not isinstance(value, (bool)):
            raise Exception('invalidArg', 'setNew(): argument must be a bool.')
            
        self.new = value
        
    def setPolling(self, value):
        if not isinstance(value, (bool)):
            raise Exception('invalidArg', 'setNew(): argument must be a bool.')
            
        self.polling = value
        
    def getPath(self):
        path = None
        if hasattr(self, 'path'):
            path = self.path
        return path
        
    def setPath(self, value):
        if isinstance(value, (str)) or value is None:
            self.path = value
        else:
            raise Exception('invalidArg', 'setPath(): argument must be a str or None.')
            
    def setSUSize(self, value):
        if isinstance(value, (int)) or value is None:
            self.path = value
        else:
            raise Exception('invalidArg', 'setPath(): argument must be an int or None.')
    
    # Set properties that are saved to the DB.
    def setStatus(self, codeValue, msgValue):
        if not isinstance(codeValue, (str)):
            raise Exception('invalidArg', 'setStatus(): fist argument must be a str.')
            
        if not isinstance(msgValue, (str)) and msgValue is not None:
            raise Exception('invalidArg', 'setStatus(): second argument must be a str or None.')
            
        self.status = codeValue
        if msgValue is not None:
            self.errmsg = msgValue
        else:
            self.errmsg = ''
            
        self.setDirty(True)
            
    def setSeries(self, value):
        if not isinstance(value, (str)):
            raise Exception('invalidArg', 'setSeries(): argument must be a str.')

        self.series = value
        self.setDirty(True)
        
    def setRetention(self, value):
        if not isinstance(value, (int)):
            raise Exception('invalidArg', 'setRetention(): argument must be an integer.')
            
        self.retention = value
        self.setDirty(True)
    
    def setStarttime(self, value):
        if not isinstance(value, (datetime)):
            raise Exception('invalidArg', 'setStarttime(): argument must be a datetime.')
            
        self.starttime = value
        self.setDirty(True)
        
    def touch(self):
        self.setStarttime(datetime.now(timezone.utc))
        
    def incrementRefcount(self):
        self.refcount += 1
        self.setDirty(True)
        
    def decrementRefcount(self):
        if self.refcount == 0:
            raise Exception('noReference', 'Cannot decrement refcount on unreferenced SU record ' + str(self.sunum) + '.')
                    
        self.refcount -= 1
        if self.refcount == 0:
            self.giveUpTheGhost = True
            
        self.setDirty(True)
        
    def setWorker(self, value):
        if not isinstance(value, (Downloader)):
            raise Exception('invalidArg', 'setWorker(): argument must be a Downloader.')
            
        if self.worker:
            raise Exception('workerRef', 'Cannot set worker for SU ' + str(self.sunum) + '. Worker already exists.')
            
        self.worker = value


class SuTable:
    rsConn = None
    rsDbLock = None # There is one global lock for the rs connection. Since the main thread and the Downloader and ScpWorker threads
                    # all share the rs connection, they all need to use the same lock.

    sumsConn = None
    sumsDbLock = None # Since all Downloader threads and the main thread share the same connection, their cursors 
                      # on these connections are not isolated. Use a lock to ensure a consistent view in the
                      # offline() method.
    
    def __init__(self, tableName, timeOut, log):
        self.tableName = tableName
        self.timeOut = timeOut # A timedelta object - the length of time to wait for a download to complete.
        self.lock = threading.Lock()
        self.log = log
        self.suDict = {}
    
    def read(self):
        # sus(sunum, starttime, refcount, status, errmsg)
        cmd = 'SELECT sunum, series, retention, starttime, refcount, status, errmsg FROM ' + self.tableName
    
        try:
            gotRsDbLock = SuTable.rsDbLock.acquire()
            if gotRsDbLock:
                with SuTable.rsConn.cursor() as cursor:
                    cursor.execute(cmd)
            
                    for record in cursor:
                        sunumStr = str(record[0])

                        self.suDict[sunumStr] = {}
                        sunum = record[0]         # integer
                        series = record[1]        # text
                        retention = record[2]     # integer
                        # Whoa! pyscopg returns timestamps as datetime.datetime objects already!
                        starttime = record[3]     # datetime.datetime
                        refcount = record[4]      # integer
                        status = record[5]        # text
                        errmsg = record[6]        # text

                        su = StorageUnit(sunum, series, retention, starttime, refcount, status, errmsg)

                        # Not read from or saved to database.
                        su.setDirty(False)
                        su.setNew(False)
                        su.setPolling(False)
                        su.setPath(None)              # The server path of the SU to be downloaded.
                        # worker is already None
                        self.suDict[sunumStr] = su
        except psycopg2.Error as exc:
            raise Exception('sutableRead', exc.diag.message_primary)
        finally:
            SuTable.rsConn.rollback() # We read from the DB only, so no need to commit anything.
            if gotRsDbLock:
                SuTable.rsDbLock.release()

    def tryRead(self):
        nAtts = 0
        while True:
            try:
                self.read()
                break
            except Exception as exc:
                if len(exc.args) != 2:
                    raise # Re-raise

                etype = exc.args[0]

                if etype == 'sutableRead':
                    if nAtts > 10:
                        raise # Re-raise
                else:
                    raise

            nAtts += 1
            time.sleep(1)
    
    # This will NOT commit database changes. You are generally going to want to do this as part of a larger db change. Commit the changes
    # after all changes that form the atomic set of changes. Do not call rsConn.commit().
    #
    # ACQUIRES THE SU TABLE LOCK.
    def updateDB(self, sunums=None):
        gotTableLock = False

        try:
            gotTableLock = self.acquireLock()

            if gotTableLock is None:
                raise Exception('lock', 'Unable to acquire SU-table lock.')
            
            # Will get all SUs if sunums is None.
            sus = self.get(sunums)

            for su in sus:
                try:
                    gotSULock = su.acquireLock()            
                    if not gotSULock:
                        raise Exception('lock', 'Unable to acquire SU ' + str(su.sunum) + ' lock.')
                    
                    if su.giveUpTheGhost:
                        cmd = 'DELETE FROM ' + self.tableName + ' WHERE sunum = ' + str(su.sunum)
                        needsRollback = True
                        try:
                            gotRsDbLock = SuTable.rsDbLock.acquire()
                            if gotRsDbLock:
                                with SuTable.rsConn.cursor() as cursor:
                                    cursor.execute(cmd)

                                needsRollback = False
                                # The cursor has been closed, but the transaction has not been committed, as designed.
                        except psycopg2.Error as exc:
                            raise Exception('sutableWrite', exc.diag.message_primary)
                        finally:
                            if needsRollback:
                                SuTable.rsConn.rollback()
                            if gotRsDbLock:
                                SuTable.rsDbLock.release()
                            
                        del self.suDict[str(su.sunum)]
                    elif su.dirty:
                        if su.new:
                            cmd = 'INSERT INTO ' + self.tableName + '(sunum, series, retention, starttime, refcount, status, errmsg) VALUES(' + str(su.sunum) + ",'" + su.series + "', " + str(su.retention) + ", '" + su.starttime.strftime('%Y-%m-%d %T%z') + "', " + str(su.refcount) + ", '" + su.status + "', '" + su.errmsg + "')"
                        else:
                            cmd = 'UPDATE ' + self.tableName + " SET series='" + su.series + "', retention=" + str(su.retention) + ", starttime='" + su.starttime.strftime('%Y-%m-%d %T%z') + "', refcount=" + str(su.refcount) + ", status='" + su.status + "', errmsg='" + su.errmsg + "' WHERE sunum=" + str(su.sunum)
                
                        self.log.writeInfo(['Updating SU db table: ' + cmd])
                
                        needsRollback = True
                        try:
                            gotRsDbLock = SuTable.rsDbLock.acquire()
                            if gotRsDbLock:
                                with SuTable.rsConn.cursor() as cursor:
                                    cursor.execute(cmd)
                                needsRollback = False
                                # The cursor has been closed, but the transaction has not been committed, as designed.
                        except psycopg2.Error as exc:
                            import traceback
                            self.log.writeError([ traceback.format_exc(5) ])
                            raise Exception('sutableWrite', traceback.format_exc(5))
                        finally:
                            if needsRollback:
                                SuTable.rsConn.rollback()
                            if gotRsDbLock:
                                SuTable.rsDbLock.release()

                        su.setDirty(False)
                        su.setNew(False)
                finally:
                    if gotSULock:
                        su.releaseLock()
        finally:
            # Always release lock.
            if gotTableLock:
                self.releaseLock()
        
    # This WILL commit changes to the db.
    def updateDbAndCommit(self, sunums=None):
        needsCommit = False
        
        try:
            self.updateDB(sunums)
            needsCommit = True
        finally:
            if needsCommit:
                SuTable.rsConn.commit()
            else:
                SuTable.rsConn.rollback()

    def acquireLock(self):
        return self.lock.acquire()
        # self.log.writeDebug(['Acquired SU-Table lock.'])
    
    def releaseLock(self):
        self.lock.release()
        # self.log.writeDebug(['Released SU-Table lock.'])
    
    # Main thread only.
    def insert(self, sunums):
        try:
            # Get lock because we are modifying self.suDict.
            gotTableLock = self.acquireLock()
            for asunum in sunums:
                sunumStr = str(asunum)
        
                if sunumStr in self.suDict:
                    raise Exception('knownSunum', 'SU-table record already exists for SU ' + sunumStr + '.')
                
                su = StorageUnit(asunum, '', -1, datetime.now(timezone.utc), 1, 'P', '')

                su.setDirty(True)
                # Set the new flag (so that the record will be INSERTed into the SU database table instead of UPDATEd).
                su.setNew(True)
                # The polling flag is set only while we are waiting for a providing site to give us paths for SUs.
                su.setPolling(False)
        
                self.suDict[sunumStr] = su
        finally:
            # Always release lock.
            if gotTableLock:
                self.releaseLock()

    def setStatus(self, sunums, code, msg=None):
        for asunum in sunums:
            sunumStr = str(asunum)
            
            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            
            try:
                gotSULock = su.acquireLock()
            
                if gotSULock is None:
                    raise Exception('lock', 'Unable to acquire SU ' + sunumStr + ' lock.')
                su.setStatus(code, msg)
            finally:
                if gotSULock:
                    su.releaseLock()

    def setSeries(self, sunums, series):
        for asunum in sunums:
            sunumStr = str(asunum)
            
            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            
            try:
                gotSULock = su.acquireLock()
            
                if gotSULock is None:
                    raise Exception('lock', 'Unable to acquire SU ' + sunumStr + ' lock.')
                su.setSeries(series)
            finally:
                if gotSULock:
                    su.releaseLock()

    def setRetention(self, sunums, retention):
        for asunum in sunums:
            sunumStr = str(asunum)
            
            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            
            try:
                gotSULock = su.acquireLock()
            
                if gotSULock is None:
                    raise Exception('lock', 'Unable to acquire SU ' + sunumStr + ' lock.')
                su.setRetention(retention)
            finally:
                if gotSULock:
                    su.releaseLock()

    def setStarttime(self, sunums, starttime):
        for asunum in sunums:
            sunumStr = str(asunum)
            
            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            
            try:
                gotSULock = su.acquireLock()
            
                if gotSULock is None:
                    raise Exception('lock', 'Unable to acquire SU ' + sunumStr + ' lock.')
                su.setStarttime(starttime)
            finally:
                if gotSULock:
                    su.releaseLock()

    def incrementRefcount(self, sunums):
        for asunum in sunums:
            sunumStr = str(asunum)
            
            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            
            try:
                gotSULock = su.acquireLock()
            
                if gotSULock is None:
                    raise Exception('lock', 'Unable to acquire SU ' + sunumStr + ' lock.')
            
                su.incrementRefcount()
            finally:
                if gotSULock:
                    su.releaseLock()

    def decrementRefcount(self, sunums):
        toDel = []
        for asunum in sunums:
            sunumStr = str(asunum)

            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            
            try:
                gotSULock = su.acquireLock()
                
                if gotSULock is None:
                    raise Exception('lock', 'Unable to acquire SU ' + sunumStr + ' lock.')
                    
                su.decrementRefcount()
            finally:
                if gotSULock:
                    su.releaseLock()

    def getWorker(self, sunum):
        sunumStr = str(sunum)
        
        # Will raise if the sunum is not in the dictionary.
        su = self.get([ sunum ])[0]
        return su.worker
        
    def setWorker(self, sunum, worker):
        sunumStr = str(sunum)

        # Will raise if the sunum is not in the dictionary.
        su = self.get([ sunum ])[0]
        
        try:
            gotSULock = su.acquireLock()
        
            if gotSULock is None:
                raise Exception('lock', 'Unable to acquire SU ' + sunumStr + ' lock.')
            su.setWorker(worker)
        finally:
            if gotSULock:
                su.releaseLock()
    
    def stopWorker(self, sunum):
        sunumStr = str(sunum)
        
        self.log.writeInfo(['Stopping worker for ' + sunumStr + '.'])
        
        # Will raise if the sunum is not in the dictionary.
        su = self.get([ sunum ])[0]
            
        if su.worker:
            su.worker.stop()
            if su.worker.isAlive():
                # Give the worker 15 seconds to self-terminate.
                su.worker.join(15)
            if su.worker.isAlive():
                # Apparently, there is no way to kill a thread from another thread. So, we are just going to
                # orphan the thread (so it doesn't use up our maxThreads quota).
                try:
                    Downloader.lock.acquire()
                    self.log.writeDebug(['(stopWorker) Class Downloader acquired Downloader lock for SU ' + sunumStr + '.'])
                    Downloader.tList.remove(su.worker) # This thread is no longer one of the running threads.
                    if len(Downloader.tList) <= Downloader.maxThreads - 1:
                        # Fire event so that main thread can add new SUs to the download queue.
                        Downloader.eventMaxThreads.set()
                        # Clear event so that main will block the next time it calls wait.
                        Downloader.eventMaxThreads.clear()
                finally:
                    Downloader.lock.release()
                    self.log.writeDebug(['(stopWorker) Class Downloader released Downloader lock for SU ' + sunumStr + '.'])

            del su.worker
            su.worker = None

    def get(self, sunums=None):
        toRet = []
        
        if not sunums:
            for sunumStr, su in self.suDict.items():
                toRet.append(su)
        else:
            for asunum in sunums:
                sunumStr = str(asunum)
        
                if not sunumStr in self.suDict or not self.suDict[sunumStr]:
                    raise Exception('unknownSunum', '(get) No SU-table record exists for SU ' + sunumStr + '.')

                toRet.append(self.suDict[sunumStr])

        return toRet
        
    # This will acquire the SU lock. The caller must release it!! Because the SU lock is acquired while the 
    # SU-table lock is held, nobody can modify the SU after this method returns and before the caller
    # releases the SU lock.
    def getAndLockSU(self, sunum):
        gotTableLock = False

        try:
            gotTableLock = self.acquireLock()

            if gotTableLock is None:
                raise Exception('lock', 'Unable to acquire SU-table lock.')
                
            su = self.get([ sunum ])[0]
            gotSULock = su.acquireLock()
            
            if gotSULock is None:
                raise Exception('lock', 'Unable to acquire SU ' + str(sunum) + ' lock.')
        except:
            su = None
            raise
        finally:
            # Always release lock.
            if gotTableLock:
                self.releaseLock()

        return su

    def getAndLockSUs(self, sunums):
        gotTableLock = False

        try:
            gotTableLock = self.acquireLock()

            if gotTableLock is None:
                raise Exception('lock', 'Unable to acquire SU-table lock.')
                
            sus = self.get(sunums)
            for su in sus:
                gotSULock = su.acquireLock()            
                if not gotSULock:
                    raise Exception('lock', 'Unable to acquire SU ' + str(su.sunum) + ' lock.')
        except:
            sus = None
            raise
        finally:
            # Always release lock.
            if gotTableLock:
                self.releaseLock()
                
        return sus        

    # Returns a list of SU objects.
    # NOT THREAD SAFE! This method is called by the main thread on start-up, before any other threads have been started.
    def getPending(self):
        pending = []
        
        for sunumStr, su in self.suDict.items():                
            if su.status == 'P':
                su.touch()
                pending.append(su)

        # Sorts in place - and returns None. The SUs will be sorted by starttime later, before Downloader threads are created
        # for them.
        pending.sort(key=lambda ansu : ansu.sunum)

        return pending
        
    # Returns a list of SU objects.
    # NOT THREAD SAFE! This method is called by the main thread on start-up, before any other threads have been started.
    def getWorking(self):
        working = []
        
        for sunumStr, su in self.suDict.items():                
            if su.status == 'W':
                su.touch()
                working.append(su)

        # Sorts in place - and returns None. The SUs will be sorted by starttime later, before Downloader threads are created
        # for them.
        working.sort(key=lambda ansu : ansu.sunum)

        return working
    
    # Returns num SU entries whose status is 'W'. If fewer than num W entries exist, then all W SU entries are returned.
    # Must acquire SU table lock before calling this method. There is no need to lock the individual SUs because the ScpWorker
    # threads must acquire the SU Table lock before calling getNextNWorking(). No other ScpWorker thread can 
    # change a W to a D while this ScpWorker is collecting SUs with a W status.
    #
    # Sort by start time - process the oldest first.
    #
    # All SUs returned MUST have the same scpUser, scpHost, and scpPort.
    def getNextNWorking(self, num):
        nworking = []
        scpUser = None
        scpHost = None
        scpPort = None
        scpInfoSet = False
        
        sortedSUs = sorted(list(self.suDict.values()), key=lambda su : su.starttime.strftime('%Y-%m-%d %T'))
        it = iter(sortedSUs)
        try:
            while num > 0:
                su = next(it)

                if su.status == 'W':
                    su.touch()
                    if su.worker is None:
                        raise Exception('workerRef', 'No worker thread assigned to SU ' + str(su.sunum) + '.')
                    if scpInfoSet == False:
                        scpUser = su.worker.scpUser
                        scpHost = su.worker.scpHost
                        scpPort = su.worker.scpPort
                        scpInfoSet = True

                    if su.worker.scpUser == scpUser and su.worker.scpHost == scpHost and su.worker.scpPort == scpPort:
                        nworking.append(su)
                        num -= 1
                else:
                    pass
        except StopIteration:
            pass
                
        return (nworking, (scpUser, scpHost, scpPort))

    def getTimeout(self):
        return self.timeOut

    @classmethod
    def offline(cls, sunums, binPath, log):
        # There is not an efficient way to check for the SU being on/offline. But we can use jsoc_fetch (vs. show_info - jsoc_fetch returns
        # JSON, which is handy). And it also can be called in a mode where it does not trigger a SUM_get() - it uses SUM_infoAns():
        #   op=exp_su requestid=NOASYNCREQUEST sunum=123456789 format=json formatvar=dataobj method=url_quick protocol=as-is
        rv = []        

        if len(sunums) > 0:
            # Ok, 86 jsoc_fetch. Using it is ridiculous (lots and lots of overhead for what is a simple DB query), and 
            # sometimes it crashes. We simply need to query the SUMS db and check to see which of the SUs in sunums 
            # are present in the sum_main/sum_partn_alloc table.             
            cmd = "SELECT T1.ds_index, T1.online_loc, T1.online_status, T1.archive_status, T1.offsite_ack, T1.history_comment, T1.owning_series, T1.storage_group, T1.bytes, T1.create_sumid, T1.creat_date, T1.username, COALESCE(T1.arch_tape, 'N/A'), COALESCE(T1.arch_tape_fn, 0), COALESCE(T1.arch_tape_date, '1958-01-01 00:00:00'), COALESCE(T1.safe_tape, 'N/A'), COALESCE(T1.safe_tape_fn, 0), COALESCE(T1.safe_tape_date, '1958-01-01 00:00:00'), COALESCE(T2.effective_date, '195801010000'), coalesce(T2.status, 0), coalesce(T2.archive_substatus, 0) FROM " + SUM_MAIN + " AS T1 LEFT OUTER JOIN " + SUM_PARTN_ALLOC + " AS T2 ON (T1.ds_index = T2.ds_index) WHERE T1.ds_index IN (" + ','.join([ str(asunum) for asunum in sunums ]) + ')'

            try:
                gotSumsDbLock = SuTable.sumsDbLock.acquire()
                if gotSumsDbLock:
                    with SuTable.sumsConn.cursor() as cursor:
                        knownSUs = {} # bitmap of SUs known to the SUMS db.
                        try:                    
                            cursor.execute(cmd)
                            log.writeDebug([ 'Successfully queried SUMS db for known SUs.' ])
                        
                            # Put each SU in the result into the bitmap.
                            for row in cursor:
                                knownSUs[str(row[0])] = True
                        
                            # Determine which SUs for which we are going to initiate Downloaders that are not already in SUMS.
                            for ansunum in sunums:                        
                                if str(ansunum) not in knownSUs:
                                    rv.append(ansunum)
                        except psycopg2.Error as exc:
                            # Handle database-command errors. These are all due to problems communicating with the SUMS db.
                            raise Exception('sumsAPI', exc.diag.message_primary + ': ' + cmd + '.') 
                        finally:
                            # Not making changes, so rollback always.
                            SuTable.sumsConn.rollback()
            finally:
                if gotSumsDbLock:
                    SuTable.sumsDbLock.release()

        return rv

class ReqTable:
    rsConn = None
    rsDbLock = None # There is one global lock for the rs connection. Since the main thread and the Downloader and ScpWorker threads
                    # all share the rs connection, they all need to use the same lock.


    def __init__(self, tableName, timeOut, log):
        self.tableName = tableName
        self.timeOut = timeOut
        self.log = log
        self.reqDict = {}
    
    def read(self):
        # requests(requestid, starttime, sunums, status, errmsg)
        cmd = 'SELECT requestid, dbhost, dbport, dbname, starttime, sunums, status, errmsg FROM ' + self.tableName
        
        try:
            gotRsDbLock = ReqTable.rsDbLock.acquire()
            if gotRsDbLock:
                with ReqTable.rsConn.cursor() as cursor:
                    cursor.execute(cmd)
        
                    for record in cursor:
                        requestidStr = str(record[0])

                        self.reqDict[requestidStr] = {}
                        self.reqDict[requestidStr]['requestid'] = record[0] # integer
                        self.reqDict[requestidStr]['dbhost'] = record[1]    # text
                        self.reqDict[requestidStr]['dbport'] = record[2]    # integer
                        self.reqDict[requestidStr]['dbname'] = record[3]    # text
                        # Whoa! pyscopg returns timestamps as datetime.datetime objects already!
                        self.reqDict[requestidStr]['starttime'] = record[4] # datetime.datetime
                        self.reqDict[requestidStr]['sunums'] = [int(asunum) for asunum in record[5].split(',')] # text (originally)
                        self.reqDict[requestidStr]['status'] = record[6]    # text
                        self.reqDict[requestidStr]['errmsg'] = record[7]    # text
                        self.reqDict[requestidStr]['dirty'] = False
        except psycopg2.Error as exc:
            raise Exception('reqtableRead', exc.diag.message_primary, cmd)
        finally:
            ReqTable.rsConn.rollback()
            
            if gotRsDbLock:
                ReqTable.rsDbLock.release()

    def tryRead(self):
        nAtts = 0
        while True:
            try:
                self.read()
                break
            except Exception as exc:
                if len(exc.args) != 2:
                    raise # Re-raise

                etype = exc.args[0]

                if etype == 'reqtableRead':
                    if nAtts > 10:
                        raise # Re-raise
                else:
                    raise

            nAtts += 1
            time.sleep(1)

    # This method finds 'N' records inserted since the last time it was run (or since the table was first read). It ignores
    # all other changes to the database table (made from outside this program) that have happened. To read those changes,
    # shut down this program, then make the changes, then start this program again.
    def refresh(self):
        # Delete existing items from self.
        self.reqDict = {}
        
        # Read the table from the database anew.
        self.tryRead()

    # This will NOT commit database changes. You are generally going to want to do this as part of a larger db change. Commit the changes
    # after all changes that form the atomic set of changes.
    def updateDB(self, requestids=None):
        if requestids:
            # Update the specified records.
            for arequestid in requestids:
                requestidStr = str(arequestid)
            
                if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                    raise Exception('unknownRequestid', 'No request-table record exists for ID ' + requestidStr + '.')
                
                if self.reqDict[requestidStr]['dirty']:
                    # The only columns that this daemon will modify are status and errmsg.
                    cmd = 'UPDATE ' + self.tableName + " SET status='" + self.reqDict[requestidStr]['status'] + "', errmsg='" + self.reqDict[requestidStr]['errmsg'] + "' WHERE requestid=" + requestidStr
                    
                    needsRollback = True
                    try:
                        gotRsDbLock = ReqTable.rsDbLock.acquire()
                        if gotRsDbLock:
                            with ReqTable.rsConn.cursor() as cursor:
                                cursor.execute(cmd)
                            needsRollback = False
                            # The cursor has been closed, but the transaction has not been committed, as designed.
                    except psycopg2.Error as exc:
                        raise Exception('reqtableWrite', exc.diag.message_primary)
                    finally:
                        if needsRollback:
                            ReqTable.rsConn.rollback()
                        if gotRsDbLock:
                            ReqTable.rsDbLock.release()

                    self.reqDict[requestidStr]['dirty'] = False
        else:
            # Update all dirty records.
            for requestidStr in self.reqDict:
                if self.reqDict[requestidStr]['dirty']:
                    # The only columns that this daemon will modify are status and errmsg.
                    cmd = 'UPDATE ' + self.tableName + " SET status='" + self.reqDict[requestidStr]['status'] + "', errmsg='" + self.reqDict[requestidStr]['errmsg'] + "' WHERE requestid='" + requestidStr + "'"
                    
                    needsRollback = True
                    try:
                        gotRsDbLock = ReqTable.rsDbLock.acquire()
                        if gotRsDbLock:
                            self.log.writeDebug([ 'Running DB command: ' + cmd +'.' ])
                            with ReqTable.rsConn.cursor() as cursor:
                                cursor.execute(cmd)
                            needsRollback = False
                            self.log.writeDebug([ 'DB command succeeded: ' + cmd +'.' ])
                            # The cursor has been closed, but the transaction has not been committed, as designed.
                    except psycopg2.Error as exc:
                        raise Exception('reqtableWrite', exc.diag.message_primary)
                    finally:
                        if needsRollback:
                            ReqTable.rsConn.rollback()
                        if gotRsDbLock:
                            ReqTable.rsDbLock.release()
                    
                    self.reqDict[requestidStr]['dirty'] = False
                    
    # This WILL commit changes to the db.
    def updateDbAndCommit(self, requestids=None):
        needsCommit = False
        
        try:
            self.updateDB(requestids)
            needsCommit = True
        finally:
            if needsCommit:
                ReqTable.rsConn.commit()
            else:
                ReqTable.rsConn.rollback()

    # This will NOT commit database changes. You are generally going to want to do this as part of a larger db change. Commit the changes
    # after all changes that form the atomic set of changes. Do not call rsConn.commit().
    def deleteDB(self, requestids):
        if len(requestids) > 0:
            for arequestid in requestids:
                requestidStr = str(requestid)
                
                if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                    raise Exception('unknownRequestid', 'No request-table record exists for ID ' + requestidStr + '.')

                del self.reqDict[requestidStr]
            
            reqidLstStr = ','.join(requestids)
            
            cmd = 'DELETE FROM ' + self.tableName + ' WHERE requestid=' + reqidLstStr
            
            needsRollback = True
            try:
                gotRsDbLock = ReqTable.rsDbLock.acquire()
                if gotRsDbLock:
                    with ReqTable.rsConn.cursor() as cursor:
                        cursor.execute(cmd)
                    needsRollback = False
                    # The cursor has been closed, but the transaction has not been committed, as designed.
            except psycopg2.Error as exc:
                raise Exception('reqtableWrite', exc.diag.message_primary + ': ' + cmd)
            finally:
                if needsRollback:
                    ReqTable.rsConn.rollback()
                if gotRsDbLock:
                    ReqTable.rsDbLock.release()

    def setStatus(self, requestids, code, msg=None):
        for arequestid in requestids:
            requestidStr = str(arequestid)
        
            if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                raise Exception('unknownRequestid', 'No request-table record exists for ID ' + requestidStr + '.')
            
            self.reqDict[requestidStr]['status'] = code
            if msg:
                self.reqDict[requestidStr]['errmsg'] = msg
            else:
                self.reqDict[requestidStr]['errmsg'] = ''
            
            # Set dirty flag
            self.reqDict[requestidStr]['dirty'] = True

    def get(self, requestids=None):
        toRet = []
    
        if not requestids:
            return [ self.reqDict[key] for (key, val) in self.reqDict.items() ]
        
        for arequestid in requestids:
            requestidStr = str(arequestid)
            
            if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                raise Exception('unknownRequestid', 'No request-table record exists for ID ' + requestidStr + '.')
    
            toRet.append(self.reqDict[requestidStr])
    
        return toRet        
    
    def getPending(self):
        pendLst = []

        for requestidStr, reqObj in self.reqDict.items():
            if reqObj['status'] == 'P':
                pendLst.append(reqObj)

        # Sort by start time. Sorts in place - and returns None.
        pendLst.sort(key=lambda dict : dict['starttime'].strftime('%Y-%m-%d %T'))
    
        return pendLst
    
    def getNew(self):
        newLst = []                
                
        for requestidStr, reqObj in self.reqDict.items():
            if reqObj['status'] == 'N':
                newLst.append(reqObj)

        # Sort by start time. Sorts in place - and returns None.
        newLst.sort(key=lambda dict : dict['starttime'].strftime('%Y-%m-%d %T'))

        return newLst

    def getDelete(self):
        deleteLst = []
                
        for requestidStr, reqObj in self.reqDict.items():
            if reqObj['status'] == 'D':
                deleteLst.append(reqObj)
                
        deleteLst.sort(key=lambda dict : dict['requestid'])

        return deleteLst

    def getTimeout(self):
        return self.timeOut

    # series (text) - Name of series whose retention is wanted.
    # request (ReqTable::reqDict[requestidStr] object) - Contains the dbhost, dbport, dbname that identifies the
    #    database that contains the series.
    @staticmethod
    def getRetention(series, request, dbuser, log):
        # Who you gonna call...jsoc_info! A la:
        # jsoc_info op=series_struct ds=hmi.v_avg120
        # Ack - there is Stanford-specific stuff in jsoc_info. Instead, just get the retention from the db. We have the
        # host/port/dbname information from request.
        newSuRetention = -1
        ns, tab = series.split('.')

        try:
            # We need to get information from the DRMS database, which might not be the same database as the remote SUMS database.
            # The class variable ReqTable.rsConn is the connection to the remote SUMS database.
            with psycopg2.connect(database=request['dbname'], user=dbuser, host=request['dbhost'], port=request['dbport']) as conn:
                with conn.cursor() as cursor:
                    # We want the new-SU retention too, not the staging retention. So extract the bottom 15 bits.
                    cmd = "SELECT retention & x'00007FFF'::int AS retention FROM " + ns + ".drms_series WHERE seriesname ILIKE '" + series + "'"
                    log.writeInfo(['Obtaining new-SU retention for series ' + series + ': ' + cmd])

                    try:
                        cursor.execute(cmd)
                        newSuRetention = cursor.fetchone()[0]
                        log.writeInfo(['Retention is ' + str(newSuRetention) + '.'])
                    except psycopg2.Error as exc:
                        # Handle database-command errors.
                        raise Exception('getRetention', exc.diag.message_primary)
            # The connection is read-only, so there is not need to commit a transaction.
        except psycopg2.DatabaseError as exc:
            # Closes the cursor and connection

            # Man, there is no way to get an error message from any exception object that will provide any information why
            # the connection failed.
            msg = 'Unable to connect to the database (no, I do not know why).'
            raise Exception('getRetention', msg)

        return newSuRetention

# The site information is stored in a public database table at Stanford. It is accessible by all remote sites
# with the sites.py cgi. To obtain information about all sites, the sites.py cgi is called with no parameters.
# Otherwise, information about a single site can be obtained by by providing the site name to the 'site' argument.
# The information is stored in drms.rs_sites on hmidb.
class SiteTable:
    def __init__(self, log):
        self.log = log
        self.siteDict = {} # Keyed by name.
        self.siteMap = {} # Map from str(code) to name.

    def read(self):
        url = 'http://jsoc.stanford.edu/cgi-bin/rssites.sh'
        with urllib.request.urlopen(url) as response:
            siteInfoStr = response.read().decode('UTF-8')

        # siteInfoStr is a string, that happens to be json.
        siteInfo = json.loads(siteInfoStr)
        
        if siteInfo['status'] != 'success':
            raise Exception('sitetableRead', "Failure calling cgi '" + url + "'.")

        # siteInfo is a dictionary, keyed by site name. Each dictionary entry is a dictionay, with two keys: code and baseurl.
        for asite, info in siteInfo.items():
            if asite == 'status':
                # Skip status.
                continue
            self.siteDict[asite] = {}
            self.siteDict[asite]['name'] = asite
            self.siteDict[asite]['code'] = info['code']
            self.siteDict[asite]['baseurl'] = info['baseurl']
            self.siteMap[str(self.siteDict[asite]['code'])] = asite

            self.log.writeInfo([ 'Reading site info for ' + asite + ': code => ' + str(self.siteDict[asite]['code']) + ', baseurl => ' + self.siteDict[asite]['baseurl'] ])

    def tryRead(self):
        nAtts = 0
        while True:
            try:
                self.read()
                break
            except Exception as exc:
                if len(exc.args) != 2:
                    raise # Re-raise

                etype = exc.args[0]

                if etype == 'sitetableRead':
                    if nAtts > 10:
                        raise # Re-raise
                else:
                    raise

            nAtts += 1
            time.sleep(1)

    @staticmethod
    def getCode(sunum):
        code = sunum >> 48
        if code & 0xC000 != 0:
            raise Exception('badSunum', 'The site-code value of SUNUM ' + sunum + ' is out of range (valid range is 0 to 16383).')

        return code

    def getURL(self, sunum):
        code = SiteTable.getCode(sunum)
        
        if not str(code) in self.siteMap or not self.siteMap[str(code)]:
            raise Exception('unknownSitecode', 'There is no site in the site table for code ' + str(code) + '.')
        
        name = self.siteMap[str(code)]
        url = self.siteDict[name]['baseurl']
        return url

class Chunker(object):
    def __init__(self, list, chSize):
        self.chunks = []
        iChunk = -1
        nElem = 1
        
        for elem in list:
            if iChunk == -1 or nElem % chSize == 0:
                iChunk += 1
                self.chunks.append([])
    
            self.chunks[iChunk].append(elem)
            nElem += 1
    
    def __iter__(self):
        return self.iterate()
    
    # Iterate through chunks.
    def iterate(self):
        i = 0
        while i < len(self.chunks):
            yield self.chunks[i]
            i += 1
            
class ScpWorker(threading.Thread):
    tList = [] # A list of running thread IDs.
    maxThreads = 64
    scpNeeded = threading.Event() # Event fired by main when a Downloader thread has changed the state of an SU from 'P' to 'W'
    scpCompleted = threading.Event()  # Event fired by ScpWorker when an ScpWorker thread had completed an SU download
    lock = threading.Lock() # Guard tList.
    
    def __init__(self, id, suTable, arguments, log):
        threading.Thread.__init__(self)
        self.id = id
        self.suTable = suTable
        self.tmpdir = arguments.tmpdir
        self.arguments = arguments
        self.log = log
        self.sdEvent = threading.Event()

    def run(self):
        gotTableLock = False
        gotSULock = False
        maxNumSUs = self.arguments.scpMaxNumSUs
        maxPayloadSUs = self.arguments.scpMaxPayload
        scpTimeOut = self.arguments.scpTimeOut
        doDownload = False
        lastDownloadTime = None
        
        while not self.sdEvent.isSet():
            self.sunums = []
            paths = []

            try:
                gotTableLock = self.suTable.acquireLock()
                
                if not gotTableLock:
                    raise Exception('lock', 'Unable to acquire SU-table lock.')
                                
                # Look for an SU whose status is 'W' (which means that a Downloader thread is requesting an ScpWorker thread perform a 
                # download for it). If we find one, set status to 'D' and download the SU. When the download is complete, set the status
                # to 'P' so the requesting Downloader thread can clean-up and set the status to 'C' for the main thread to handle.
                (workingSUs, (user, host, port)) = self.suTable.getNextNWorking(maxNumSUs)
                susToDownload = []
                
                doDownload = False
                
                numSUs = len(workingSUs)
                # No need to lock su for suSize - that gets set before ScpWorker sees the SU.
                payload = sum([ su.worker.suSize for su in workingSUs ])
                currentTime = datetime.now(timezone.utc)

                if numSUs > 0:
                    doDownload = payload > maxPayloadSUs or numSUs == maxNumSUs
                    
                    # Now we need to modify workingSUs if the payload would be too large, or there would be too many SUs for this scp.
                    payload = 0
                    numSUs = 0
                    for su in workingSUs:
                        if (payload + su.worker.suSize > maxPayloadSUs and numSUs > 0) or numSUs + 1 > maxNumSUs:
                            break
                        
                        susToDownload.append(su)
                        payload += su.worker.suSize
                        numSUs += 1                    
                    
                    self.log.writeDebug([ 'doDownload calc (payload, maxPayLoadSUs, numSUs, maxNumSUs, currentTime, lastDownloadTime, scpTimeOut): ' + str(payload) + ',' + str(maxPayloadSUs) + ',' + str(numSUs) + ',' + str(maxNumSUs) + ',' + str(currentTime) + ',' + str(lastDownloadTime) + ',' + str(scpTimeOut) + ')' ])

                    if doDownload:                            
                        # Set lastDownloadTime. We will keep setting this, until we complete the scp - that is the best place to
                        # set it, but errors before that could cause us to not set it.
                        lastDownloadTime = datetime.now(timezone.utc)
                    else:
                        # Now it could be that at least one SU has been waiting a long time. If that is true, then 
                        # do a download.
                        if lastDownloadTime is None:
                            lastDownloadTime = datetime.now(timezone.utc)
                            
                        if currentTime - lastDownloadTime > scpTimeOut:
                            doDownload = True
                else:
                    doDownload = False
                
                if doDownload:
                    self.log.writeInfo([ 'ScpWorker ' + str(self.id) + ' - Time for a download! Collected ' + str(len(susToDownload)) + ' for download.' ])
                    
                    # Now acquire SU lock for all SUs so no other thread modifies the SU while we are processing the download.
                    for su in susToDownload:
                        try:
                            gotSULock = su.acquireLock()
                            if not gotSULock:
                                raise Exception('lock', 'Unable to acquire SU ' + str(su.sunum) + ' lock.')
                            self.sunums.append(su.sunum)
                            
                            serverPath = su.worker.path
                            if not serverPath:
                                raise Exception('scpSU', 'Server SU path is not known.')
                            paths.append(serverPath)
                        
                            # Set status to D to prevent another ScpWorker from processing the download. Must do this
                            # before the SU Table lock is released, otherwise another ScpWorker could grab the same
                            # SU that this ScpWorker just grabbed.
                            self.log.writeInfo([ 'ScpWorker setting SU ' + str(su.sunum) + ' status to D.' ])
                            su.setStatus('D', None)
                        finally:
                            if gotSULock:
                                su.releaseLock()
                                
                    if len(self.sunums) == 0:
                        doDownload = False
            finally:
                # Always release lock.
                if gotTableLock:
                    self.suTable.releaseLock()

            if doDownload:
                try:
                    self.log.writeInfo([ 'ScpWorker ' + str(self.id) + ' downloading SUs ' + ','.join([ str(asunum) for asunum in self.sunums ]) + '.' ])                        

                    # Don't forget to make the temporary directory first.
                    if not os.path.exists(self.tmpdir):
                        self.log.writeInfo([ 'Creating temporary download directory ' + self.tmpdir + '.' ])
                        os.mkdir(self.tmpdir)

                    lastDownloadTime = datetime.now(timezone.utc)
                    
                    cmd = 'scp -r -P ' + port + ' ' + user + '@' + host + ':"' + ' '.join(paths) + '" ' + self.tmpdir
                    self.log.writeInfo([ 'ScpWorker ' + str(self.id) + ' running ' + cmd + '.' ])
                    try:
                        # check_call(cmdList)
                        # The scp process will inherit stdin, stdout, and stderr from this script.
                        # proc = Popen(cmdList, stdout=PIPE, stderr=PIPE)
                        proc = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE, start_new_session=True)
                    except OSError as exc:
                        import traceback
                        self.log.writeError([ traceback.format_exc(5) ])
                        raise Exception('scpSU', "Cannot run command '" + cmd + "' ")
                    except ValueError as exc:
                        import traceback
                        self.log.writeError([ traceback.format_exc(5) ])
                        raise Exception('scpSU', "scp command '" + cmd + "' called with invalid arguments.")

                    # Poll for completion
                    while True:
                        # The Python documentation is confusing at best. I think we have to look at the proc.returncode attribute
                        # to determine if the child process has completed. None means it hasn't. If the value is not None, then 
                        # the child process has terminated, and the value is the child process's return code.
                        lastDownloadTime = datetime.now(timezone.utc)
                        
                        proc.poll()
                        if proc.returncode is not None:
                            self.log.writeInfo([ 'ScpWorker ' + str(self.id) + ' - scp process exited with return code ' + str(proc.returncode) + '.' ])
                            lastDownloadTime = datetime.now(timezone.utc)
                            if proc.returncode != 0:
                                out, err = proc.communicate()
                                msg = 'Command "' + cmd + '" returned non-zero status code ' + str(proc.returncode) + '.'
                                if err is not None:
                                    self.log.writeError([ 'scp stderr msg: ' + err.decode('UTF8') ])
                                raise Exception('scpSU', msg)
                            break
                                  
                        try:
                            # If getAndLockSUs() raises, then sus is None.
                            atLeastOneGoodSU = False
                            # We actually do not need to lock the SUs here - but it doesn't hurt.
                            sus = self.suTable.getAndLockSUs(self.sunums)
                            for su in sus:
                                if su.status == 'D':                                
                                    # Check for SU download time-out. We keep doing the download, unless all SUs have timed out.
                                    timeNow = datetime.now(su.starttime.tzinfo)
                                    if timeNow > su.starttime + self.suTable.getTimeout():
                                        self.log.writeInfo([ 'Download of SUNUM ' + str(su.sunum) + ' timed-out.' ])
                                        self.log.writeInfo([ 'ScpWorker setting SU ' + str(su.sunum) + ' status to E (for time-out).' ])
                                        su.setStatus('E', 'Download timed-out.')
                                    else:
                                        atLeastOneGoodSU = True

                            if self.sdEvent.isSet():
                                # Kill the download and also exit the ScpWorker thread.
                                self.log.writeDebug([ 'ScpWorker ' + str(self.id) + ' received sdEvent. Killing scp.'])
                                proc.kill()
                                try:
                                    proc.communicate(timeout=2)
                                    self.log.writeInfo([ 'Successfully killed ScpWorker ' + str(self.id) + ' download.' ])
                                except TimeoutExpired:
                                    self.log.writeWarn([ 'Unable to kill scp for ScpWorker ' + str(self.id) + '.' ])
                                    
                                raise Exception('shutDown', 'ScpWorker ' + str(self.id) + ' is observing the global shutdown and exiting now.')
                            
                            if not atLeastOneGoodSU:
                                # Go on to the next set of requested SUs. Don't exit the ScpWorker thread.
                                proc.kill()
                                raise Exception('noSusForDl', 'The downloads of all SUs in the payload have been either canceled or have timed-out.')
                        finally:
                            # Always release SU locks. 
                            if sus:
                                for su in sus:
                                    su.releaseLock()

                        time.sleep(1) # In scp process poll loop.

                    try:
                        # Must lock SUs here. The Downloader could have set status to S, so if we don't hold a lock, we could
                        # overwrite the status to P.
                        sus = self.suTable.getAndLockSUs(self.sunums)
                        for su in sus:
                            if su.status == 'D':
                                # The download for this SU has not been canceled.
                                self.log.writeInfo([ 'ScpWorker setting SU ' + str(su.sunum) + ' status to P.' ])
                                su.setStatus('P', None)
                    finally:
                        # Always release lock.
                        if sus:
                            for su in sus:
                                su.releaseLock()

                    # Flush the change to disk.
                    self.log.writeDebug([ 'Updating SU table.' ])
                    self.suTable.updateDbAndCommit()

                    self.log.writeInfo([ 'ScpWorker ' + str(self.id) + ' - scp command succeeded for SUs ' + ','.join([ str(asunum) for asunum in self.sunums ]) + '.' ])
                
                except Exception as exc:
                    if len(exc.args) == 2:
                        type = exc.args[0]
                        msg = exc.args[1]
                        
                        # These errors should not cause the ScpWorker thread to exit. Set status to 'E'.
                        if type == 'scpSU':
                            try:
                                # Must lock SUs here. The Downloader could have set status to S, so if we don't hold a lock, we could
                                # overwrite the status to E.
                                sus = self.suTable.getAndLockSUs(self.sunums)
                                for su in sus:
                                    self.log.writeInfo([ 'ScpWorker setting SU ' + str(su.sunum) + ' status to E.' ])
                                    su.setStatus('E', msg)
                            finally:
                                # Always release lock.
                                if sus:
                                    for su in sus:
                                        su.releaseLock()

                            self.log.writeError([ msg ])
                        elif type == 'noSusForDl':
                            # The SU statuses have already been updated with a non-P status.
                            self.log.writeInfo([ msg ])
                        elif type == 'shutDown':
                            self.log.writeInfo([ msg ])
                            try:
                                # Must lock SUs here.
                                sus = self.suTable.getAndLockSUs(self.sunums)
                                for su in sus:
                                    if su.status == 'D':
                                        # The ScpWorker was in the middle of downloading the SU when rsumds.py was shut-down.
                                        # Set the status back to 'W' - we basically want to pretend that the download
                                        # never started.                                
                                        self.log.writeInfo([ 'Shutting down - ScpWorker ' + str(self.id) +' setting SU ' + str(su.sunum) + ' status to W. Daemon will initiate SU download upon restart.' ])
                                        su.setStatus('W', None)
                                    else:
                                        # If the status was P, then the download for this SU already succeeded. Or the Downloader
                                        # saw the shut-down signal, and set that status back to P. If it was W, then 
                                        # let the Downloader handle cleaning up - it will set the status back to P so the download
                                        # will be attempted again during the next run of rsumsd.py.
                                        pass
                            finally:
                                # Always release lock.
                                if sus:
                                    for su in sus:
                                        su.releaseLock()

                            self.log.writeInfo([ msg ])
                        else:
                            raise
                    else:
                        raise

                # Move on to the next scp without sleeping. Wake up a Downloader.
                ScpWorker.scpCompleted.set()
                # Clear event so that main will block the next time it calls wait.
                ScpWorker.scpCompleted.clear()
            else:
                # There were fewer than scpBatchSize requests for an SU download. Wait for more with ScpWorker.scpNeeded.wait().
                # Set a timeout so we can gracefully exit if the shutdown event has been triggered (just in case this thread
                # blocks on wait - when the shutdown happens, the scpNeeded event will be triggered, however).
                
                # It could be the case that the last Downloader has fired scpNeeded, but there still aren't enough SUs
                # to trigger a download. If that is the case, then we must let scpNeeded time-out before we do check to see
                # if it is time to do a download. If the scpNeeded.wait() timeout is long, then we won't check for the 
                # ScpWorker timeout for a long time, even though the ScpWorker timeout might be short.
                timeOutToUse = min(scpTimeOut, timedelta(seconds=10))
                
                try:
                    ScpWorker.scpNeeded.wait(timeOutToUse.total_seconds())
                except RuntimeError:
                    pass
        
        # This thread is about to terminate. 
        # We need to check the class tList variable to update it, so we need to acquire the lock.
        try:
            ScpWorker.lock.acquire()
            ScpWorker.tList.remove(self) # This thread is no longer one of the running threads.
        finally:
            ScpWorker.lock.release()        
        
    # Called from main thread
    def stop(self):
        self.log.writeInfo([ 'Stopping ScpWorker ' + str(self.id) + ' - (it may take 10 seconds for the ScpWorker to stop).' ])
        
        # Fire event to stop thread.
        self.sdEvent.set()
        self.log.writeDebug([ 'Set sdEvent in ScpWorker ' + str(self.id) + '.' ])
        
        # Fire scpNeeded event so that the ScpWorker thread will not block on wait.
        ScpWorker.scpNeeded.set()
    
    @staticmethod
    def newThread(id, suTable, arguments, log):
        worker = ScpWorker(id, suTable, arguments, log)
        ScpWorker.tList.append(worker)
        worker.start()

# Downloads a single SU. Ingests it into SUMs (SUMS allows to ingestion of a single SU at a time only.). Updates
# the SU table status for that SU.
# The main thread sets the SU status to 'P'. The Downloader thread sets that status to 'W' to request a ScpWorker
# thread to download the SU. The ScpWorker thread sets the status to 'D' while it is processing the download. When
# the download is complete, the ScpWorker thread sets the status back to 'P'.
class Downloader(threading.Thread):
    sumsConn = None
    sumsDbLock = None # Since all Downloader threads share the same connection, their cursors on these connections
                      # are not isolated. Use a lock to ensure that only one Download is modifying SUMS at one time.

    tList = [] # A list of running thread IDs.
    maxThreads = 16 # Default. Can be overriden with the Downloader.setMaxThreads() method.
    eventMaxThreads = threading.Event() # Event fired when the number of threads decreases.
    lock = threading.Lock() # Guard tList.

    def __init__(self, sunum, path, series, suSize, retention, sus, scpUser, scpHost, scpPort, binPath, arguments, log):
        threading.Thread.__init__(self)
        self.sunum = sunum
        self.path = path
        self.series = series
        self.suSize = suSize
        self.retention = retention
        self.suTable = sus
        self.scpUser = scpUser
        self.scpHost = scpHost
        self.scpPort = scpPort
        self.binPath = binPath
        self.tmpdir = arguments.tmpdir
        self.arguments = arguments
        self.log = log
        self.sdEvent = threading.Event()

    def run(self):
        # Sub-out the download to an ScpWorker instance. To do that, se the SU status to 'W'. The ScpWorker
        # that is used will set the status to 'D' so that no other ScpWorker attempt to download the SU 
        # as well. When the ScpWorker completes the download, it will set the status to 'P' again.
        self.log.writeInfo([ 'Downloader running for SU ' + str(self.sunum) + '.' ])
        
        suDlPath = None
        sudir = None

        try:
            su = None
            try:
                self.log.writeDebug([ 'Downloader acquiring lock for SU ' + str(self.sunum) + '.' ])
                su = self.suTable.getAndLockSU(self.sunum)

                if su.status != 'P' and su.status != 'W':
                    raise Exception('downloader', 'SU ' + str(su.sunum) + ' not pending.')
                
                if su.status != 'W':
                    # Let an ScpWorker thread handle the download. We do that by setting the status to 'W'.
                    # Upon recovery from a daemon shutdown, we may start certain SUs in the W state, in which
                    # case we do not need to set the status to W.
                    self.log.writeInfo([ 'Setting SU ' + str(self.sunum) + " status to W." ])
                
                    # The ScpWorker learns of the source path, the SU size, the scp user, etc., from su.worker.
                    su.setStatus('W', None)
                
                suDlPath = os.path.join(self.tmpdir, 'D' + str(su.sunum))
            finally:
                # Always release lock.
                if su:
                    su.releaseLock()

            # Wake up an ScpWorker.
            ScpWorker.scpNeeded.set()
            # Clear event so that main will block the next time it calls wait.
            ScpWorker.scpNeeded.clear()

            # Wait for the ScpWorker thread to finish downloading the SU (look for a 'P' status).
            while not self.sdEvent.isSet():
                su = None
                try:
                    su = self.suTable.getAndLockSU(self.sunum)

                    # Check for download error or completion. Call get() again, since the SU record could have been deleted (due to
                    # abnormal execution).
                    if su.status == 'W' or su.status == 'D':
                        # ScpDownloader is still performing the download. Don't do anything
                        pass
                    elif su.status == 'P':
                        # ScpDownloader is done performing the download.
                        break
                    else:
                        raise Exception('downloader', 'The download of SU ' + str(su.sunum) + ' errored-out.')
                finally:
                    # Always release lock.
                    if su:
                        su.releaseLock()

                # Wait for ANY ScpWorker to complete a download. The downloaded SU might not be the one needed by
                # this Downloader thread. If a shutdown is happening, then all ScpWorker threads should fire the 
                # scpCompleted event, releasing all blocking Downloader threads. But just in case, set a 10-second 
                # timeout - this raises if the timeout occurs. It is here that we will catch other errors too, like
                # a download time-out for this SU.
                try:
                    ScpWorker.scpCompleted.wait(10)
                except RuntimeError:
                    pass
            
            # The download has completed (it is in self.tmpdir), or a shut-down is being observed.
            
            # If a shut-down is being observed, then the SU status must be W or P or E. If there is no shut-down happening, 
            # then the status must be P or E.
            if self.sdEvent.isSet():
                # If status == 'P', then go ahead and save the SU in SUMS. Below, set status to C. This SU was successfully downloaded.
                # If status == 'E', the download failed, and if status == 'W', then we pretend the download never started (although
                # it could have started, but got canceled by the shutdown).
                su = None
                try:
                    su = self.suTable.getAndLockSU(self.sunum)
                    if su.status != 'P':            
                        raise Exception('shutDown', 'Downloader thread for SU ' + str(self.sunum) + ' is observing the global shutdown and exiting now.')
                finally:
                    # Always release lock.
                    if su:
                        su.releaseLock()

            # At this point, we need to allocate (mkdir) a new SU directory, move the downloaded SU content into this SUDIR, 
            # then commit the newly created SU into SUMS. The previous incarnations of remote-sums-type code all used the SUMS
            # API to achieve the first and last steps. To use the SUMS API, code need to be written in C and it needs to link
            # to the SUMS library. The first remote-sums code did this by running a DRMS module, wherein all three steps were
            # performed. The JMD uses vso_sum_alloc (a DRMS module with access to the SUMS API) to allocate the SU directory, 
            # then it copies the downloaded SU content into the directory, and then it calls vso_sum_put to commit the SU.
            # However, at a high rate of download, the SUMS API seems to have problems, resulting in the vso_sum_alloc and/or
            # vso_sum_put calls to hang for minutes. We have not been able to track down this issue, but it appears to have
            # something to do with either saturation of socket resources and/or RPC resources and/or SUMS queues.
            
            # This script by-passes SUMS altogether to avoid the issues with SUMS and/or DRMS modules hanging under higher load.
            # The first step in by-passing SUMS is to perform the equivalent of the SUM_open() API call. Then we can call the
            # equivalent of the SUM_alloc2() call to allocate a new SU directory, followed by the copying of the download SU
            # content into this new SU directory. Then we can call the equivalent of the SUM_put() API call to commit the 
            # SU, and then we can call the equivalent of the SUM_close() API call to end the SUMS session.
            
            # We need to connect to the SUMS database before we can modify SUMS objects.
            # The DB transaction is NOT in autocommit mode.
            # We might need to put all of this in a lock to keep other Downloader objects from modifying the same DB tables
            # at the same time (cursors held by different Downloaders are not isolated).
            try:
                gotSumsDbLock = Downloader.sumsDbLock.acquire()
                if gotSumsDbLock:
                    with Downloader.sumsConn.cursor() as cursor:
                        needsCommit = False
                        try: 
                            # Put all of this in one transaction. If everything is good, commit the transaction. If an 
                            # exception occurs, roll back.
                    
                            ##### SUM_open() port #####
                            # This increments the sequence that supplies the sumid and inserts that sumid into the sum_open table.
                            cmd = "SELECT NEXTVAL('public.sum_seq')"
                            cursor.execute(cmd)
                            records = cursor.fetchall()
                            if len(records) != 1:
                                raise Exception('sumsAPI', 'Unexpected response when fetching sumid from sequence.')
                        
                            sumid = records[0][0]

                            currentTimeStr = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            cmd = 'INSERT INTO public.sum_open(sumid, open_date) VALUES (' + str(sumid) + ", '" + currentTimeStr + "')"
                            cursor.execute(cmd)
                            ##### SUM_open() port - end #####
                    
                            self.log.writeInfo([ 'Successfully called SUM_open() port for SU ' +  str(self.sunum) + '.'])
                            self.log.writeInfo([ 'sumid is ' + str(sumid) + '.' ])
            
                            ##### SUM_alloc2() port #####
                            #   First, find a partition that has enough available space for the size of the SU to be downloaded.
                            cmd = 'SELECT PARTN_NAME FROM public.sum_partn_avail WHERE AVAIL_BYTES >= 1024 AND PDS_SET_NUM = 0'
                            cursor.execute(cmd)
                            records = cursor.fetchall()
                            if len(records) < 1:
                                raise Exception('sumsAPI', 'Cannot allocate a new Storage Unit in SUMS - out of space.')
                        
                            partitions = []
                            for rec in records:
                                partitions.append(rec[0])
                        
                            #   Second, randomly choose one of the partitions to put the new SU into. We want to spread the write load over available 
                            #   partitions.
                            randIndex = random.randint(0, len(partitions) - 1)
                            partition = partitions[randIndex]
                            sudir = os.path.join(partition, 'D' + str(self.sunum))
                            os.mkdir(sudir)
                            os.chmod(sudir, 0O2755)
        
                            #   Third, insert a record into the sum_partn_alloc table for this SU. status is DARW, which is 1. effective_date is "0".
                            cmd = "INSERT INTO public.sum_partn_alloc(wd, sumid, status, bytes, effective_date, archive_substatus, group_id, safe_id, ds_index) VALUES ('" + sudir + "', '" + str(sumid) + "', 1, 1024, '0', 0, 0, 0, 0)"
                            cursor.execute(cmd)
                            ##### SUM_alloc2() port - end #####

                            self.log.writeInfo([ 'Succeeded allocating a new SU: ' +  str(self.sunum) + '.' ])
                    
                            #    Fourth, move the downloaded SU files into the chosen SUMS partition. SUM_alloc2() code calls mkdir, so we cannot
                            #    move the top-level D___ directory into the SUMS partition. Instead we have to move, recursively,  all files and directories in 
                            #    the downloaded D___ directory into the SUMS D___ directory.
                            files = os.listdir(suDlPath)
                            self.log.writeInfo([ 'Moving downloaded SU content from ' + suDlPath + ' into allocated SU (' + sudir  + ').' ])

                            try:
                                for afile in files:
                                    src = os.path.join(suDlPath, afile)
                                    shutil.move(src, sudir)
                            except shutil.Error as exc: 
                                import traceback
                                self.log.writeError([ traceback.format_exc(5) ])
                                raise Exception('mvSU', 'Unable to move SU file ' + afile + ' into SUdir ' + sudir + '.')

                            self.log.writeInfo([ 'Move of SU ' + str(self.sunum) + ' content succeeded.' ])
                    
                            ##### SUM_put() port #####
                            # The original SUM_put() call called "chmown" to change the ownership of the
                            # files in the SU dir to the SUM_MANAGER. However, this is not necessary since rsumsd.py is run by the 
                            # SUM_MANAGER.
                    
                            #   First, chmod all directories to 0755. All regular files get their user/group/other read enabled, and their
                            #   user write enabled, and their group and other write disabled.
                            for root, dirs, files in os.walk(sudir):
                                for adir in dirs:
                                    fullPath = os.path.join(root, adir)
                                    os.chmod(fullPath, 0O0755)
                                for afile in files:
                                    fullPath = os.path.join(root, afile)
                                    st = os.stat(fullPath)
                                    newMod = st.st_mode | stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH | stat.S_IWUSR & ~stat.S_IWGRP & ~stat.S_IWOTH
                                    os.chmod(fullPath, newMod)
                            
                            #   Second, update SUMS sum_main database table - Calculate SU dir number of bytes, set online status to 'Y', set archstatus to 'N', 
                            #   set offsiteack to 'N', set dsname to seriesname, set storagegroup to tapegroup, set storageset to tapegroup / 10000,
                            #   set username to getenv('USER') or nouser if no USER env, set mode to TEMP + TOUCH, set apstatus: if SUMS_TAPE_AVAILABLE ==>
                            #   DAAP (4), else DADP (2), set archsub ==> DAAEDDP (32), set effective_date to tdays in the future (with format "%04d%02d%02d%02d%02d").
                            #   Insert all of this into sum_main.
                            numBytes = os.path.getsize(sudir) + sum([ os.path.getsize(fullPath) for fullPath in [ os.path.join(root, afile) for root, dirs, files in os.walk(sudir) for afile in files ] ]) + sum([ os.path.getsize(fullPath) for fullPath in [ os.path.join(root, adir) for root, dirs, files in os.walk(sudir) for adir in dirs ] ])
                            if self.arguments.tapesysexists:
                                apStatus = 4 # DAAP
                            else:
                                apStatus = 2 # DADP
        
                            createDate = datetime.now()
                            createDateStr = createDate.strftime('%Y-%m-%d %H:%M:%S')
                            expDate = createDate + timedelta(days=self.retention)
                            effDate = expDate.strftime('%Y%m%d%H%M')

                            # storage_group is the tape group. It should come from the series definition, but remote sites have been using 0 for years.            
                            cmd = "INSERT INTO public.sum_main(online_loc, online_status, archive_status, offsite_ack, history_comment, owning_series, storage_group, storage_set, bytes, ds_index, create_sumid, creat_date, access_date, username) VALUES ('" + sudir + "', 'Y', 'N', 'N', '', '" + self.series + "', 0, 0, " + str(numBytes) + ', ' + str(self.sunum) + ', ' + str(sumid) + ", '" + createDateStr + "', '" + createDateStr + "', '" + os.getenv('USER', 'nouser') + "')"
                            cursor.execute(cmd)
                    
                            self.log.writeInfo([ 'Successfully inserted record into sum_main for SU ' + str(self.sunum) + '.' ])
        
                            #    Third, update SUMS sum_partn_alloc table - Insert a new row into sum_partn_alloc for this SU. The SUM_alloc2() port will result in
                            #    a row in sum_partn_alloc with a ds_index of 0, which does not make sense to me. But the SUM_close() port will delete
                            #    that row. By the time this thread terminates, there will be only a single row for this SU in sum_partn_alloc. substatus is DAAEDDP (32).
                            #    But first, delete any existing DADP (delete pending) rows for this sunum if the status of the SU for the new row is DADP.
                            if apStatus == 2:
                                # We do this simply to ensure that we do not have two sum_partn_alloc records with status DADP (delete pending).
                                cmd = 'DELETE FROM public.sum_partn_alloc WHERE ds_index = ' + str(self.sunum) + ' AND STATUS = 2'
                                cursor.execute(cmd)
                                self.log.writeInfo([ 'Successfully deleted old DADP record from sum_partn_alloc for SU ' + str(self.sunum) + '.' ])
                    
                            cmd = "INSERT INTO public.sum_partn_alloc(wd, sumid, status, bytes, effective_date, archive_substatus, group_id, safe_id, ds_index) VALUES ('" + sudir + "', " + str(sumid) + ', ' + str(apStatus) + ', ' + str(numBytes) + ", '" + effDate + "', 32, 0, 0, " + str(self.sunum) + ')'
                            cursor.execute(cmd)
                            self.log.writeInfo([ 'Successfully inserted record into sum_partn_alloc for SU ' + str(self.sunum) + '.' ])
                            ##### SUM_put() port - end #####
                    
                            self.log.writeInfo([ 'Commit of SU ' + str(self.sunum) + ' succeeded.' ])
                    
                            ##### SUM_close() port #####
                            # Delete sum_partn_alloc records for read-only partitions (status == 8) and read-write partitions (status == 1).
                            cmd = 'DELETE FROM public.sum_partn_alloc WHERE sumid = ' + str(sumid) + ' AND (status = 8 OR status = 1)'
                            cursor.execute(cmd)
                            self.log.writeInfo([ 'Successfully deleted read-only and read-write records from sum_partn_alloc for SU ' + str(self.sunum) + '.' ])
                    
                            # Delete the temporary ds_index = 0 records created during the SUM_put() port. I still do not know why this record
                            # was created in the first place.
                            cmd = 'DELETE FROM public.sum_open WHERE sumid = ' + str(sumid)
                            cursor.execute(cmd)
                            self.log.writeInfo([ 'Successfully deleted temporary (ds_index == 0) records from sum_open for SU ' + str(self.sunum) + '.' ])
                            ##### SUM_close() port - end #####
                            
                            needsCommit = True
                        except psycopg2.Error as exc:
                            # Handle database-command errors. These are all due to problems communicating with the SUMS db.
                    
                            # Clean-up
                            if os.path.exists(sudir):
                                shutil.rmtree(sudir)
                            if os.path.exists(suDlPath):
                                shutil.rmtree(suDlPath)
                            raise Exception('sumsAPI', exc.diag.message_primary + ': ' + cmd + '.') 
                        except Exception as exc:
                            # Clean-up
                            if os.path.exists(sudir):
                                shutil.rmtree(sudir)
                            if os.path.exists(suDlPath):
                                shutil.rmtree(suDlPath)
                            raise
                        finally:
                            if needsCommit:
                                Downloader.sumsConn.commit()
                            else:
                                Downloader.sumsConn.rollback()
            finally:
                if gotSumsDbLock:
                    Downloader.sumsDbLock.release()

            # Remove temporary directory.
            self.log.writeInfo([ 'Removing empty temporary download directory ' + suDlPath + '.' ])
            try:
                if os.path.exists(suDlPath):
                    shutil.rmtree(suDlPath)
            except OSError as exc:
                raise Exception('rmTmpSU', exc.strerror)
           
            self.log.writeInfo([ 'Removal of temporary directory ' + suDlPath + ' succeeded.' ])
 
            # Update SU table. Set SU-table record status to 'C'. Must first lock the SU table since we are modifying it. Also,
            # the state may not be 'P' due to some problem cropping up in the meantime. Only set to 'C' if the state is 'P'.
            try:
                su = None
                su = self.suTable.getAndLockSU(self.sunum)
                    
                if su.status == 'P':
                    self.log.writeInfo([ 'Setting SU ' + str(self.sunum) + ' status to complete.' ])
                    su.setStatus('C', None)
            finally:
                # Always release lock.
                if su:
                    su.releaseLock()
 
        except Exception as exc:
            if len(exc.args) == 2:
                type = exc.args[0]
                msg = exc.args[1]

                if type == 'shutDown':
                    # If the status is W, then either the download never happened, or it got canceled part-way through and
                    # the ScpWorker thread set its status back to W. The daemon will resume upon restart, starting
                    # Downloader threads for each SU that has status W.
                    pass
                else:
                    if type == 'scpSU' or type == 'sumsAPI' or type == 'mvSU' or type == 'rmTmpSU':
                        msg = 'Error downloading storage unit ' + str(self.sunum) + ': ' + msg
                    elif type == 'unknownSunum':
                        msg = 'Cannot download SU ' + str(self.sunum) + '. No SU record. ' + msg
                    elif type == 'downloader':
                        msg = 'Downloader thread error for SU ' + str(self.sunum) + ': ' + msg
                    else:
                        import traceback
                        msg = traceback.format_exc(5)
                    
                    # Set SU status to E.
                    su = None
                    try:
                        su = self.suTable.getAndLockSU(self.sunum)
                        self.log.writeError([ 'Setting SU ' + str(self.sunum) + ' status to error.' ])
                        self.log.writeError([ msg ])
                        su.setStatus('E', msg)
                    finally:
                        # Always release lock.
                        if su:
                            su.releaseLock()
            else:
                import traceback
                msg = traceback.format_exc(5)

                # Set SU status to E.
                su = None
                try:
                    su = self.suTable.getAndLockSU(self.sunum)
                    self.log.writeError([ 'Setting SU ' + str(self.sunum) + ' status to error.' ])
                    self.log.writeError([ msg ])
                    su.setStatus('E', msg)
                finally:
                    # Always release lock.
                    if su:
                        su.releaseLock()
                
            # Must clean-up SU dir and the downloaded files.
            try:
                if sudir and os.path.exists(sudir):
                    shutil.rmtree(sudir)
                if suDlPath and os.path.exists(suDlPath):
                    shutil.rmtree(suDlPath)
            except OSError as exc:
                pass
                
        # Update SU table (write-out status, error or success, to the DB).
        # This is a no-op if no SUs were actually modified.
        self.suTable.updateDbAndCommit()

        # This thread is about to terminate. 
        # We need to check the class tList variable to update it, so we need to acquire the lock.
        try:
            Downloader.lock.acquire()
            Downloader.tList.remove(self) # This thread is no longer one of the running threads.
            # Use <= because we don't know if we were able to reach Downloader.maxThreads thread running due 
            # to system-resource limitations.
            if len(Downloader.tList) <= Downloader.maxThreads - 1:
                # Fire event so that main thread can add new SUs to the download queue.
                self.log.writeDebug([ 'OK to start new download threads.' ])
                Downloader.eventMaxThreads.set()
                # Clear event so that main will block the next time it calls wait.
                Downloader.eventMaxThreads.clear()
                
            self.log.writeInfo([ 'Downloader for SU ' + str(self.sunum) + ' exiting.' ])
        finally:
            Downloader.lock.release()

    def stop(self):
        self.log.writeInfo([ 'Stopping Downloader (SUNUM ' + str(self.sunum) + '). It may take 10 seconds for Downloader to stop.' ])
        
        # Fire event to stop thread.
        self.sdEvent.set()
        
        # Fire scpCompleted event so that the Downloader thread will not block on wait.
        ScpWorker.scpCompleted.set()

    # Must acquire Downloader lock BEFORE calling newThread() since newThread() will append to tList (the Downloader threads will delete from tList as they complete).
    @staticmethod
    def newThread(sunum, path, series, suSize, retention, sus, scpUser, scpHost, scpPort, binPath, arguments, log):
        dl = Downloader(sunum, path, series, suSize, retention, sus, scpUser, scpHost, scpPort, binPath, arguments, log)
        sus.setWorker(sunum, dl)
        dl.tList.append(dl)
        
        try:
            dl.start()
        except RuntimeError:
            # Cannot start a new thread, so rollback and re-raise so the calling thread can handle the error.
            Downloader.tList.remove(dl)
            su = sus.get([ sunum ])[0]
        
            try:
                gotSULock = su.acquireLock()
        
                if gotSULock is None:
                    raise Exception('lock', 'Unable to acquire SU ' + str(sunum) + ' lock.')
                del su.worker
                su.worker = None
            finally:
                if gotSULock:
                    su.releaseLock()

            raise Exception('startThread', 'Cannot start a new Downloader thread due to system resource limitations.')

    @classmethod
    def setMaxThreads(cls, maxThreads):
        cls.maxThreads = maxThreads

class ProviderPoller(threading.Thread):
    tList = [] # A list of running thread IDs.
    maxThreads = 32 
    eventMaxThreads = threading.Event() # Event fired when the number of threads decreases.
    lock = threading.Lock() # Guard tList.
    
    def __init__(self, url, requestID, sunums, sus, reqTable, request, dbUser, binPath, arguments, log):
        threading.Thread.__init__(self)
        self.url = url
        self.requestID = requestID # The provider request ID.
        self.sunums = sunums # The sunums requested from provider (under request ID self.requestID).
        self.suTable = sus
        self.reqTable = reqTable
        self.request = request # The ReqTable::reqdict[requestidStr] object (the row in the request table)
        self.dbUser = dbUser
        self.binPath = binPath
        self.arguments = arguments
        self.log = log
        self.startTime = datetime.now(timezone.utc) # Cool bug. This used to be self.start. But the parent object has a method named 'start' The effect was to override the method with an attribute.
        self.timeOut = sus.getTimeout()
        self.sdEvent = threading.Event()

    def run(self):
        values = {'requestid' : self.requestID, 'sunums' : 'none'}
        data = urllib.parse.urlencode(values)
        errMsg = None
        timeToLog = True
        loopN = 0

        # Set the in-memory ProviderPoller flag for all sunums in this request.
        for asunum in self.sunums:
            try:
                su = self.suTable.getAndLockSU(asunum)
                su.setPolling(True)
            except Exception as exc:
                if len(exc.args) != 2:
                    raise # Re-raise

                etype = exc.args[0]

                if etype != 'unknownSunum':
                    raise
            finally:
                if su:
                    su.releaseLock()

        dlInfo = {}
        dlInfo['status'] = 'pending'
        while not self.sdEvent.isSet():
            if datetime.now(self.startTime.tzinfo) > self.startTime + self.timeOut:
                # The providing site has not completed the export, and the time-out has elapsed. Give up.
                errMsg = 'Timed-out waiting for providing site to return paths to requested SUs (' + ','.join([ str(asunum) for asunum in self.sunums ]) + ') - provider request ' + self.requestID + '.'
                break

            if timeToLog:
                self.log.writeInfo([ 'Checking on request to provider (provider request ' + self.requestID + ').' ])
                self.log.writeInfo([ 'URL is ' + self.url + '/rs.sh' + '?' + data ])

            with urllib.request.urlopen(self.url + '/rs.sh' + '?' + data) as response:
                dlInfoStr = response.read().decode('UTF-8')

            dlInfo = json.loads(dlInfoStr)

            if timeToLog:
                self.log.writeInfo([ 'Provider returns status ' + dlInfo['status'] + '.' ])

            if dlInfo['status'] != 'pending':
                break;

            time.sleep(1)
            loopN += 1
            
            # Log every 5 seconds.
            timeToLog = (loopN % 5 == 0)

        # We might not have printed to log.
        if not errMsg and not timeToLog:
            self.log.writeInfo([ 'Checking on request to provider (provider request ' + self.requestID + ').' ])
            self.log.writeInfo([ 'URL is ' + self.url + '/rs.sh' + '?' + data ])
            self.log.writeInfo([ 'Provider returns status ' + dlInfo['status'] + '.' ])

        # We are done polling, remove the polling flag.
        for asunum in self.sunums:
            try:
                su = self.suTable.getAndLockSU(asunum)
                su.setPolling(False)
            except Exception as exc:
                if len(exc.args) != 2:
                    raise # Re-raise

                etype = exc.args[0]

                if etype != 'unknownSunum':
                    raise
            finally:
                if su:
                    su.releaseLock()

        if dlInfo['status'] != 'complete':
            # Set all SU records to 'E' (rsumds.py timed-out waiting for the SUs to be ready at the providing site).
            if not errMsg:
                errMsg = 'The providing site failed to return paths to requests SUs.'
            # SuTable::setStatus() will acquire and release all SU locks.
            self.suTable.setStatus(self.sunums, 'E', errMsg)
        else:
            # Start a download for each SU. If we cannot start the download for any reason, then set the SU status to 'E'.
            self.log.writeInfo([ 'The SU paths are ready.' ])
            paths = dlInfo['paths']

            retentions = {}
            for (asunum, path, series, suSize) in paths:
                try:
                    su = self.suTable.getAndLockSU(asunum)

                    if not path:
                            # A path of None means that the SUNUM was invalid. We want to set the SU status to 'E'.
                        su.setStatus('E', 'SU ' + str(asunum) + ' is not valid at the providing site.')
                        continue
                    elif path == '':
                        # An empty-string path means that the SUNUM was valid, but that the SU referred to was offline, and could not
                        # be placed back online - it is not archived.
                        # ART - I need to figure out how to place the SUNUM in SUMS so that its archive flag is N (not archived).
                        su.setStatus('C', 'SU ' + str(asunum) + ' refers to an offline SU valid at the providing site that was not archived. It cannot be downloaded.')
                        continue
                    
                    if suSize is None:
                        suSize = 0
                
                    if series in retentions:
                        retention = retentions[series]
                    else:
                        # request provides the host, port, and dbname to use with jsoc_info to fetch the retention value.
                        retention = ReqTable.getRetention(series, self.request, self.dbUser, self.log)
                        retentions[series] = retention
                    
                        # Save series and retention.
                        su.setSeries(series)
                        su.setRetention(retention)
                finally:
                    if su:
                        su.releaseLock()

                while not self.sdEvent.isSet():
                    Downloader.lock.acquire()
                    try:
                        if len(Downloader.tList) < Downloader.maxThreads:
                            self.log.writeInfo([ 'Instantiating a Downloader for SU ' + str(asunum) + '.' ])
                            Downloader.newThread(asunum, path, series, suSize, retention, self.suTable, dlInfo['scpUser'], dlInfo['scpHost'], dlInfo['scpPort'], self.binPath, self.arguments, self.log)
                            break # The finally clause will ensure the Downloader lock is released.
                    except Exception as exc:
                        if len(exc.args) != 2:
                            raise # Re-raise

                        etype = exc.args[0]

                        if etype != 'startThread':
                            raise
                    
                        # Ran out of system resources - could not start new thread. Just wait for a thread slot to become free.
                    finally:
                        Downloader.lock.release()

                    Downloader.eventMaxThreads.wait()
                    # We woke up, but we do not know if there are any open threads in the thread pool. Loop and check
                    # tList again.
            
        # Flush the change to disk.
        self.suTable.updateDbAndCommit()
        
        try:
            ProviderPoller.lock.acquire()
            ProviderPoller.tList.remove(self) # This thread is no longer one of the running threads.
            # Use <= because we don't know if we were able to reach Downloader.maxThreads thread running due 
            # to system-resource limitations.
            if len(ProviderPoller.tList) <= ProviderPoller.maxThreads - 1:
                # Fire event so that main thread can add new sunums to the ProviderPoller queue.
                ProviderPoller.eventMaxThreads.set()
                # Clear event so that main will block the next time it calls wait.
                ProviderPoller.eventMaxThreads.clear()
        finally:
            ProviderPoller.lock.release()

    def stop(self):
        self.log.writeInfo([ 'Stopping ProviderPoller (requestID ' + str(self.requestID) + ').' ])
        self.sdEvent.set()

    @staticmethod
    def newThread(url, requestID, sunums, sus, reqTable, request, dbUser, binPath, arguments, log):
        poller = ProviderPoller(url, requestID, sunums, sus, reqTable, request, dbUser, binPath, arguments, log)
        poller.tList.append(poller)
        try:
            poller.start()
        except RuntimeError:
            poller.tList.remove(poller)
            del poller
            raise Exception('startThread', 'Cannot start a new ProviderPoller thread due to system resource limitations.')

def readTables(sus, requests, sites):
    if sus:
        sus.tryRead()

    if requests:
        requests.tryRead()
    
    if sites:
        sites.tryRead()

# Process the SUs for the source site represented by url.
# url - the base URL to the rs.sh cgi (e.g., http://jsoc.stanford.edu/cgi-bin) from which SU paths can be obtained.
# sunums - a list of sorted SUNUMs to download.
# sus - the SU table object that represents the SU database table.
# request - ReqTable::dict[requestidStr] object.
# binPath - the local path to the binaries needed to ingest the downloaded SU into SUMS. This is mostly likely the path to
#           the DRMS binaries (one binary needed is vso_sum_alloc)
# log - the log to write messages to.
# reprocess - the SUs identified are all being reprocessed. They all have a status of 'P' in the SU table. There was
#             some interruption that caused the download to be lost.
# reset - reset the processing start time for each SU. Ignored, unless reprocess is true
def processSUs(url, sunums, sus, reqTable, request, dbUser, binPath, arguments, log, reprocess=False, reset=False):
    # Get path to SUs by calling the rs.sh cgi at the owning remote site (url identifies the remote site).
    # Create the sunum= argument.

    # Skip any SUs that have already been processed. Pending SUs are OK to restart, however. It may be the case
    # that rsumds.py was interrupted during a download, in which case, we want to re-download the SU.
    workingSunums = []
    for asunum in sunums:
        try:
            su = sus.get([ asunum ])[0]
            if su.status == 'P' or su.status == 'W':
                if not reprocess:
                    # Accidental attempt to reprocess an SU whose processing has already started.
                    raise Exception('accidentalRepro', 'An accidental attempt to reprocess pending SU ' + str(asunum) + ' occurred.')
                workingSunums.append(asunum)
                # Reset start time.
                if reset:
                    sus.setStarttime([ asunum ], datetime.now(timezone.utc))
        except Exception as exc:
            if len(exc.args) != 2:
                raise # Re-raise

            etype = exc.args[0]

            if etype != 'unknownSunum':
                raise

            workingSunums.append(asunum)
            # Create a new SU table record for this SU.
            log.writeInfo([ 'Inserting a new SU table record for ' + str(asunum) + '.' ])
            # Will set status to P.
            sus.insert([ asunum ])

    sunumLst = ','.join(str(asunum) for asunum in workingSunums)
    values = {'requestid' : 'none', 'sunums' : sunumLst}
    data = urllib.parse.urlencode(values)
    log.writeInfo([ 'Requesting paths for SUNUMs ' + sunumLst + '. URL is ' + url + '/rs.sh' + '?' + data + '.' ])
    
    with urllib.request.urlopen(url + '/rs.sh' + '?' + data) as response:    
        dlInfoStr = response.read().decode('UTF-8')

    dlInfo = json.loads(dlInfoStr)

    if dlInfo['status'] == 'complete':
        # All of the requested SUs are online at the providing site.
        paths = dlInfo['paths']

        # Start a download for each SU. If we cannot start the download for any reason, then set the SU status to 'E'.
        retentions = {}
        for (asunum, path, series, suSize) in paths:
            if not path:
                # A path of None means that the SUNUM was invalid. We want to set the SU status to 'E'.
                sus.setStatus([ asunum ], 'E', 'SU ' + str(asunum) + ' is not valid at the providing site.')
                continue
            elif path == '':
                # An empty-string path means that the SUNUM was valid, but that the SU referred to was offline, and could not
                # be placed back online - it is not archived. 
                # ART - I need to figure out how to place the SUNUM in SUMS so that its archive flag is N (not archived).
                sus.setStatus([ asunum ], 'C', 'SU ' + str(asunum) + ' refers to an offline SU valid at the providing site that was not archived. It cannot be downloaded.')
                continue
                
            if suSize is None:
                suSize = 0

            # Get retention, if it hasn't been gotten yet. If we are re-processing SUs, then the retention has already been determined and saved.
            if reprocess:
                # We know this won't raise, because we already obtained the su object at the beginning of this method.
                su = sus.get([ asunum ])[0]
                retention = su.retention
            else:
                if series in retentions:
                    retention = retentions[series]
                else:
                    # request provides the host, port, and dbname to use with jsoc_info to fetch the retention value.
                    retention = ReqTable.getRetention(series, request, dbUser, log)
                    retentions[series] = retention
                    
                # Save series and retention.
                sus.setSeries([ asunum ], series)
                sus.setRetention([ asunum ], retention)

            while True:
                Downloader.lock.acquire()
                try:
                    if len(Downloader.tList) < Downloader.maxThreads:
                        log.writeInfo([ 'Instantiating a Downloader for SU ' + str(asunum) + '.' ])
                        Downloader.newThread(asunum, path, series, suSize, retention, sus, dlInfo['scpUser'], dlInfo['scpHost'], dlInfo['scpPort'], binPath, arguments, log)
                        break # The finally clause will ensure the Downloader lock is released.
                except Exception as exc:
                    if len(exc.args) != 2:
                        raise # Re-raise

                    etype = exc.args[0]

                    if etype != 'startThread':
                        raise
                    
                    log.writeError([ 'Cannot start Downloader thread. Out of system resources. Will try again when existing Downloaders complete.' ])
                    # Ran out of system resources - could not start new thread. Just wait for a thread slot to become free.
                finally:
                    Downloader.lock.release()

                log.writeDebug([ 'Main thread waiting for thread slot for SU ' + str(asunum) + '.' ])
                Downloader.eventMaxThreads.wait()
                # We woke up, but we do not know if there are any open threads in the thread pool. Loop and check
                # tList again.

        # For each SU that was requested, but for which no path was given in the response, update its SU-table record with an error status.
        pathInResp = dict([ (str(asunum), True) for (asunum, path, series, suSize) in paths ])
        for asunum in workingSunums:
            if str(asunum) not in pathInResp:
                sus.setStatus([asunum], 'E', 'Providing site cannot provide a path for SU ' + str(asunum) + '.')
    elif dlInfo['status'] == 'pending':
        # One or more of the requested SUs is offline. Poll until they are ready. Ideally we wouldn't block the main
        # thread here, waiting for the SUs to be available. We could spawn a thread to poll, freeing up the main
        # thread to continue with other requests. But in the interest of time, just poll for now. Must acquire
        # su-table lock when it is finally time to start downloads.
        log.writeInfo([ 'Request includes one or more SUs that are offline at the providing site. Waiting for providing site to put them online.' ])

        while True:
            ProviderPoller.lock.acquire()
            try:
                if len(ProviderPoller.tList) < ProviderPoller.maxThreads:
                    log.writeInfo([ 'Instantiating a PollerProvider for sunums ' + ','.join(workingSunums) + '.' ])
                    ProviderPoller.newThread(url, dlInfo['requestid'], workingSunums, sus, reqTable, request, dbUser, binPath, arguments, log)
                    break
            except Exception as exc:
                if len(exc.args) != 2:
                    raise # Re-raise

                etype = exc.args[0]

                if etype != 'startThread':
                    raise
                    
                log.writeError([ 'Cannot start ProviderPoller thread. Out of system resources. Will try again when existing ProviderPollers complete.' ])
            finally:
                ProviderPoller.lock.release()

            ProviderPoller.eventMaxThreads.wait()
    else:
        # Error of some kind.
        # Update the SU-table status of the SUs to 'E'.
        sus.setStatus(workingSunums, 'E', 'Unable to obtain paths from providing site.\n' + dlInfo['statusMsg'] + '.')

class LogLevelAction(argparse.Action):
    def __call__(self, parser, namespace, value, option_string=None):
        valueLower = value.lower()
        if valueLower == 'critical':
            level = logging.CRITICAL
        elif valueLower == 'error':
            level = logging.ERROR
        elif valueLower == 'warning':
            level = logging.WARNING
        elif valueLower == 'info':
            level = logging.INFO
        elif valueLower == 'debug':
            level = logging.DEBUG
        else:
            level = logging.ERROR

        setattr(namespace, self.dest, level)
        
        rv = RET_SUCCESS
        
def updateDBAndCommit(log, rsConn, rsDbLock, requests, suTable, reqID, sunums):
    try:
        sql = []

        # There is no requests table lock since only the main thread accesses the requests table.            
        gotSUTableLock = suTable.acquireLock()

        if gotSUTableLock is None:
            raise Exception('lock', 'Unable to acquire SU-table lock.')

        # Will get all SUs if sunums is None.
        sus = suTable.get(sunums)

        for su in sus:
            try:
                gotSULock = su.acquireLock()
                if not gotSULock:
                    raise Exception('lock', 'Unable to acquire SU ' + str(su.sunum) + ' lock.')
                
                if su.giveUpTheGhost:
                    sql.append('DELETE FROM ' + suTable.tableName + ' WHERE sunum = ' + str(su.sunum))                        
                    del suTable.suDict[str(su.sunum)]
                elif su.dirty:
                    if su.new:
                        sql.append('INSERT INTO ' + suTable.tableName + '(sunum, series, retention, starttime, refcount, status, errmsg) VALUES(' + str(su.sunum) + ",'" + su.series + "', " + str(su.retention) + ", '" + su.starttime.strftime('%Y-%m-%d %T%z') + "', " + str(su.refcount) + ", '" + su.status + "', '" + su.errmsg + "')")
                    else:
                        sql.append('UPDATE ' + suTable.tableName + " SET series='" + su.series + "', retention=" + str(su.retention) + ", starttime='" + su.starttime.strftime('%Y-%m-%d %T%z') + "', refcount=" + str(su.refcount) + ", status='" + su.status + "', errmsg='" + su.errmsg + "' WHERE sunum=" + str(su.sunum))

                    su.setDirty(False)
                    su.setNew(False)
            finally:
                if gotSULock:
                    su.releaseLock()
                    
        if requests.reqDict[str(reqID)]['dirty']:
            # The only columns that this daemon will modify are status and errmsg.
            log.writeDebug([ 'Updating Requests Table for request ' + str(reqID) + '.' ])
            sql.append('UPDATE ' + requests.tableName + " SET status='" + requests.reqDict[str(reqID)]['status'] + "', errmsg='" + requests.reqDict[str(reqID)]['errmsg'] + "' WHERE requestid='" + str(reqID) + "'")

            requests.reqDict[str(reqID)]['dirty'] = False
            
        # Do the SQL.
        needsRollback = True
        try:
            gotRsDbLock = rsDbLock.acquire()
            if gotRsDbLock:
                for cmd in sql:
                    log.writeDebug([ 'Running DB command: ' + cmd + '.' ])
                    with rsConn.cursor() as cursor:
                        cursor.execute(cmd)
                    log.writeDebug([ 'DB command succeeded: ' + cmd + '.' ])
                needsRollback = False

                # The cursor has been closed, but the transaction has not been committed, as designed.
        except psycopg2.Error as exc:
            raise Exception('reqtableWrite', exc.diag.message_primary)
        finally:
            if needsRollback:
                rsConn.rollback()
            else:
                log.writeDebug([ 'Committing Req and SU changes to DB.' ])
                rsConn.commit()
                log.writeDebug([ 'Successfully committed Req and SU changes to DB.' ])
            if gotRsDbLock:
                rsDbLock.release()
    finally:
        # Always release locks.
        if gotSUTableLock:
            suTable.releaseLock()

if __name__ == "__main__":
    try:
        sumsDrmsParams = SumsDrmsParams()
        if sumsDrmsParams is None:
            raise Exception('drmsParams', 'Unable to locate DRMS parameters file (drmsparams.py).')

        parser = CmdlParser(usage='%(prog)s [ -h ] [ sutable=<storage unit table> ] [ reqtable=<request table> ] [ --dbname=<db name> ] [ --dbhost=<db host> ] [ --dbport=<db port> ] [ --binpath=<executable path> ] [ --logfile=<base log-file name> ]')
    
        # Optional parameters - no default argument is provided, so the default is None, which will trigger the use of what exists in the configuration file
        # (which is drmsparams.py).
        parser.add_argument('r', '--reqtable', help='The database table that contains records of the SU-request being processed. If provided, overrides default specified in configuration file.', metavar='<request unit table>', dest='reqtable', default=sumsDrmsParams.get('RS_REQUEST_TABLE'))
        parser.add_argument('s', '--sutable', help='The database table that contains records of the storage units being processed. If provided, overrides default specified in configuration file.', metavar='<storage unit table>', dest='sutable', default=sumsDrmsParams.get('RS_SU_TABLE'))
        parser.add_argument('n', '--nworkers', help='The number of scp worker threads.', metavar='<number of worker threads>', dest='nWorkers', type=int, default=sumsDrmsParams.get('RS_N_WORKERS'))
        parser.add_argument('t', '--tmpdir', help='The temporary directory to use for scp downloads.', metavar='<temporary directory>', dest='tmpdir', default=sumsDrmsParams.get('RS_TMPDIR'))
        parser.add_argument('-N', '--dbname', help='The name of the database that contains the series table from which records are to be deleted.', metavar='<db name>', dest='dbname', default=sumsDrmsParams.get('RS_DBNAME'))
        parser.add_argument('-U', '--dbuser', help='The name of the database user account.', metavar='<db user>', dest='dbuser', default=sumsDrmsParams.get('RS_DBUSER'))
        parser.add_argument('-H', '--dbhost', help='The host machine of the database that contains the series table from which records are to be deleted.', metavar='<db host machine>', dest='dbhost', default=sumsDrmsParams.get('RS_DBHOST'))
        parser.add_argument('-P', '--dbport', help='The port on the host machine that is accepting connections for the database that contains the series table from which records are to be deleted.', metavar='<db host port>', dest='dbport', default=int(sumsDrmsParams.get('RS_DBPORT')))
        parser.add_argument('-b', '--binpath', help='The path to executables run by this daemon (e.g., vso_sum_alloc, vso_sum_put).', metavar='<executable path>', dest='binpath', default=sumsDrmsParams.get('RS_BINPATH'))
        parser.add_argument('-l', '--loglevel', help='Specifies the amount of logging to perform. In order of increasing verbosity: critical, error, warning, info, debug', dest='loglevel', action=LogLevelAction, default=logging.ERROR)
                
        arguments = Arguments(parser)
        
        arguments.setArg('lockfile', sumsDrmsParams.get('RS_LOCKFILE'))
        arguments.setArg('dltimeout', timedelta(seconds=int(sumsDrmsParams.get('RS_DLTIMEOUT'))))
        arguments.setArg('reqtimeout', timedelta(seconds=int(sumsDrmsParams.get('RS_REQTIMEOUT'))))
        arguments.setArg('maxthreads', int(sumsDrmsParams.get('RS_MAXTHREADS')))
        arguments.setArg('scpMaxNumSUs', int(sumsDrmsParams.get('RS_SCP_MAXSUS')))
        arguments.setArg('scpMaxPayload', 1024 * 1024 * int(sumsDrmsParams.get('RS_SCP_MAXPAYLOAD')))
        arguments.setArg('scpTimeOut', timedelta(seconds=int(sumsDrmsParams.get('RS_SCP_TIMEOUT'))))
        arguments.setArg('logdir', sumsDrmsParams.get('RS_LOGDIR'))
        
        arguments.setArg('sumsdbname', sumsDrmsParams.get('DBNAME') + '_sums')
        arguments.setArg('sumsdbuser', sumsDrmsParams.get('SUMS_MANAGER'))
        arguments.setArg('sumsdbhost', sumsDrmsParams.get('SUMS_DB_HOST'))
        arguments.setArg('sumsdbport', int(sumsDrmsParams.get('SUMPGPORT')))
        
        arguments.setArg('tapesysexists', int(sumsDrmsParams.get('SUMS_TAPE_AVAILABLE')) == 1)
        
        pid = os.getpid()

        # Create/Initialize the log file.
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
        rslog = Log(os.path.join(sumsDrmsParams.get('RS_LOGDIR'), LOG_FILE_BASE_NAME + '_' + datetime.now().strftime('%Y%m%d') + '.txt'), arguments.loglevel, formatter)
        rslog.writeCritical([ 'Starting up remote-SUMS daemon.' ])
        rslog.writeCritical([ 'Logging threshold level is ' + rslog.getLevel() + '.' ]) # Critical - always write the log level to the log.
        arguments.dump(rslog)
        
        rslog.writeInfo([ 'Setting download timeout to ' + str(arguments.dltimeout) + ' (the daemon must complete the download within this interval).' ])
        rslog.writeInfo([ 'Setting request timeout to ' + str(arguments.reqtimeout) + ' (the daemon must locate the request within this interval).' ])
        rslog.writeInfo([ 'Setting scp timeout to ' + str(arguments.scpTimeOut) + ' (each ScpWorker waits this long at most before initiating the next scp.).' ])

        thContainer = [ arguments, str(pid), rslog, None ]
        
        # TerminationHandler opens a DB connection to the RS database (which is the same as the DRMS database, most likely).
        with TerminationHandler(thContainer) as th:
            rsConn = th.rsConn()
            sumsConn = th.sumsConn()

            rsDbLock = threading.RLock() # global
            sumsDbLock = threading.Lock() # global
            
            rslog.writeInfo([ 'Obtained script file lock.' ])

            suTable = arguments.sutable
            reqTable = arguments.reqtable

            sus = None
            requests = None
            sites = None

            # Read the storage-unit and request tables. Do this only once per daemon run. However, we save this table
            # information every iteration of the daemon loop, just in case a crash happens. After a crash, when
            # the daemon starts up, it will retrieve the latest saved information so the disruption will be minimal.
            # We will have to clean up any pending downloads since the threads managing those downloads will have
            # been lost, and we cannot trust that the downloads completed successfully (although they might have).
            # A fancier implementation would be some kind of download manager that can recover partially downloaded
            # storage units, but who has the time :)

            SuTable.rsConn = rsConn # Class variable
            SuTable.sumsConn = sumsConn # Class variable
            SuTable.rsDbLock = rsDbLock # Class variable
            SuTable.sumsDbLock = sumsDbLock # Class variable
            sus = SuTable(suTable, arguments.dltimeout, rslog)

            ReqTable.rsConn = rsConn # Class variable
            ReqTable.rsDbLock = rsDbLock # Class variable
            requests = ReqTable(reqTable, arguments.reqtimeout, rslog)
            
            Downloader.sumsConn = sumsConn
            Downloader.sumsDbLock = sumsDbLock # Class variable

            sites = SiteTable(rslog)

            # This function will try to read each table 10 times before giving up (and raising an exception).
            readTables(sus, requests, sites)

            # Set max number of threads we can process at once.
            Downloader.setMaxThreads(arguments.maxthreads)
            
            #################
            # RESTART W SUS #
            #################
            # Recover working downloads that got disrupted from a daemon crash/shutdown. The number of working thread must
            # be fewer than the max number of threads allowed (# P plus # W status threads <= max number threads).
            susWorking = sus.getWorking()
            if len(susWorking) > Downloader.maxThreads:
                raise Exception('maxThreads', 'There are too many SU records with status W. Please delete all records from ' + suTable + ' and ' + reqTable + ' and start daemon again.')

            siteSunums = {}
            for asu in susWorking:
                timeNow = datetime.now(asu.starttime.tzinfo)
                if timeNow > asu.starttime + sus.getTimeout():
                    # Set SU status to 'E'.
                    rslog.writeInfo([ 'Download of SUNUM ' + str(asu.sunum) + ' timed-out.' ])
                    asu.setStatus('E', 'Download timed-out.')
                    continue

                rslog.writeInfo([ 'Recovering interrupted download for SUNUM ' + str(asu.sunum) + '.' ])

                try:
                    siteURL = sites.getURL(asu.sunum)
                except Exception as exc:
                    if len(exc.args) != 2:
                        raise # Re-raise

                    etype = exc.args[0]
                    msg = exc.args[1]
                
                    if etype is not 'unknownSitecode':
                        raise
                        
                    # Skip this SU. No need to acquire lock since no other threads are operating on SUs at this point.
                    asu.setStatus('E', 'Uknown site code for SU ' + str(asu.sunum) + '.')
                    continue
                
                if siteURL not in siteSunums:
                    siteSunums[siteURL] = []

                siteSunums[siteURL].append(asu.sunum)

            sus.updateDbAndCommit()
            
            # There is no need to acquire the SU-table lock. processSUs() will start new threads that can modify the
            # SU-record statuses, but by the time that happens, the main thread will be done reading those statuses.
            for url, sunumList in siteSunums.items():
                if len(sunumList) > 0:
                    # Chunk is a list of SUNUMs (up to 64 of them).
                    sunumList.sort()
                    chunker = Chunker(sunumList, 64)
                    for chunk in chunker:
                        # processSUs(..., reprocess=False) would attempt to insert a new record in the sus table for each SUNUM. By
                        # setting the last reprocess argument to True, we do not insert a new record, but instead continue to use
                        # the existing record. 
                        try:
                            processSUs(url, chunk, sus, requests, None, arguments.dbuser, arguments.binpath, arguments, rslog, True)
                        except Exception as exc:
                            if len(exc.args) != 2:
                                raise # Re-raise

                            etype = exc.args[0]
                            msg = exc.args[1]

                            # Do not die - just reject reprocess attempt of the request. Eventually, the downloads will time-out, and the 
                            # status of the request will be marked 'E'.
                            rslog.writeInfo([ 'Failed to reprocess SUs ' + ','.join([ str(asunum) for asunum in chunk ]) + '.' ])                    
                
            # Make N threads that handle scp commands. Each of the worker threads will use one of these threads to perform
            # the actual scp command.
            # MUST do this here, after all 'W' SU Downloader threads have been started, but before 'P' Downloader threads have been
            # started. The ScpWorker threads need the parent Downloader threads to exist before it can process 'W' SUs. But if there
            # are more than maxThreads 'P' SUs, then the processSUs code will block until threads free up, and that cannot happen
            # if the ScpWorker threads are not running.
            for nthread in range(1, arguments.nWorkers + 1):
                ScpWorker.lock.acquire()
                try:
                    if len(ScpWorker.tList) < ScpWorker.maxThreads:
                        rslog.writeInfo([ 'Instantiating ScpWorker ' + str(nthread) + '.' ])
                        ScpWorker.newThread(nthread, sus, arguments, rslog)
                    else:
                        break # The finally clause will ensure the ScpWorker lock is released.
                finally:
                    ScpWorker.lock.release()

            #################
            # RESTART P SUS #
            #################
            # Recover pending downloads that got disrupted from a daemon crash. All SU downloads that are in the pending
            # state at the time the tables are read were disrupted. There are no other threads running at this point, so 
            # there is no need to acquire a lock.
            susPending = sus.getPending()
            
            siteSunums = {}
            for asu in susPending:
                timeNow = datetime.now(asu.starttime.tzinfo)
                if timeNow > asu.starttime + sus.getTimeout():
                    # Set SU status to 'E'.
                    rslog.writeInfo([ 'Download of SUNUM ' + str(asu.sunum) + ' timed-out.' ])
                    asu.setStatus('E', 'Download timed-out.')
                    continue

                rslog.writeInfo([ 'Recovering interrupted download for SUNUM ' + str(asu.sunum) + '.' ])

                try:
                    siteURL = sites.getURL(asu.sunum)
                except Exception as exc:
                    if len(exc.args) != 2:
                        raise # Re-raise

                    etype = exc.args[0]
                    msg = exc.args[1]
                
                    if etype is not 'unknownSitecode':
                        raise
                        
                    # Skip this SU. No need to acquire lock since no other threads are operating on SUs at this point.
                    asu.setStatus('E', 'Uknown site code for SU ' + str(asu.sunum) + '.')
                    continue
                
                if siteURL not in siteSunums:
                    siteSunums[siteURL] = []

                siteSunums[siteURL].append(asu.sunum)

            sus.updateDbAndCommit()

            # There is no need to acquire the SU-table lock. processSUs() will start new threads that can modify the
            # SU-record statuses, but by the time that happens, the main thread will be done reading those statuses.
            for url, sunumList in siteSunums.items():
                if len(sunumList) > 0:
                    # Chunk is a list of SUNUMs (up to 64 of them).
                    sunumList.sort()
                    chunker = Chunker(sunumList, 64)
                    for chunk in chunker:
                        # processSUs(..., reprocess=False) would attempt to insert a new record in the sus table for each SUNUM. By
                        # setting the last reprocess argument to True, we do not insert a new record, but instead continue to use
                        # the existing record. 
                        try:
                            processSUs(url, chunk, sus, requests, None, arguments.dbuser, arguments.binpath, arguments, rslog, True)
                        except Exception as exc:
                            if len(exc.args) != 2:
                                raise # Re-raise

                            etype = exc.args[0]
                            msg = exc.args[1]

                            # Do not die - just reject reprocess attempt of the request. Eventually, the downloads will time-out, and the 
                            # status of the request will be marked 'E'.
                            rslog.writeInfo([ 'Failed to reprocess SUs ' + ','.join([ str(asunum) for asunum in chunk ]) + '.' ])
            
            # Start of main loop.
            loopN = 0
            while True:
                # For each 'P' request in the request table, check to see if the requested downloads have completed yet.

                # Log every 5 seconds.
                timeToLog = (loopN % 5 == 0)
                
                # PROCESS PENDING REQUESTS.
                reqsPending = requests.getPending()
                for arequest in reqsPending:
                    done = True
                    reqError = False
                    sunums = arequest['sunums']
                    errMsg = ''
                    processing = {}

                    for asunum in sunums:
                        asu = sus.get([ asunum ])[0]

                        if str(asunum) in processing:
                            # Skip duplicates.
                            rslog.writeInfo([ 'Skipping pending request for SU ' + str(asunum) + ' - this is a duplicate SU.' ])
                            continue
                        else:
                            processing[str(asunum)] = True                                        
                        
                        if asu.status == 'P' or asu.status == 'W' or asu.status == 'D':
                            rslog.writeInfo([ 'Download of SU ' + str(asu.sunum)  + ' is pending.' ])
                            done = False
                        elif asu.status == 'E':
                            rslog.writeInfo([ 'Download of SU ' + str(asu.sunum)  + ' has errored-out.' ])
                            errMsg = asu.errmsg
                            reqError = True
                        elif asu.status == 'C':
                            rslog.writeInfo([ 'Download of SU ' + str(asu.sunum)  + ' has completed.' ])
                        else:
                            raise Exception('unknownStatus', 'SU ' + str(asu.sunum) + ' has an unkonw status of ' + asu.status + '.')
                    
                    if done:
                        # There are no pending downloads for this request. Set this request's status to 'C' or 'E', and decrement
                        # refcount on each SU.
                        if reqError:
                            rslog.writeInfo([ 'Request number ' + str(arequest['requestid']) + ' for SUNUM(s) ' + ','.join([ str(asunum) for asunum in arequest['sunums'] ]) + ' errored-out.' ])
                            requests.setStatus([ arequest['requestid'] ], 'E', errMsg)
                        else:
                            rslog.writeInfo([ 'Request number ' + str(arequest['requestid']) + ' for SUNUM(s) ' + ','.join([ str(asunum) for asunum in arequest['sunums'] ]) + ' completed successfully.' ])
                            requests.setStatus([ arequest['requestid'] ], 'C')

                        # These next two calls can modify the db state! Put them in a transaction so that they form an atomic
                        # operation. We do not want an interruption to cause the first to happen, but not the second.
                        needsCommit = False
                        
                        # ART - Do not hold rsDbLock here! The functions called may acquire other locks, leading to deadlock.
                        # The rsDbLock should be for executing SQL only.
                        rslog.writeDebug([ 'Decrementing refcount for SUs: ' + ','.join([ str(ansunum) for ansunum in list(set(sunums)) ]) ])
                        # Remove duplicates from list first. We do not need to preserve the order of the SUNUMs
                        # before calling decrementRefcount() since that function uses a hash lookup on the SUNUM
                        # to find the associated refcount. Does not modify SU db table.
                        sus.decrementRefcount(list(set(sunums)))

                        # Commit both DB tables to DB in a single transaction.
                        updateDBAndCommit(rslog, rsConn, rsDbLock, requests, sus, arequest['requestid'], None)

                # Right here is where we can find orphaned sus records and delete them. We have a list of all reachable SUs now that we've 
                # iterated through the pending requests. We now iterate through ALL sus records and delete any that are not reachable.
                # xxx

                # For each 'N' request in the request table, start a new set of downloads (if there is no download currently running -
                # i.e., no SU record) or increment the refcounts on the downloads (if there are downloads currently running - i.e.,
                # an SU record exists). But Before starting a new download, make sure that requested SU is not already online.
                # Due to race conditions, a request could have caused a download to occur needed by another request whose state is 'N'.
                requests.refresh() # Clients may have added requests to the queue.

                # PROCESS NEW REQUESTS
                reqsNew = requests.getNew()

                for arequest in reqsNew:
                    timeNow = datetime.now(arequest['starttime'].tzinfo)
                    if timeNow > arequest['starttime'] + requests.getTimeout():
                        rslog.writeInfo([ 'Request number ' + str(arequest['requestid']) + ' timed-out.' ])
                        requests.setStatus([arequest['requestid']], 'E', 'Request timed-out.')
                        needsCommit = False
                        try:
                            gotRsDbLock = rsDbLock.acquire()
                            if gotRsDbLock:
                                with rsConn.cursor() as cursor:
                                    requests.updateDB([ arequest['requestid'] ])
                                needsCommit = True
                        finally:
                            if needsCommit:
                                rsConn.commit()
                            else:
                                rsConn.rollback()
                                
                            if gotRsDbLock:
                                rsDbLock.release()
                                
                        # On to next request.
                        continue
                    
                    sunums = arequest['sunums']
                    rslog.writeInfo([ 'Found a new download request, id ' + str(arequest['requestid']) + ' for SUNUMs ' + ','.join([str(asunum) for asunum in sunums]) + '.' ])
                    
                    # Get all SU records for which a download is already in progress.
                    unknown = [] # An SU that is not being processed
                    known = [] # An SU that is being processed.
                    processing = {} # The SUs in sunums that are currently being processed. Use this to avoid duplicate SUs.
                    skipRequest = False

                    for asunum in sunums:
                        if str(asunum) in processing:
                            # Skip duplicates.
                            rslog.writeInfo([ 'Skipping request for SU ' + str(asunum) + ' - this is a duplicate SU.' ])
                            continue
                        else:
                            processing[str(asunum)] = True
                            
                        try:
                            asu = sus.get([ asunum ])[0] # Will raise if asunum is unknown.
                            
                            # If we get here, then the SU is already being processed as part of a previous request.
                            # If the SU is in the 'E' or 'C' state, then we cannot process this request. We must wait until
                            # this SU has been cleared out of the sus table when the pending requests are processed.
                            if asu.status != 'P':
                                rslog.writeInfo([ 'Deferring request, id ' + str(arequest['requestid']) + '. At least one previous request for this SU must be completed first.' ])
                                skipRequest = True
                                break
                            
                            rslog.writeInfo([ 'A download for SU ' + str(asunum)+ ' is already in progress.' ])
                            known.append(asunum)
                        except Exception as exc:
                            if len(exc.args) != 2:
                                raise # Re-raise
                                
                            etype = exc.args[0]
                            msg = exc.args[1]
    
                            if etype == 'unknownSunum':
                                unknown.append(asunum)
                                
                    if skipRequest:
                        continue

                    # Increment the refcount on all SU records for the SUs being requested by the new request. This modifies the
                    # sus object.
                    sus.incrementRefcount(known)
                    
                    offlineSunums = SuTable.offline(unknown, arguments.binpath, rslog)
                    offlineSunumsDict = dict([ (str(asunum), True) for asunum in offlineSunums ])
                    
                    dlsToStart = []
                    toComplete = []
                    for asunum in unknown:
                        if str(asunum) in offlineSunumsDict:
                            rslog.writeInfo([ 'SU ' + str(asunum) + ' is offline - will start a download' ])
                            dlsToStart.append(asunum)
                        else:
                            rslog.writeInfo([ 'SU ' + str(asunum) + ' is online already - will NOT start a download.' ])
                            toComplete.append(asunum)
                    
                    # Insert a new SU record for all unknown SUs that are already online. These calls modify the sus object.
                    sus.insert(toComplete)
                    sus.setStatus(toComplete, 'C')
                        
                    # Start downloads for all unknown, offline SUs
                    siteSunums = {}
                    for asunum in dlsToStart:
                        try:
                            siteURL = sites.getURL(asunum)
                        except Exception as exc:
                            if len(exc.args) != 2:
                                raise # Re-raise

                            etype = exc.args[0]
                            msg = exc.args[1]
                
                            if etype is not 'unknownSitecode':
                                raise
                        
                            # Skip this request - invalid SU.
                            msg = 'Uknown site code for SU ' + str(asunum) + '. Skipping request ' + str(arequest['requestid']) + '.'
                            rslog.writeError([ msg ])
                            requests.setStatus([ arequest['requestid'] ], 'E', msg)
                            skipRequest = True
                            break
                        
                        if siteURL not in siteSunums:
                            siteSunums[siteURL] = []
                        
                        siteSunums[siteURL].append(asunum)
                        
                    requests.updateDbAndCommit()
                        
                    if skipRequest:
                        # On to next new request.
                        continue
                        
                    for url, sunumList in siteSunums.items():
                        if len(sunumList) > 0:
                            # Chunk is a list of SUNUMs (up to 64 of them).
                            sunumList.sort()
                            chunker = Chunker(sunumList, 64)
                            for chunk in chunker:
                                # We want to always insert a record for each SU into the SU table. Do not provide the insertRec
                                # argument to do so. This call creates new SU-table records, so it modifies the sus object.
                                processSUs(url, chunk, sus, requests, arequest, arguments.dbuser, arguments.binpath, arguments, rslog)
                    
                    # The new request has been fully processed. Change its status from 'N' to 'P'.
                    # This call modifies the requests object.
                    requests.setStatus([arequest['requestid']], 'P')

                    # At this point, both the requests and sus object have been modified, but have not been flushed to disk.
                    # Flush them, but do this inside a transaction so that the first does not happen without the second.
                    needsCommit = False
                    try:
                        sus.updateDB(sunums)
                        requests.updateDB([ arequest['requestid'] ])
                        needsCommit = True
                    finally:
                        if needsCommit:
                            rsConn.commit()
                        else:
                            rsConn.rollback()

                # Delete all request-table records whose state is 'D'. It doesn't matter if this operation gets interrupted. If
                # that happens, then these delete-pending records will be deleted the next time this code runs uninterrupted.
                reqsToDelete = requests.getDelete()
                requests.deleteDB(reqsToDelete)
               
                # Must poll for new requests to appear in requests table.
                time.sleep(1)
                loopN += 1
                # I wonder if Python throws an exception if loopN rolls-over. Does it roll-over at the 32-bit or 64-bit boundary? 
                if loopN >= 0x7FFFFFFF:
                    loopN = 0
                # End of main loop.
            
            # Save the db state when exiting.
            rslog.writeInfo([ 'Remote-sums daemon is exiting. Saving database tables.' ])
            needsCommit = False
            try:
                sus.updateDB()
                requests.updateDB()
                needsCommit = True
            finally:
                if needsCommit:
                    rsConn.commit()
                else:
                    rsConn.rollback()

        # DB connection was terminated.            
        # Lock was released
        if thContainer[3] is not None:
            rv = thContainer[3]
        
    except Exception as exc:
        if len(exc.args) != 2:
            raise # Re-raise
        
        etype = exc.args[0]
        msg = exc.args[1]
       
        if etype == 'drmsLock':
            rslog.writeError([ 'Error locking file: ' + lockFile + '\n' + msg ])
            rv = RET_LOCK
        elif etype == 'CmdlParser-ArgUnrecognized' or etype == 'CmdlParser-ArgBadformat':
            rslog.writeError([ msg ])
            rv = RET_INVALIDARGS
        elif etype == 'invalidArg':
            rslog.writeError([ msg ])
            rv = RET_INVALIDARGS
        elif etype == 'sitetableRead':
            rslog.writeError([ 'Unable to load site table: ' + msg ])
            rv = RET_SITETABLE_LOAD
        elif etype == 'sutableRead':
            rslog.writeError([ 'Unable to read from storage-unit table: ' + msg ])
            rv = RET_SUTABLE_READ
        elif etype == 'sutableWrite':
            rslog.writeError([ 'Unable to write to storage-unit table: ' + msg ])
            rv = RET_SUTABLE_WRITE
        elif etype == 'reqtableRead':
            rslog.writeError([ 'Unable to read from storage-unit-request table: ' + msg ])
            rv = RET_REQTABLE_READ
        elif etype == 'reqtableWrite':
            rslog.writeError([ 'Unable to write to storage-unit-request table: ' + msg ])
            rv = RET_REQTABLE_WRITE
        elif etype == 'badLogfile':
            print('Cannot access log file: ' + msg, file=sys.stderr)
            rv = RET_LOGFILE
        elif etype == 'badLogwrite':
            print('Cannot access log file: ' + msg, file=sys.stderr)
            rv = RET_LOGFILE
        elif etype == 'findOffline':
            rslog.writeError([ 'Cannot determine online disposition: ' + msg ])
            rv = RET_OFFLINE
        elif etype == 'getRetention':
            rslog.writeError([ 'Cannot obtain retention value: ' + msg ])
            rv = RET_GETRETENTION
        elif etype == 'unknownRequestid':
            rslog.writeError([ 'Oops! ' + msg ])
            rv = RET_UKNOWNREQUEST
        elif etype == 'unknownSunum':
            rslog.writeError([ 'Oops! ' + msg ])
            rv = RET_UKNOWNSU
        elif etype == 'workerRef':
            rslog.writeError([ msg ])
            rv = RET_WORKERREF
        elif etype == 'noReference':
            rslog.writeError([ msg ])
            rv = RET_UKNOWNSU
        elif etype == 'unknownSitecode':
            rslog.writeError([ msg ])
            rv = RET_UKNOWNSITECODE
        elif etype == 'knownSunum':
            rslog.writeError([ msg ])
            rv = RET_DUPLICATESUNUM
        elif etype == 'dbUpdate':
            rslog.writeError([ msg ])
            rv = RET_DBUPDATE
        elif etype == 'sumsAPI':
            rslog.writeError([ msg ])
            rv = RET_SUMS
        elif etype == 'maxThreads':
            rslog.writeError([ msg ])
            rv = RET_TOOMANYTHREADS
        elif etype == 'unknownStatus':
            rs.log.writeError([ msg ])
            rv = RET_UNKNOWNSTATUS
        else:
            rslog.writeError([ 'Unhandled exception. Remote-sums daemon is exiting. Rolling back uncommitted database changes. ' ])
            raise # Re-raise

if rslog:
    rslog.writeInfo([ 'Exiting with return status ' + str(rv) + '.' ])
# Will not exit process if threads are still running. Set global shutdown flag that threads are monitoring. When they see the flag, they
# will terminate too.

logging.shutdown()
sys.exit(rv)
