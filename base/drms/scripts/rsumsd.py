#!/usr/bin/env python

from __future__ import print_function
import sys
import re
import os
import stat
import filecmp
import thread
import psycopg2
import threading
import fcntl
from datetime import datetime, timedelta
import pytz
import urllib
import urllib2
import json
import signal
import time
from copy import deepcopy
import shutil
import psycopg2
import random
sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../../include'))
from drmsparams import DRMSParams
sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../../base/libs/py'))
from drmsCmdl import CmdlParser
from drmsLock import DrmsLock
from subprocess import check_output, check_call, CalledProcessError, Popen, PIPE

# This script runs as a daemon at the site that has requested an SU that does not belong to the site. It is responsible for contacting
# the owning site and requesting the path to the desired SUs. The owning site must be running the rs.sh CGI to respond to the requesting
# site's request.

# There are three database tables: 1., a DRMS site table, 2., a request table, and 3., an SU table (sunum, starttime, status, errmsg)
# The site table (sitename, sitecode, baseurl) provides
# the information needed to query the providing site for the information needed to scp the requested SUs to the requesting site. The URL to the cgi
# program that provides scp information is formed by appending "rs.py" to baseurl. There are two parameters for this cgi: 1., "requestid", and 2., "sunums".
# During the initial request to the providing site, the requesting site will provide a requestid of "none", and a comma-separated list of SUNUMs
# in the sunums argument. Should the providing site have all the requested SUs online, then it will return the scp information needed to access those
# SUs, and a status of "complete". If, however, the SUs are not all online, the initial request will start an asynchronous tape-read of the offline SUs,
# returning a request ID that identifies the initial request, and a status of "pending". The requesting site must then poll for completion by
# periodically calling the cgi with a status request. To make a status request, the requestid argument contains the requestid returned by the
# inital CGI call, and the sunums argument contains "none". While the data are not ready, the status request returns a status of "pending". When
# the data are online, the status request returns a status of "complete".
#
# The request table (requestid, sunums, status) is populated by drms_storageunit.c. For all SUs that are offline and are not owned by the running
# DRMS/SUMS, the code inserts a record into the request table. The requestid is a UUID (within the running DRMS/SUMS) generated by a sequence table.
# The list of SUNUMs is put in sunums, and the initial status is set to 'N' (New request). drms_storageunit.c chunks such SUNUMs into
# manageable-sized requests. After inserting one or more such records into the request table, drms_storageunit.c then polls these records,
# waiting for the status to become 'C' (Complete request). After that happens, drms_storageunit.c then calls SUM_get() again on these SUs
# to obtain their paths. This daemon, rsumsd.py, periodically and reads all records in the request table. For each SU in each 'N' record
# (a single request, which could be requesting multiple SUs) the daemon first checks if the SU is already being processed. If so, then
# the SU table is not modified. The status of the record in the request table is set to 'P'. If the SU is not in the SU table, then
# this can mean one of two things. The daemon has never processed this SU, or the daemon has already processed this SU. When the daemon has completed
# processing an SU, it deletes the record for the SU from the SU table. If the latter is true, the daemon should not re-process the same SU. To distinguish
# between these two possibilities, the daemon first checks to see if the SU is already present in SUMS (it calls show_info -o sunum=SUNUM).
# If the SU is online, then the daemon does nothing. But if it is offline, then the daemon inserts a record for the SU into the SU table, and starts
# processing that SU. The status of that SU-table record is set to 'P', as is the status of the request record containing that SUNUM.
#
# For each SU in each 'P' request record, the daemon searches for records in the SU table. If one or more such records exist in the SU table, then
# the request record is left in the pending state. The next time the daemon scans the request records, it will again check the SU table looking
# for completion of all SUs. When that occurs, the status of the request record is set to 'C', indicating that the request is complete. The
# drms_storageunit.c code will then call SUMS to get the newly created paths to the requested SUs.

RET_SUCCESS = 0
RET_INVALIDARGS = 1
RET_LOCK = 2
RET_SUTABLE_READ = 3
RET_SUTABLE_WRITE = 4
RET_REQTABLE_READ = 5
RET_REQTABLE_WRITE = 6
RET_SITETABLE_LOAD = 7
RET_DBCOMMAND = 8
RET_LOGFILE = 9
RET_OFFLINE = 10
RET_GETRETENTION = 11
RET_UKNOWNREQUEST = 12
RET_UKNOWNSU = 13
RET_WORKERREF = 14
RET_UKNOWNSITECODE = 15
RET_DUPLICATESUNUM = 16
RET_DBUPDATE = 17
RET_SUMS = 18

LOG_FILE_BASE_NAME = 'rslog'

class SumsDrmsParams(DRMSParams):
    def __init__(self):
        super(SumsDrmsParams, self).__init__()

    def get(self, name):
        val = super(SumsDrmsParams, self).get(name)

        if val is None:
            raise Exception('drmsParams', 'Unknown DRMS parameter: ' + name + '.')
        return val


class Arguments(object):

    def __init__(self, parser):
        # This could raise in a few places. Let the caller handle these exceptions.
        self.parser = parser
        
        # Parse the arguments.
        self.parse()
        
        # Set all args.
        self.setAllArgs()
        
    def parse(self):
        try:
            self.parsedArgs = self.parser.parse_args()      
        except Exception as exc:
            if len(exc.args) == 2:
                type, msg = exc
                  
                if type != 'CmdlParser-ArgUnrecognized' and type != 'CmdlParser-ArgBadformat':
                    raise # Re-raise

                raise Exception('args', msg)
            else:
                raise # Re-raise

    def setArg(self, name, value):
        if not hasattr(self, name):
            # Since Arguments is a new-style class, it has a __dict__, so we can
            # set attributes directly in the Arguments instance.
            setattr(self, name, value)
        else:
            raise Exception('args', 'Attempt to set an argument that already exists: ' + name + '.')

    def setAllArgs(self):
        for key,val in list(vars(self.parsedArgs).items()):
            self.setArg(key, val)
        
    def getArg(self, name):
        try:
            return getattr(self, name)
        except AttributeError as exc:
            raise Exception('args', 'Unknown argument: ' + name + '.')

class Log(object):
    """Manage a logfile."""

    def __init__(self, file):
        self.fileName = file
        self.fobj = None
        
        try:
            head, tail = os.path.split(file)

            if not os.path.isdir(head):
                os.mkdir(head)
            fobj = open(self.fileName, 'a')
        except OSError as exc:
            type, value, traceback = sys.exc_info()
            raise Exception('badLogfile', 'Unable to access ' + "'" + value.filename + "'.")
        except IOError as exc:
            type, value, traceback = sys.exc_info()
            raise Exception('badLogfile', 'Unable to open ' + "'" + value.filename + "'.")
        
        self.fobj = fobj

    def __del__(self):
        if self.fobj:
            self.fobj.close()

    def write(self, text):
        try:
            lines = ['[' + datetime.now().strftime('%Y-%m-%d %T') + '] ' + line + '\n' for line in text]
            self.fobj.writelines(lines)
            self.fobj.flush()
        except IOError as exc:
            type, value, traceback = sys.exc_info()
            raise Exception('badLogwrite', 'Unable to write to ' + value.filename + '.')

class StorageUnit(object):
    def __init__(self, sunum, series, retention, starttime, refcount, status, errmsg):
        self.sunum = sunum
        self.series = series
        self.retention = retention
        self.starttime = starttime
        self.refcount = refcount
        self.status = status
        self.errmsg = errmsg
        
        # Not saved in the DB.
        self.giveUpTheGhost = False
        self.dirty = False
        self.new = False
        self.polling = False
        self.path = None
        self.suSize = None
        self.worker = None
        
        self.lock = thread.allocate_lock()
        
    def acquireLock(self):
        return self.lock.acquire()
    
    def releaseLock(self):
        self.lock.release()
        
    def setDirty(self, value):
        if not isinstance(value, (bool)):
            raise Exception('invalidArg', 'setDirty(): argument must be a bool.')
        
        self.dirty = value
    
    # Set properties that are NOT saved to the DB.
    def setNew(self, value):
        if not isinstance(value, (bool)):
            raise Exception('invalidArg', 'setNew(): argument must be a bool.')
            
        self.new = value
        
    def setPolling(self, value):
        if not isinstance(value, (bool)):
            raise Exception('invalidArg', 'setNew(): argument must be a bool.')
            
        self.polling = value
        
    def getPath(self):
        path = None
        if hasattr(self, 'path'):
            path = self.path
        return path
        
    def setPath(self, value):
        if isinstance(value, (str)) or value is None:
            self.path = value
        else:
            raise Exception('invalidArg', 'setPath(): argument must be a str or None.')
            
    def setSUSize(self, value):
        if isinstance(value, (int)) or value is None:
            self.path = value
        else:
            raise Exception('invalidArg', 'setPath(): argument must be an int or None.')
    
    # Set properties that are saved to the DB.
    def setStatus(self, codeValue, msgValue):
        if not isinstance(codeValue, (str)):
            raise Exception('invalidArg', 'setStatus(): fist argument must be a str.')
            
        if not isinstance(msgValue, (str)) and msgValue is not None:
            raise Exception('invalidArg', 'setStatus(): second argument must be a str or None.')
            
        self.status = codeValue
        if value is not None:
            self.errmsg = msg
        else:
            self.errmsg = ''
            
        self.setDirty(True)
            
    def setSeries(self, value):
        if not isinstance(value, (str)):
            raise Exception('invalidArg', 'setSeries(): argument must be a str.')

        self.series = value
        self.setDirty(True)
        
    def setRetention(self, value):
        if not isinstance(value, (integer)):
            raise Exception('invalidArg', 'setRetention(): argument must be an integer.')
            
        self.retention = value
        self.setDirty(True)
    
    def setStarttime(self, value):
        if not isinstance(value, (datetime.datetime)):
            raise Exception('invalidArg', 'setStarttime(): argument must be a datetime.')
            
        self.starttime = value
        self.setDirty(True)
        
    def incrementRefcount(self):
        self.refcount += 1
        self.setDirty(True)
        
    def decrementRefcount(self):
        if self.refcount == 0:
            raise Exception('noReference', 'Cannot decrement refcount on unreferenced SU record ' + str(self.sunum) + '.')
                    
        self.refcount -= 1
        if self.refcount == 0:
            self.giveUpTheGhost = True
            
        self.setDirty(True)
        
    def setWorker(self, value):
        if not isinstance(value, (Downloader)):
            raise Exception('invalidArg', 'setWorker(): argument must be a Downloader.')
            
        if self.worker:
            raise Exception('workerRef', 'Cannot set worker for SU ' + str(self.sunum) + '. Worker already exists.')
            
        self.worker = value

class SuTable:
    cursor = None
                
    def __init__(self, tableName, timeOut, log):
        self.tableName = tableName
        self.timeOut = timeOut # A timedelta object - the length of time to wait for a download to complete.
        self.lock = thread.allocate_lock()
        self.locked = False
        self.log = log
        self.suDict = {}
    
    @classmethod
    def setCursor(cls, cursorIn):
        cls.cursor = cursorIn

    def read(self):
        # sus(sunum, starttime, refcount, status, errmsg)
        cmd = 'SELECT sunum, series, retention, starttime, refcount, status, errmsg FROM ' + self.tableName
    
        try:
          cursor.execute(cmd)
    
        except psycopg2.Error as exc:
            raise Exception('sutableRead', exc.diag.message_primary)
    
        for record in cursor:
            sunumStr = str(record[0])
            
            self.suDict[sunumStr] = {}
            sunum = record[0]         # integer
            series = record[1]        # text
            retention = record[2]     # integer
            # Whoa! pyscopg returns timestamps as datetime.datetime objects already!
            starttime = record[3]     # datetime.datetime
            refcount = record[4]      # integer
            status = record[5]        # text
            errmsg = record[6]        # text

            su = StorageUnit(sunum, series, retention, starttime, refcount, status, errmsg)
            
            # Not read from or saved to database.
            su.setDirty(False)
            su.setNew(False)
            su.setPolling(False)
            su.setPath(None)              # The server path of the SU to be downloaded.
            su.setWorker(None)
            self.suDict[sunumStr] = su

    def tryRead(self):
        nAtts = 0
        while True:
            try:
                self.read()
                break
            except Exception as exc:
                if len(exc.args) != 2:
                    raise # Re-raise

                etype = exc.args[0]

                if etype == 'sutableRead':
                    if nAtts > 10:
                        raise # Re-raise
                else:
                    raise

            nAtts += 1
            time.sleep(1)

    # This will NOT commit database changes. You are generally going to want to do this as part of a larger db change. Commit the changes
    # after all changes that form the atomic set of changes.
    def updateDB(self, sunums=None):
        toDel = []
        
        if sunums:
            # Update a subset of all records.
            for asunum in sunums:
                sunumStr = str(asunum)

                try:
                    gotSuLock = False
                    # Will raise if the sunum is not in the dictionary.
                    su = self.getAndLockSU(asunum)
                    gotSuLock = True
                
                    if su.giveUpTheGhost:
                        toDel.append(asunum)
                    elif su.dirty:
                        if su.new:
                            cmd = 'INSERT INTO ' + self.tableName + '(sunum, series, retention, starttime, refcount, status, errmsg) VALUES(' + sunumStr + ",'" + su.series + "', " + str(su.retention) + ", '" + su.starttime.strftime('%Y-%m-%d %T') + "', " + str(su.refcount) + ", '" + su.status + "', '" + su.errmsg + "')"
                        else:
                            # The fields excluded from the update statement are not modified once set.
                            cmd = 'UPDATE ' + self.tableName + " SET series='" + su.series + "', retention=" + str(su.retention) + ", starttime='" + su.starttime.strftime('%Y-%m-%d %T') + "', refcount=" + str(su.refcount) + ", status='" + su.status + "', errmsg='" + su.errmsg + "' WHERE sunum=" + sunumStr
                
                        self.log.write(['Updating SU db table: ' + cmd])
                
                        try:
                            cursor.execute(cmd)
            
                        except psycopg2.Error as exc:
                            raise Exception('sutableWrite', exc.diag.message_primary)
                
                        su.setDirty(False)
                        su.setNew(False)
                finally:
                    if gotSuLock:
                        su.releaseLock()
                    
        else:
            # Update all dirty records.
            for sunumStr in self.suDict:
                try:
                    gotSuLock = False
                    # Will raise if the sunum is not in the dictionary.
                    su = self.getAndLockSU(asunum)
                    gotSuLock = True
            
                    if su.giveUpTheGhost:
                        toDel.append(asunum)
                    elif su.dirty:
                        if su.new:
                            cmd = 'INSERT INTO ' + self.tableName + '(sunum, series, retention, starttime, refcount, status, errmsg) VALUES(' + sunumStr + ",'" + su.series + "', " + str(su.retention) + ", '" + su.starttime.strftime('%Y-%m-%d %T') + "', " + str(su.refcount) + ", '" + su.status + "', '" + su.errmsg + "')"
                        else:
                            cmd = 'UPDATE ' + self.tableName + " SET series='" + su.series + "', retention=" + str(su.retention) + ", starttime='" + su.starttime.strftime('%Y-%m-%d %T') + "', refcount=" + su.refcount) + ", status='" + su.status + "', errmsg='" + su.errmsg + "' WHERE sunum=" + sunumStr
                    
                        self.log.write(['Updating SU db table: ' + cmd])
                    
                        try:
                            cursor.execute(cmd)
                    
                        except psycopg2.Error as exc:
                            import traceback
                            self.log.write([ traceback.format_exc(5) ])
                            raise Exception('sutableWrite', traceback.format_exc(5))
                    
                        su.setDirty(False)
                        su.setNew(False)
                finally:
                    if gotSuLock:
                        su.releaseLock()
                    
        self.deleteDB(toDel)

    # This will NOT commit database changes. You are generally going to want to do this as part of a larger db change. Commit the changes
    # after all changes that form the atomic set of changes.
    def deleteDB(self, sunums):
        if len(sunums) > 0:
            # Delete the in-memory cache of these sunums
            for asunum in sunums:
                sunumStr = str(asunum)
                
                # Will raise if the sunum is not in the dictionary.
                del self.suDict[sunumStr]
            
            sunumLstStr = ','.join([str(asunum) for asunum in sunums])
        
            cmd = 'DELETE FROM ' + self.tableName + ' WHERE sunum IN (' + sunumLstStr + ')'
        
            try:
                cursor.execute(cmd)
                
            except psycopg2.Error as exc:
                raise Exception('sutableWrite', exc.diag.message_primary)

    def acquireLock(self):
        return self.lock.acquire()
        # self.log.write(['Acquired SU-Table lock.'])
    
    def releaseLock(self):
        self.lock.release()
        # self.log.write(['Released SU-Table lock.'])
    
    def insert(self, sunums):
        for asunum in sunums:
            sunumStr = str(asunum)
        
            if sunumStr in self.suDict:
                raise Exception('knownSunum', 'SU-table record already exists for SU ' + sunumStr + '.')
                
            su = StorageUnit(asunum, '', -1, datetime.now(), 1, 'P', '')

            su.setDirty(True)
            # Set the new flag (so that the record will be INSERTed into the SU database table instead of UPDATEd).
            su.setNew(True)
            # The polling flag is set only while we are waiting for a providing site to give us paths for SUs.
            su.setPolling(False)
        
            self.suDict[sunumStr] = su

    def setStatus(self, sunums, code, msg=None):
        for asunum in sunums:
            sunumStr = str(asunum)
            
            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            su.setStatus(code, msg)

    def setSeries(self, sunums, series):
        for asunum in sunums:
            sunumStr = str(asunum)
            
            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            su.setSeries(series)

    def setRetention(self, sunums, retention):
        for asunum in sunums:
            sunumStr = str(asunum)
            
            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            su.setRetention(retention)

    def setStarttime(self, sunums, starttime):
        for asunum in sunums:
            sunumStr = str(asunum)
            
            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            su.setStarttime(starttime)

    def incrementRefcount(self, sunums):
        for asunum in sunums:
            sunumStr = str(asunum)
            
            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            su.incrementRefcount()

    def decrementRefcount(self, sunums):
        toDel = []
        for asunum in sunums:
            sunumStr = str(asunum)

            # Will raise if the sunum is not in the dictionary.
            su = self.get([ asunum ])[0]
            su.decrementRefcount()
                    
    def getWorker(self, sunum):
        sunumStr = str(sunum)
        
        # Will raise if the sunum is not in the dictionary.
        su = self.get([ sunum ])[0]
        return su.worker
        
    def setWorker(self, sunum, worker):
        sunumStr = str(sunum)

        # Will raise if the sunum is not in the dictionary.
        su = self.get([ sunum ])[0]
        su.setWorker(worker)
    
    def stopWorker(self, sunum):
        sunumStr = str(sunum)
        
        # Will raise if the sunum is not in the dictionary.
        su = self.get([ sunum ])[0]
            
        if su.worker:
            su.worker.stop()
            if su.worker.isAlive():
                # Give the worker 15 seconds to self-terminate.
                su.worker.join(15)
            if su.worker.isAlive():
                # Apparently, there is no way to kill a thread from another thread. So, we are just going to
                # orphan the thread (so it doesn't use up our maxThreads quota).
                try:
                    Downloader.lock.acquire()
                    self.log.write(['(stopWorker) Class Downloader acquired Downloader lock for SU ' + sunumStr + '.'])
                    Downloader.tList.remove(su.worker) # This thread is no longer one of the running threads.
                    if len(Downloader.tList) == Downloader.maxThreads - 1:
                        # Fire event so that main thread can add new SUs to the download queue.
                        Downloader.eventMaxThreads.set()
                        # Clear event so that main will block the next time it calls wait.
                        Downloader.eventMaxThreads.clear()
                finally:
                    Downloader.lock.release()
                    self.log.write(['(stopWorker) Class Downloader released Downloader lock for SU ' + sunumStr + '.'])

                del su.worker

    def get(self, sunums=None):
        toRet = []
        
        if not sunums:
            for sunumStr in self.suDict.iterkeys():
                toRet.append(self.suDict[sunumStr])
        else:
            for asunum in sunums:
                sunumStr = str(asunum)
        
                if not sunumStr in self.suDict or not self.suDict[sunumStr]:
                    raise Exception('unknownSunum', '(get) No SU-table record exists for SU ' + sunumStr + '.')

                toRet.append(self.suDict[sunumStr])

        return toRet
        
    # This will acquire the SU lock. The caller must release it!! Because the SU lock is acquired while the 
    # SU-table lock is held, nobody can modify the SU after this method returns and before the caller
    # releases the SU lock.
    def getAndLockSU(self, sunum):
        gotTableLock = False

        try:
            gotTableLock = self.suTable.acquireLock()

            if gotTableLock is None:
                raise Exception('lock', 'Unable to acquire SU-table lock.')
                
            su = get([ sunum ])[0]
            gotSULock = su.acquireLock()
            
            if gotSULock is None:
                raise Exception('lock', 'Unable to acquire SU ' + str(sunum) + ' lock.')
        finally:
            # Always release lock.
            if gotTableLock:
                self.suTable.releaseLock()
                
        return su

    def getAndLockSUs(self, sunums):
        gotTableLock = False
        sus = None

        try:
            gotTableLock = self.suTable.acquireLock()

            if gotTableLock is None:
                raise Exception('lock', 'Unable to acquire SU-table lock.')
                
            sus = get(sunums)
            for su in sus:
                gotSULock = su.acquireLock()            
                if not gotSULock:
                    raise Exception('lock', 'Unable to acquire SU ' + str(su.sunum) + ' lock.')
        finally:
            # Always release lock.
            if gotTableLock:
                self.suTable.releaseLock()
                
        return sus        

    # Returns a dictionary of SU records.
    def getPending(self):
        pending = []
        
        for sunumStr in self.suDict.iterkeys():
            # Will raise if the sunum is not in the dictionary.
            su = self.get([ int(sunumStr) ])[0]

            if su.status == 'P':
                pending.append(su)

        # Sorts in place - and returns None.
        pending.sort(key=lambda(su) : su.sunum)

        return pending
        
    # Returns num SU entries whose status is 'W'. If fewer than num W entries exist, then all W SU entries are returned.
    def getNextNWorking(self, num):
        nworking = []
        
        it = iter(sorted(self.suDict.items())
        try:
            while num > 0:
                su = it.next()
            
                try:
                    # We are going to access data from the SU - get lock first.
                    gotLock = su.acquireLock()
                    if gotLock is None:
                        raise Exception('lock', 'Unable to acquire SU ' + str(su.sunum) + ' lock.')

                    if su.status == 'W':
                        nworking.append(su)
                finally:
                    if gotLock:
                        su.releaseLock()
                        
                num -= 1
        except StopIteration:
            pass            
                
        return nworking

    def getTimeout(self):
        return self.timeOut

    @classmethod
    def offline(cls, sunums, binPath, log):
        # There is not an efficient way to check for the SU being on/offline. But we can use jsoc_fetch (vs. show_info - jsoc_fetch returns
        # JSON, which is handy). And it also can be called in a mode where it does not trigger a SUM_get() - it uses SUM_infoAns():
        #   op=exp_su requestid=NOASYNCREQUEST sunum=123456789 format=json formatvar=dataobj method=url_quick protocol=as-is
        rv = []        

        if len(sunums) > 0:
            cmd = [binPath + '/jsoc_fetch', 'op=exp_su', 'requestid=NOASYNCREQUEST', 'format=json', 'formatvar=dataobj', 'method=url_quick', 'protocol=as-is', 'sunum=' + ','.join([str(asunum) for asunum in sunums])]
            log.write(['Checking online disposition: ' + ' '.join(cmd)])
        
            try:
                resp = check_output(cmd)
                output = resp.decode('utf-8')
                lines = output.split('\n')
        
            except ValueError:
                raise Exception('findOffline', "Unable to run command: '" + ' '.join(cmd) + "'.")
            except CalledProcessError as exc:
                raise Exception('findOffline', "Command '" + ' '.join(cmd) + "' returned non-zero status code " + str(exc.returncode))
        
            jsonRsp = []
        
            # output is not strictly JSON. There is an HTTP header we need to remove.
            regExp = re.compile(r'Content-type')
            for line in lines:
                if len(line) == 0:
                    continue
                match = regExp.match(line)
                if match:
                    continue
                jsonRsp.append(line)
        
            jsonObj = json.loads(''.join(jsonRsp))
            for sunum in jsonObj['data']:
                # sustatus could be 'I' (the local SUMS knows nothing about this SU) or 'Y' (it is in the SUMS db, and it is online). It cannot be 'N' or 'X'
                # because if it were retrievable from tape, it would have been retrieved instead of finding its way into the requests table.
                if jsonObj['data'][sunum]['sustatus'] == 'I':
                    rv.append(sunum)
        return rv

class ReqTable:
    cursor = None
    
    def __init__(self, tableName, timeOut, log):
        self.tableName = tableName
        self.timeOut = timeOut
        self.log = log
        self.reqDict = {}
    
    @classmethod
    def setCursor(cls, cursorIn):
        cls.cursor = cursorIn
    
    def read(self):
        # requests(requestid, starttime, sunums, status, errmsg)
        cmd = 'SELECT requestid, dbhost, dbport, dbname, starttime, sunums, status, errmsg FROM ' + self.tableName
        
        try:
            cursor.execute(cmd)
        
        except psycopg2.Error as exc:
            raise Exception('reqtableRead', exc.diag.message_primary, cmd)
        
        for record in cursor:
            requestidStr = str(record[0])

            self.reqDict[requestidStr] = {}
            self.reqDict[requestidStr]['requestid'] = record[0] # integer
            self.reqDict[requestidStr]['dbhost'] = record[1]    # text
            self.reqDict[requestidStr]['dbport'] = record[2]    # integer
            self.reqDict[requestidStr]['dbname'] = record[3]    # text
            # Whoa! pyscopg returns timestamps as datetime.datetime objects already!
            self.reqDict[requestidStr]['starttime'] = record[4] # datetime.datetime
            self.reqDict[requestidStr]['sunums'] = [int(asunum) for asunum in record[5].split(',')] # text (originally)
            self.reqDict[requestidStr]['status'] = record[6]    # text
            self.reqDict[requestidStr]['errmsg'] = record[7]    # text
            self.reqDict[requestidStr]['dirty'] = False

    def tryRead(self):
        nAtts = 0
        while True:
            try:
                self.read()
                break
            except Exception as exc:
                if len(exc.args) != 2:
                    raise # Re-raise

                etype = exc.args[0]

                if etype == 'reqtableRead':
                    if nAtts > 10:
                        raise # Re-raise
                else:
                    raise

            nAtts += 1
            time.sleep(1)

    # This method finds 'N' records inserted since the last time it was run (or since the table was first read). It ignores
    # all other changes to the database table (made from outside this program) that have happened. To read those changes,
    # shut down this program, then make the changes, then start this program again.
    def refresh(self):
        if not cursor:
            raise Exception('noCursor', 'Cannot refresh the requests table because no database cursor exists.')
        
        # Delete existing items from self.
        self.reqDict = {}
        
        # Read the table from the database anew.
        self.tryRead()

    # This will NOT commit database changes. You are generally going to want to do this as part of a larger db change. Commit the changes
    # after all changes that form the atomic set of changes.
    def updateDB(self, requestids=None):
        if requestids:
            # Update the specified records.
            for arequestid in requestids:
                requestidStr = str(arequestid)
            
                if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                    raise Exception('unknownRequestid', 'No request-table record exists for ID ' + requestidStr + '.')
                
                if self.reqDict[requestidStr]['dirty']:
                    # The only columns that this daemon will modify are status and errmsg.
                    cmd = 'UPDATE ' + self.tableName + " SET status='" + self.reqDict[requestidStr]['status'] + "', errmsg='" + self.reqDict[requestidStr]['errmsg'] + "' WHERE requestid=" + requestidStr
                    
                    try:
                        cursor.execute(cmd)
                    
                    except psycopg2.Error as exc:
                        raise Exception('reqtableWrite', exc.diag.message_primary)
                    
                    self.reqDict[requestidStr]['dirty'] = False
        else:
            # Update all dirty records.
            for requestidStr in self.reqDict:
                if self.reqDict[requestidStr]['dirty']:
                    # The only columns that this daemon will modify are status and errmsg.
                    cmd = 'UPDATE ' + self.tableName + " SET status='" + self.reqDict[requestidStr]['status'] + "', errmsg='" + self.reqDict[requestidStr]['errmsg'] + "' WHERE requestid='" + requestidStr + "'"
                    
                    try:
                        cursor.execute(cmd)
                    
                    except psycopg2.Error as exc:
                        raise Exception('reqtableWrite', exc.diag.message_primary)
                    
                    self.reqDict[requestidStr]['dirty'] = False

    # This will NOT commit database changes. You are generally going to want to do this as part of a larger db change. Commit the changes
    # after all changes that form the atomic set of changes.
    def deleteDB(self, requestids):
        if len(requestids) > 0:
            for arequestid in requestids:
                requestidStr = str(requestid)
                
                if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                    raise Exception('unknownRequestid', 'No request-table record exists for ID ' + requestidStr + '.')

                del self.reqDict[requestidStr]
            
            reqidLstStr = ','.join(requestids)
            
            cmd = 'DELETE FROM ' + self.tableName + ' WHERE requestid=' + reqidLstStr
            
            try:
                cursor.execute(cmd)
            
            except psycopg2.Error as exc:
                raise Exception('reqtableWrite', exc.diag.message_primary + ': ' + cmd)
        
    def setStatus(self, requestids, code, msg=None):
        for arequestid in requestids:
            requestidStr = str(arequestid)
        
            if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                raise Exception('unknownRequestid', 'No request-table record exists for ID ' + requestidStr + '.')
            
            self.reqDict[requestidStr]['status'] = code
            if msg:
                self.reqDict[requestidStr]['errmsg'] = msg
            else:
                self.reqDict[requestidStr]['errmsg'] = ''
            
            # Set dirty flag
            self.reqDict[requestidStr]['dirty'] = True

    def get(self, requestids=None):
        toRet = []
    
        if not requestids:
            return [ self.reqDict[key] for (key, val) in self.reqDict.items() ]
        
        for arequestid in requestids:
            requestidStr = str(arequestid)
            
            if not requestidStr in self.reqDict or not self.reqDict[requestidStr]:
                raise Exception('unknownRequestid', 'No request-table record exists for ID ' + requestidStr + '.')
    
            toRet.append(self.reqDict[requestidStr])
    
        return toRet
    
    def getPending(self):
        pendLst = []
    
        for requestidStr in self.reqDict.iterkeys():
            if self.reqDict[requestidStr]['status'] == 'P':
                pendLst.append(self.reqDict[requestidStr])
    
        # Sort by start time. Sorts in place - and returns None.
        pendLst.sort(key=lambda(dict): dict['starttime'].strftime('%Y-%m-%d %T'))
            
        return pendLst
    
    def getNew(self):
        newLst = []
        
        for requestidStr in self.reqDict.iterkeys():
            if self.reqDict[requestidStr]['status'] == 'N':
                newLst.append(self.reqDict[requestidStr])
        
        # Sort by start time. Sorts in place - and returns None.
        newLst.sort(key=lambda(dict): dict['starttime'].strftime('%Y-%m-%d %T'))

        return newLst

    def getDelete(self):
        deleteLst = []

        for requestidStr in self.reqDict.iterkeys():
            if self.reqDict[requestidStr]['status'] == 'D':
                deleteLst.append(self.reqDict[requestidStr])

        deleteLst.sort(key=lambda(dict): dict['requestid'])

        return deleteLst

    def getTimeout(self):
        return self.timeOut

    # series (text) - Name of series whose retention is wanted.
    # request (ReqTable::reqDict[requestidStr] object) - Contains the dbhost, dbport, dbname that identifies the
    #    database that contains the series.
    @staticmethod
    def getRetention(series, request, dbuser, binPath, log):
        # Who you gonna call...jsoc_info! A la:
        # jsoc_info op=series_struct ds=hmi.v_avg120
        # Ack - there is Stanford-specific stuff in jsoc_info. Instead, just get the retention from the db. We have the
        # host/port/dbname information from request.
        newSuRetention = -1
        ns, tab = series.split('.')

        try:
            with psycopg2.connect(database=request['dbname'], user=dbuser, host=request['dbhost'], port=request['dbport']) as conn:
                with conn.cursor() as cursor:
                    # We want the new-SU retention too, not the staging retention. So extract the bottom 15 bits.
                    cmd = "SELECT retention & x'00007FFF'::int AS retention FROM " + ns + ".drms_series WHERE seriesname ILIKE '" + series + "'"
                    log.write(['Obtaining new-SU retention for series ' + series + ': ' + cmd])

                    try:
                        cursor.execute(cmd)
                        newSuRetention = cursor.fetchone()[0]
                        log.write(['Retention is ' + str(newSuRetention) + '.'])
                    except psycopg2.Error as exc:
                        # Handle database-command errors.
                        raise Exception('getRetention', exc.diag.message_primary)

        except psycopg2.DatabaseError as exc:
            # Closes the cursor and connection

            # Man, there is no way to get an error message from any exception object that will provide any information why
            # the connection failed.
            msg = 'Unable to connect to the database (no, I do not know why).'
            raise Exception('getRetention', msg)

        return newSuRetention

# The site information is stored in a public database table at Stanford. It is accessible by all remote sites
# with the sites.py cgi. To obtain information about all sites, the sites.py cgi is called with no parameters.
# Otherwise, information about a single site can be obtained by by providing the site name to the 'site' argument.
# The information is stored in drms.rs_sites on hmidb.
class SiteTable:
    def __init__(self, log):
        self.log = log
        self.siteDict = {} # Keyed by name.
        self.siteMap = {} # Map from str(code) to name.

    def read(self):
        url = 'http://jsoc.stanford.edu/cgi-bin/rssites.sh'
        req = urllib2.Request(url)
        response = urllib2.urlopen(req)
        siteInfoStr = response.read()

        # siteInfoStr is a string, that happens to be json.
        siteInfo = json.loads(siteInfoStr)
        
        if siteInfo['status'] != 'success':
            raise Exception('sitetableRead', "Failure calling cgi '" + url + "'.")

        # siteInfo is a dictionary, keyed by site name. Each dictionary entry is a dictionay, with two keys: code and baseurl.
        for asite in siteInfo.iterkeys():
            if asite == 'status':
                # Skip status.
                continue
            self.siteDict[asite] = {}
            self.siteDict[asite]['name'] = asite
            self.siteDict[asite]['code'] = siteInfo[asite]['code']
            self.siteDict[asite]['baseurl'] = siteInfo[asite]['baseurl']
            self.siteMap[str(self.siteDict[asite]['code'])] = asite

            self.log.write(['Reading site info for ' + asite + ': code => ' + str(self.siteDict[asite]['code']) + ', baseurl => ' + self.siteDict[asite]['baseurl']])

    def tryRead(self):
        nAtts = 0
        while True:
            try:
                self.read()
                break
            except Exception as exc:
                if len(exc.args) != 2:
                    raise # Re-raise

                etype = exc.args[0]

                if etype == 'sitetableRead':
                    if nAtts > 10:
                        raise # Re-raise
                else:
                    raise

            nAtts += 1
            time.sleep(1)

    @staticmethod
    def getCode(sunum):
        code = sunum >> 48
        if code & 0xC000 != 0:
            raise Exception('badSunum', 'The site-code value of SUNUM ' + sunum + ' is out of range (valid range is 0 to 16383).')

        return code

    def getURL(self, sunum):
        code = SiteTable.getCode(sunum)
        
        if not str(code) in self.siteMap or not self.siteMap[str(code)]:
            raise Exception('unknownSitecode', 'There is no site in the site table for code ' + str(code) + '.')
        
        name = self.siteMap[str(code)]
        url = self.siteDict[name]['baseurl']
        return url

class Chunker(object):
    def __init__(self, list, chSize):
        self.chunks = []
        iChunk = -1
        nElem = 1
        
        for elem in list:
            if iChunk == -1 or nElem % chSize == 0:
                iChunk += 1
                self.chunks.append([])
    
            self.chunks[iChunk].append(elem)
            nElem += 1
    
    def __iter__(self):
        return self.iterate()
    
    # Iterate through chunks.
    def iterate(self):
        i = 0
        while i < len(self.chunks):
            yield self.chunks[i]
            i += 1
            
class ScpWorker(threading.Thread):
    tList = [] # A list of running thread IDs.
    maxThreads = 8
    scpNeeded = threading.Event() # Event fired by main when a Downloader thread has changed the state of an SU from 'P' to 'W'
    scpCompleted = threading.Event()  # Event fired by ScpWorker when an ScpWorker thread had completed an SU download
    lock = threading.Lock() # Guard tList.
    lastDownloadTime = datetime.now(pytz.utc) # The datetime of the last time any ScpWorker downloaded a set of SUs.
    
    def __init__(self):
        threading.Thread.__init__(self, suTable, scpUser, scpHost, scpPort, binPath, arguments, log)
        self.suTable = suTable
        self.scpUser = scpUser
        self.scpHost = scpHost
        self.scpPort = scpPort
        self.binPath = binPath # scp bin path
        self.tmpdir = arguments.tmpdir
        self.arguments = arguments
        self.log = log
        self.sdEvent = threading.Event()
        
    def run(self):
        gotTableLock = False
        gotSULock = False
        maxNumSUs = self.arguments.scpMaxNumSUs
        maxPayloadSUs = self.arguments.scpMaxPayload
        scpTimeOut = self.arguments.scpTimeOut
        doDownload = False
        
        while not self.sdEvent.isSet():
            sunums = []
            paths = []

            try:
                gotTableLock = self.suTable.acquireLock()
                
                if not gotTableLock:
                    raise Exception('lock', 'Unable to acquire SU-table lock.')
                                
                # Look for an SU whose status is 'W' (which means that a Downloader thread is requesting an ScpWorker thread perform a 
                # download for it). If we find one, set status to 'D' and download the SU. When the download is complete, set the status
                # to 'P' so the requesting Downloader thread can clean-up and set the status to 'C' for the main thread to handle.
                workingSUs = self.suTable.getNextNWorking(maxNumSUs)
                
                numSUs = len(workingSUs)
                payload = sum([ su.suSize for su in workingSUs ])
                currentTime = datetime.now(pytz.utc)
                
                doDownload = payload > maxPayloadSUs or numSUs == maxNumSUs or currentTime - lastDownloadTime > scpTimeOut
                
                if doDownload:
                    # Now acquire SU lock for all SUs so no other thread modifies the SU while we are processing the download.
                    for su in workingSUs:
                        gotSULock = su.acquireLock()
                        if not gotSULock:
                            raise Exception('lock', 'Unable to acquire SU ' + str(su.sunum) + ' lock.')
                        sunums.append(su.sunum)
            finally:
                # Always release lock.
                if gotTableLock:
                    self.suTable.releaseLock()

            if doDownload:
                try:
                    self.log.write([ 'Downloading SUs ' + ','.join([ str(asunum) for asunum in sunums ]) + '.' ])
                    
                    for su in workingSUs:
                        # Set status to D to prevent another ScpWorker from processing the download.
                        su.setStatus('D', None)
                    
                        # Make sure path is set.
                        serverPath = su.getPath()
                        if not serverPath:
                            raise Exception('scpSU', 'Server SU path is not known.')
                        paths.append(serverPath)
                finally:
                    # Always release the SU locks. We know we have all SU locks, otherwise we would have thrown an exception earlier.
                    for su in workingSus:
                        su.releaseLock()

                # Don't forget to make the temporary directory first.
                if not os.path.exists(self.tmpdir):
                    self.log.write(['Creating temporary download directory ' + self.tmpdir + '.'])
                    os.mkdir(self.tmpdir)

                cmdList = ['scp', '-r', '-P', self.scpPort, self.scpUser + '@' + self.scpHost + ':"' + ' '.join(paths) + '"', self.tmpdir]
                self.log.write([ 'Running ' + ' '.join(cmdList) + '.' ])
                try:
                    # check_call(cmdList)
                    # The scp process will inherit stdin, stdout, and stderr from this script.
                    proc = Popen(cmdList, stdout=PIPE, stderr=PIPE)
                except OSError as exc:
                    raise Exception('scpSU', "Cannot run command '" + ' '.join(cmdList) + "' ")
                except ValueError as exc:
                    raise Exception('scpSU', "scp command '" + ' '.join(cmdList) + "' called with invalid arguments.")

                # Poll for completion
                while True:                    
                    try:
                        # If getAndLockSUs() raises, then sus is None.
                        atLeastOneGoodSU = False
                        sus = self.suTable.getAndLockSUs(sunums)
                        for su in sus:
                            if su.status == 'S':
                                # Continue the download for the remaining SUs whose downloads are not being shutdown.
                                pass
                            elif su.status == 'D':                                
                                # Check for SU download time-out. We keep doing the download, unless all SUs have timed out.
                                timeNow = datetime.now(su.starttime.tzinfo)
                                if timeNow > su.starttime + self.suTable.getTimeout():
                                    self.log.write(['Download of SUNUM ' + str(su.sunum) + ' timed-out.'])
                                    su.setStatus('E', 'Download timed-out.')
                                else:
                                    atLeastOneGoodSU = True

                        if self.sdEvent.isSet():
                            # Kill the download and also exit the ScpWorker thread.
                            proc.kill()
                            self.log.write([ 'ScpWorker thread is observing the global shutdown and exiting now.' ])
                            break
                            
                        if not atLeastOneGoodSU:
                            # Go on to the next set of requested SUs. Don't exit the ScpWorker thread.
                            break
                    finally:
                        # Always release SU locks. 
                        if sus:
                            for su in sus:
                                su.releaseLock()

                    # The Python documentation is confusing at best. I think we have to look at the proc.returncode attribute
                    # to determine if the child process has completed. None means it hasn't. If the value is not None, then 
                    # the child process has terminated, and the value is the child process's return code.
                    proc.poll()
                    if proc.returncode is not None:
                        if proc.returncode != 0:
                            out, err = proc.communicate()
                            msg = 'Command "' + ' '.join(cmdList) + '" returned non-zero status code ' + str(proc.returncode) + '.'
                            if err is not None:
                                self.log.write([ 'scp stderr msg: ' + err.decode('UTF8') ])
                            raise Exception('scpSU', msg) 
                        break

                    time.sleep(1)

                if atLeastOneGoodSU and not self.sdEvent.isSet():
                    try:
                        sus = self.suTable.getAndLockSUs(sunums)
                        for su in sus:
                            if su.status == 'D':
                                # The download for this SU has not been canceled.
                                su.setStatus('P', None)
                    finally:
                        # Always release lock.
                        if sus:
                            for su in sus:
                                su.releaseLock()

                    try:
                        gotTableLock = self.suTable.acquireLock()

                        if not gotTableLock:
                            raise Exception('lock', 'Unable to acquire SU-table lock.')
                        
                        # Flush the change to disk.
                        self.suTable.updateDB()
                    finally:
                        # Always release lock.
                        if gotTableLock:
                            self.suTable.releaseLock()

                    self.log.write(['scp command succeeded for SU ' + str(su.sunum) + '.'])
                
                # Move on to the next scp without sleeping
                
                # Wake up a Downloader.
                ScpWorker.scpCompleted.set()
                # Clear event so that main will block the next time it calls wait.
                ScpWorker.scpCompleted.clear()                
            else:
                # There were fewer than scpBatchSize requests for an SU download. Wait for more with ScpWorker.scpNeeded.wait().
                # Set a timeout so we can gracefully exit if the shutdown event has been triggered (just in case this thread
                # blocks on wait - when the shutdown happens, the scpNeeded event will be triggered, however).
                try:
                    ScpWorker.scpNeeded.wait(10)
                except RuntimeError:
                    pass
                
    def stop(self):
        self.log.write([ 'Stopping ScpWorker.' ])
        
        sus = None
        try:
            sus = self.suTable.getAndLockSUs(sunums)
            for su in sus:
                su.setStatus('S', 'Download interrupted during shutdown.')
                 self.log.write(['Setting SU ' + str(su.sunum) + ' status to stopped.'])
        finally:
            # Always release lock.
            if sus:
                for su in sus:
                    su.releaseLock()
        
        # Fire event to stop thread.
        self.sdEvent.set()
        
        # Fire scpNeeded event so that the ScpWorker thread will not block on wait.
        ScpWorker.scpNeeded.set()
    
    def newThread(suTable, scpUser, scpHost, scpPort, binPath, arguments, log):
        worker = ScpWorker(suTable, scpUser, scpHost, scpPort, binPath, arguments, log)
        ScpWorker.tList.append(worker)
        worker.start()

# Downloads a single SU. Ingests it into SUMs (SUMS allows to ingestion of a single SU at a time only.). Updates
# the SU table status for that SU.
# The main thread sets the SU status to 'P'. The Downloader thread sets that status to 'W' to request a ScpWorker
# thread to download the SU. The ScpWorker thread sets the status to 'D' while it is processing the download. When
# the download is complete, the ScpWorker thread sets the status back to 'P'.
class Downloader(threading.Thread):
    tList = [] # A list of running thread IDs.
    maxThreads = 16 # Default. Can be overriden with the Downloader.setMaxThreads() method.
    eventMaxThreads = threading.Event() # Event fired when the number of threads decreases.
    lock = threading.Lock() # Guard tList.

    def __init__(self, sunum, path, series, suSize, retention, sus, scpUser, scpHost, scpPort, binPath, arguments, log):
        threading.Thread.__init__(self)
        self.sunum = sunum
        self.path = path
        self.series = series
        self.suSize = suSize
        self.retention = retention
        self.suTable = sus
        self.scpUser = scpUser
        self.scpHost = scpHost
        self.scpPort = scpPort
        self.binPath = binPath
        self.tmpdir = arguments.tmpdir
        self.arguments = arguments
        self.log = log
        self.sdEvent = threading.Event()

    def run(self):
        # Sub-out the download to an ScpWorker instance. To do that, se the SU status to 'W'. The ScpWorker
        # that is used will set the status to 'D' so that no other ScpWorker attempt to download the SU 
        # as well. When the ScpWorker completes the download, it will set the status to 'P' again.
        try:
            su = None
            try:
                su = self.suTable.getAndLockSU(self.sunum)

                if su.status != 'P':
                    raise Exception('downloader', 'SU ' + str(su.sunum) + ' not pending.')
                    
                # Let an ScpWorker thread handle the download. We do that by setting the status to 'W'.
                self.log.write(['Setting SU ' + str(self.sunum) + ' status to start scp.'])
                su.setStatus('W', None)
                
                # Tell ScpWorker the source path.
                su.setPath(self.path)
                
                # Tell ScpWorker the SU size.
                su.setSUSize(self.suSize)
                
                suDlPath = os.path.join(self.tmpdir, 'D', str(su.sunum))
            finally:
                # Always release lock.
                if su:
                    su.releaseLock()

            # Wake up an ScpWorker.
            ScpWorker.scpNeeded.set()
            # Clear event so that main will block the next time it calls wait.
            ScpWorker.scpNeeded.clear()

            # Wait for the ScpWorker thread to finish downloading the SU (look for a 'P' status).
            while not self.sdEvent.isSet():
                su = None
                try:
                    su = self.suTable.getAndLockSU(self.sunum)

                    # Check for download error or completion. Call get() again, since the SU record could have been deleted (due to
                    # abnormal execution).
                    if su.status == 'W' or su.status == 'D':
                        # ScpDownloader is still performing the download. Don't do anything
                        pass
                    elif su.status == 'P':
                        # ScpDownloader is done performing the download.
                        break
                    else:
                        raise Exception('downloader', 'The download of SU ' + str(su.sunum) + ' errored-out.')
                finally:
                    # Always release lock.
                    if su:
                        su.releaseLock()

                # Wait for ANY ScpWorker to complete a download. The downloaded SU might not be the one needed by
                # this Downloader thread. If a shutdown is happening, then all ScpWorker threads should fire the 
                # scpCompleted event, releasing all blocking Downloader threads. But just in case, set a 10-second 
                # timeout - this raises if the timeout occurs.
                try:
                    ScpWorker.scpCompleted.wait(10)
                except RuntimeError:
                    pass
            
            # The download has completed.

            # At this point, we need to allocate (mkdir) a new SU directory, move the downloaded SU content into this SUDIR, 
            # then commit the newly created SU into SUMS. The previous incarnations of remote-sums-type code all used the SUMS
            # API to achieve the first and last steps. To use the SUMS API, code need to be written in C and it needs to link
            # to the SUMS library. The first remote-sums code did this by running a DRMS module, wherein all three steps were
            # performed. The JMD uses vso_sum_alloc (a DRMS module with access to the SUMS API) to allocate the SU directory, 
            # then it copies the downloaded SU content into the directory, and then it calls vso_sum_put to commit the SU.
            # However, at a high rate of download, the SUMS API seems to have problems, resulting in the vso_sum_alloc and/or
            # vso_sum_put calls to hang for minutes. We have not been able to track down this issue, but it appears to have
            # something to do with either saturation of socket resources and/or RPC resources and/or SUMS queues.
            
            # This script by-passes SUMS altogether to avoid the issues with SUMS and/or DRMS modules hanging under higher load.
            # The first step in by-passing SUMS is to perform the equivalent of the SUM_open() API call. Then we can call the
            # equivalent of the SUM_alloc2() call to allocate a new SU directory, followed by the copying of the download SU
            # content into this new SU directory. Then we can call the equivalent of the SUM_put() API call to commit the 
            # SU, and then we can call the equivalent of the SUM_close() API call to end the SUMS session.
            
            # We need to connect to the SUMS database before we can modify SUMS objects.
            with psycopg2.connect(database=arguments.sumsdbname, user=arguments.sumsdbuser, host=arguments.sumsdbhost, port=arguments.sumsdbport) as conn:
                rslog.write(['Connected to database ' + arguments.sumsdbname + ' on ' + arguments.sumsdbhost + ':' + str(arguments.sumsdbport) + ' as user ' + arguments.sumsdbuser])
                with conn.cursor() as cursor:
                    try: 
                        # Put all of this in one transaction. If everything is good, commit the transaction. If an 
                        # exception occurs, roll back.
                        
                        ##### SUM_open() port #####
                        # This increments the sequence that supplies the sumid and inserts that sumid into the sum_open table.
                        cmd = "SELECT NEXTVAL('public.sum_seq')"
                        cursor.execute(cmd)
                        records = cursor.fetchall()
                        if len(records) != 1:
                            raise Exception('sumsAPI', 'Unexpected response when fetching sumid from sequence.')
                            
                        sumid = records[0][0]
    
                        currentTimeStr = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        cmd = 'INSERT INTO public.sum_open(sumid, open_date) VALUES (' + str(sumid) + ", '" + currentTimeStr + "')"
                        cursor.execute(cmd)
                        ##### SUM_open() port - end #####
                        
                        self.log.write([ 'Successfully called SUM_open() port for SU ' +  str(self.sunum) + '.'])
                        self.log.write([ 'sumid is ' + str(sumid) + '.' ])
                
                        ##### SUM_alloc2() port #####
                        #   First, find a partition that has enough available space for the size of the SU to be downloaded.
                        cmd = 'SELECT PARTN_NAME FROM public.sum_partn_avail WHERE AVAIL_BYTES >= 1024 AND PDS_SET_NUM = 0'
                        cursor.execute(cmd)
                        records = cursor.fetchall()
                        if len(records) < 1:
                            raise Exception('sumsAPI', 'Cannot allocate a new Storage Unit in SUMS - out of space.')
                            
                        partitions = []
                        for rec in records:
                            partitions.append(rec[0])
                            
                        #   Second, randomly choose one of the partitions to put the new SU into. We want to spread the write load over available 
                        #   partitions.
                        randIndex = random.randint(0, len(partitions) - 1)
                        partition = partitions[randIndex]
                        sudir = os.path.join(partition, 'D' + str(self.sunum))
                        os.mkdir(sudir)
                        os.chmod(sudir, 0O2755)
            
                        #   Third insert a record into the sum_partn_alloc table for this SU. status is DARW, which is 1. effective_date is "0".
                        cmd = "INSERT INTO public.sum_partn_alloc(wd, sumid, status, bytes, effective_date, archive_substatus, group_id, safe_id, ds_index) VALUES ('" + sudir + "', '" + str(sumid) + "', 1, 1024, '0', 0, 0, 0, 0)"
                        cursor.execute(cmd)
                        ##### SUM_alloc2() port - end #####

                        self.log.write(['Succeeded allocating a new SU: ' +  str(self.sunum) + '.'])
                        
                        files = os.listdir(suDlPath)
                        self.log.write(['Moving downloaded SU content from ' + suDlPath + ' into allocated SU (' + sudir  + ').'])

                        try:
                            for afile in files:
                                src = os.path.join(suDlPath, afile)
                                shutil.move(src, sudir)
                        except shutil.Error as exc: 
                            import traceback
                            self.log.write([ traceback.format_exc(5) ])
                            raise Exception('mvSU', 'Unable to move SU file ' + afile + ' into SUdir ' + sudir + '.')

                        self.log.write(['Move of SU ' + str(self.sunum) + ' content succeeded.'])
                        
                        ##### SUM_put() port #####
                        # The original SUM_put() call called "chmown" to change the ownership of the
                        # files in the SU dir to the SUM_MANAGER. However, this is not necessary since rsumsd.py is run by the 
                        # SUM_MANAGER.
                        
                        #   First, chmod all directories to 0755. All regular files get their user/group/other read enabled, and their
                        #   user write enabled, and their group and other write disabled.
                        for root, dirs, files in os.walk(sudir):
                            for adir in dirs:
                                fullPath = os.path.join(root, adir)
                                os.chmod(fullPath, 0O0755)
                            for afile in files:
                                fullPath = os.path.join(root, afile)
                                st = os.stat(fullPath)
                                newMod = st.st_mode | stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH | stat.S_IWUSR & ~stat.S_IWGRP & ~stat.S_IWOTH
                                os.chmod(fullPath, newMod)
                                
                        #   Second, update SUMS sum_main database table - Calculate SU dir number of bytes, set online status to 'Y', set archstatus to 'N', 
                        #   set offsiteack to 'N', set dsname to seriesname, set storagegroup to tapegroup, set storageset to tapegroup / 10000,
                        #   set username to getenv('USER') or nouser if no USER env, set mode to TEMP + TOUCH, set apstatus: if SUMS_TAPE_AVAILABLE ==>
                        #   DAAP (4), else DADP (2), set archsub ==> DAAEDDP (32), set effective_date to tdays in the future (with format "%04d%02d%02d%02d%02d").
                        #   Insert all of this into sum_main.
                        numBytes = os.path.getsize(sudir) + sum([ os.path.getsize(fullPath) for fullPath in [ os.path.join(root, afile) for root, dirs, files in os.walk(sudir) for afile in files ] ]) + sum([ os.path.getsize(fullPath) for fullPath in [ os.path.join(root, adir) for root, dirs, files in os.walk(sudir) for adir in dirs ] ])
                        if self.arguments.tapesysexists:
                            apStatus = 4 # DAAP
                        else:
                            apStatus = 2 # DADP
            
                        createDate = datetime.now()
                        createDateStr = createDate.strftime('%Y-%m-%d %H:%M:%S')
                        expDate = createDate + timedelta(days=self.retention)
                        effDate = expDate.strftime('%Y%m%d%H%M')

                        # storage_group is the tape group. It should come from the series definition, but remote sites have been using 0 for years.            
                        cmd = "INSERT INTO public.sum_main(online_loc, online_status, archive_status, offsite_ack, history_comment, owning_series, storage_group, storage_set, bytes, ds_index, create_sumid, creat_date, access_date, username) VALUES ('" + sudir + "', 'Y', 'N', 'N', '', '" + self.series + "', 0, 0, " + str(numBytes) + ', ' + str(self.sunum) + ', ' + str(sumid) + ", '" + createDateStr + "', '" + createDateStr + "', '" + os.getenv('USER', 'nouser') + "')"
                        cursor.execute(cmd)
                        
                        self.log.write([ 'Successfully inserted record into sum_main for SU ' + str(self.sunum) + '.' ])
            
                        #    Third, update SUMS sum_partn_alloc table - Insert a new row into sum_partn_alloc for this SU. The SUM_alloc2() port will result in
                        #    a row in sum_partn_alloc with a ds_index of 0, which does not make sense to me. But the SUM_close() port will delete
                        #    that row. By the time this thread terminates, there will be only a single row for this SU in sum_partn_alloc. substatus is DAAEDDP (32).
                        #    But first, delete any existing DADP (delete pending) rows for this sunum if the status of the SU for the new row is DADP.
                        if apStatus == 2:
                            cmd = 'DELETE FROM public.sum_partn_alloc WHERE ds_index = ' + str(self.sunum) + ' AND STATUS = 2'
                            cursor.execute(cmd)
                            self.log.write([ 'Successfully deleted old DADP record from sum_partn_alloc for SU ' + str(self.sunum) + '.' ])
                        
                        cmd = "INSERT INTO public.sum_partn_alloc(wd, sumid, status, bytes, effective_date, archive_substatus, group_id, safe_id, ds_index) VALUES ('" + sudir + "', " + str(sumid) + ', ' + str(apStatus) + ', ' + str(numBytes) + ", '" + effDate + "', 32, 0, 0, " + str(self.sunum) + ')'
                        cursor.execute(cmd)
                        self.log.write([ 'Successfully inserted record into sum_partn_alloc for SU ' + str(self.sunum) + '.' ])
                        ##### SUM_put() port - end #####
                        
                        self.log.write(['Commit of SU ' + str(self.sunum) + ' succeeded.'])
                        
                        ##### SUM_close() port #####
                        # Delete sum_partn_alloc records for read-only partitions (status == 8) and read-write partitions (status == 1).
                        cmd = 'DELETE FROM public.sum_partn_alloc WHERE sumid = ' + str(sumid) + ' AND (status = 8 OR status = 1)'
                        cursor.execute(cmd)
                        self.log.write([ 'Successfully deleted read-only and read-write records from sum_partn_alloc for SU ' + str(self.sunum) + '.' ])
                        
                        # Delete the temporary ds_index = 0 records created during the SUM_put() port. I still do not know why this record
                        # was created in the first place.
                        cmd = 'DELETE FROM public.sum_open WHERE sumid = ' + str(sumid)
                        cursor.execute(cmd)
                        self.log.write([ 'Successfully deleted temporary (ds_index == 0) records from sum_open for SU ' + str(self.sunum) + '.' ])
                        ##### SUM_close() port - end #####
                    except psycopg2.Error as exc:
                        # Handle database-command errors. These are all due to problems communicating with the SUMS db.
                        conn.rollback()
                        
                        # Clean-up
                        if os.path.exists(sudir):
                            shutil.rmtree(sudir)
                        if os.path.exists(dlSuPath):
                            shutil.rmtree(dlSuPath)
                        raise Exception('sumsAPI', exc.diag.message_primary + ': ' + cmd + '.') 
                    except Exception as exc:
                        conn.rollback()
                        
                        # Clean-up
                        if os.path.exists(sudir):
                            shutil.rmtree(sudir)
                        if os.path.exists(dlSuPath):
                            shutil.rmtree(dlSuPath)
                        raise

                    conn.commit()

            # Remove temporary directory.
            self.log.write(['Removing temporary download directory ' + dlSuPath + '.'])
            try:
                if os.path.exists(dlSuPath):
                    shutil.rmtree(dlSuPath)
            except OSError as exc:
                raise Exception('rmTmpSU', exc.strerror)
           
            self.log.write(['Removal of temporary directory ' + dlSuPath + ' succeeded.'])
 
            # Update SU table. Set SU-table record status to 'C'. Must first lock the SU table since we are modifying it. Also,
            # the state may not be 'P' due to some problem cropping up in the meantime. Only set to 'C' if the state is 'P'.
            try:
                su = None
                su = self.suTable.getAndLockSU(self.sunum)
                    
                if su.status == 'P':
                    self.log.write(['Setting SU ' + str(self.sunum) + ' status to complete.'])
                    su.setStatus('C', None)
            finally:
                # Always release lock.
                if su:
                    su.releaseLock()
 
        except Exception as exc:
            if len(exc.args) == 2:
                type = exc[0]
                msg = exc[1]

                if type == 'scpSU' or type == 'sumsAPI' or type == 'mvSU' or type == 'rmTmpSU':
                    try:
                        gotSULock = False
                        su = self.suTable.getAndLockSU(self.sunum)
                        gotSULock = True

                        self.log.write(['Setting SU ' + str(self.sunum) + ' status to error.'])
                        su.setStatus('E', 'Error downloading storage unit ' + str(self.sunum) + ': ' + msg)
                    except Exception:
                        # Catch everything and just let the thread pass away peacefully.
                        pass
                    finally:
                        if gotSULock:
                            su.releaseLock()
                    
                elif type == 'unknownSunum':
                    self.log.write(['Cannot download SU. No SU record.' + msg])
                elif type == 'downloader':
                    self.log.write([msg])
                else:
                    import traceback
                    self.log.write([ traceback.format_exc(5) ])
            else:
                import traceback
                self.log.write([ traceback.format_exc(5) ])
                
        # Update SU table (write-out status, error or success, to the DB).
        try:
            gotTableLock = self.suTable.acquireLock()

            if not gotTableLock:
                raise Exception('lock', 'Unable to acquire SU-table lock.')

            # This is a no-op if no SUs were actually modified.
            self.suTable.updateDB()
        finally:
            # Always release lock.
            if gotTableLock:
                self.suTable.releaseLock()

        # This thread is about to terminate. 
        # We need to check the class tList variable to update it, so we need to acquire the lock.
        try:
            Downloader.lock.acquire()
            self.log.write(['Class Downloader acquired Downloader lock for SU ' + str(self.sunum) + '.'])
            Downloader.tList.remove(self) # This thread is no longer one of the running threads.
            if len(Downloader.tList) == Downloader.maxThreads - 1:
                # Fire event so that main thread can add new SUs to the download queue.
                self.log.write(['OK to start new download threads.'])
                Downloader.eventMaxThreads.set()
                # Clear event so that main will block the next time it calls wait.
                Downloader.eventMaxThreads.clear()
        finally:
            Downloader.lock.release()
            self.log.write(['Class Downloader released Downloader lock for SU ' + str(self.sunum) + '.'])


    def stop(self):
        self.log.write([ 'Stopping Downloader (SUNUM ' + str(self.sunum) + ').' ])
        
        su = None
        try:
            su = self.suTable.getAndLockSU(self.sunum)                
            self.log.write(['Setting SU ' + str(self.sunum) + ' status to stopped.'])
            su.setStatus('S', None)
        finally:
            # Always release lock.
            if su:
                su.releaseLock()
        
        # Fire event to stop thread.
        self.sdEvent.set()

    # Must acquire Downloader lock BEFORE calling newThread() since newThread() will append to tList (the Downloader threads will delete from tList as they complete).
    @staticmethod
    def newThread(sunum, path, series, suSize, retention, sus, scpUser, scpHost, scpPort, binPath, arguments, log):
        dl = Downloader(sunum, path, series, suSize, retention, sus, scpUser, scpHost, scpPort, binPath, arguments, log)
        sus.setWorker(sunum, dl)
        dl.tList.append(dl)
        dl.start()

    @classmethod
    def setMaxThreads(cls, maxThreads):
        cls.maxThreads = maxThreads

class ProviderPoller(threading.Thread):
    def __init__(self, url, requestID, sunums, sus, reqTable, request, dbUser, binPath, arguments, log):
        threading.Thread.__init__(self)
        self.url = url
        self.requestID = requestID # The provider request ID.
        self.sunums = sunums # The sunums requested from provider (under request ID self.requestID).
        self.suTable = sus
        self.reqTable = reqTable
        self.request = request # The ReqTable::reqdict[requestidStr] object (the row in the request table)
        self.dbUser = dbUser
        self.binPath = binPath
        self.arguments = arguments
        self.log = log
        self.startTime = datetime.now() # Cool bug. This used ot be self.start. But the parent object has a method named 'start' The effect was to override the method with an attribute.
        self.timeOut = sus.getTimeout()

    def run(self):
        values = {'requestid' : self.requestID, 'sunums' : 'none'}
        data = urllib.urlencode(values)
        errMsg = None
        timeToLog = True
        loopN = 0

        gotLock = False
        try:
            # Set the in-memory ProviderPoller flag for all sunums in this request.
            gotLock = self.suTable.acquireLock()
            for asunum in self.sunums:
                try:
                    asu = self.suTable.get([ asunum ])[0]
                    asu.setPolling(True)
                    self.suTable.updateDB([ asunum ])
                except Exception as exc:
                    if len(exc.args) != 2:
                        raise # Re-raise

                    etype = exc.args[0]

                    if etype != 'unknownSunum':
                        raise
        finally:
            if gotLock:
                self.suTable.releaseLock()

        dlInfo = {}
        dlInfo['status'] = 'pending'
        while True:
            if datetime.now(self.startTime.tzinfo) > self.startTime + self.timeOut:
                # The providing site has not completed the export, and the time-out has elapsed. Give up.
                errMsg = 'Timed-out waiting for providing site to return paths to requested SUs (' + ','.join([ str(asunum) for asunum in self.sunums ]) + ') - provider request ' + self.requestID + '.'
                break

            if timeToLog:
                self.log.write(['Checking on request to provider (provider request ' + self.requestID + ').'])
                self.log.write(['URL is ' + self.url + '/rs.sh' + '?' + data])

            req = urllib2.Request(self.url + '/rs.sh', data)
            response = urllib2.urlopen(req)
            dlInfoStr = response.read()
            dlInfo = json.loads(dlInfoStr)

            if timeToLog:
                self.log.write(['Provider returns status ' + dlInfo['status'] + '.'])

            if dlInfo['status'] != 'pending':
                break;

            time.sleep(1)
            loopN += 1
            
            # Log every 5 seconds.
            timeToLog = (loopN % 5 == 0)

        # We might not have printed to log.
        if not errMsg and not timeToLog:
            self.log.write(['Checking on request to provider (provider request ' + self.requestID + ').'])
            self.log.write(['URL is ' + self.url + '/rs.sh' + '?' + data])
            self.log.write(['Provider returns status ' + dlInfo['status'] + '.'])

        # We must acquire the SU-table lock since we will be updating the status fields for individual SUs.
        gotLock = False
        try:
            gotLock = self.suTable.acquireLock()

            # We are done polling, remove the polling flag.
            for asunum in self.sunums:
                try:
                    asu = self.suTable.get([ asunum ])[0]
                    asu.setPolling(False)
                    self.suTable.updateDB([ asunum ])
                except Exception as exc:
                    if len(exc.args) != 2:
                        raise # Re-raise

                    etype = exc.args[0]

                    if etype != 'unknownSunum':
                        raise
 
            if dlInfo['status'] != 'complete':
                # Set all SU records to 'E' (rsumds.py timed-out waiting for the SUs to be ready at the providing site).
                if not errMsg:
                    errMsg = 'The providing site failed to return paths to requests SUs.'
                self.suTable.setStatus(self.sunums, 'E', errMsg)

                # Flush the change to disk.
                self.suTable.updateDB(self.sunums)
            else:
                # Start a download for each SU. If we cannot start the download for any reason, then set the SU status to 'E'.
                self.log.write(['The SU paths are ready.'])
                paths = dlInfo['paths']

                retentions = {}
                for (asunum, path, series, suSize) in paths:
                    if not path:
                            # A path of None means that the SUNUM was invalid. We want to set the SU status to 'E'.
                        self.suTable.setStatus([ asunum ], 'E', 'SU ' + str(asunum) + ' is not valid at the providing site.')
                        continue
                    elif path == '':
                        # An empty-string path means that the SUNUM was valid, but that the SU referred to was offline, and could not
                        # be placed back online - it is not archived.
                        # ART - I need to figure out how to place the SUNUM in SUMS so that its archive flag is N (not archived).
                        self.suTable.setStatus([ asunum ], 'C', 'SU ' + str(asunum) + ' refers to an offline SU valid at the providing site that was not archived. It cannot be downloaded.')
                        continue
                        
                    if suSize is None:
                        suSize = 0
                    
                    if series in retentions:
                        retention = retentions[series]
                    else:
                        # request provides the host, port, and dbname to use with jsoc_info to fetch the retention value.
                        retention = ReqTable.getRetention(series, self.request, self.dbUser, self.binPath, self.log)
                        retentions[series] = retention
                        
                        # Save series and retention.
                        sus.setSeries([ asunum ], series)
                        sus.setRetention([ asunum ], retention)

                    while True:
                        Downloader.lock.acquire()
                        try:
                            if len(Downloader.tList) < Downloader.maxThreads:
                                self.log.write(['Instantiating a Downloader for SU ' + str(asunum) + '.'])
                                Downloader.newThread(asunum, path, series, suSize, retention, self.suTable, dlInfo['scpUser'], dlInfo['scpHost'], dlInfo['scpPort'], self.binPath, self.arguments, self.log)
                                break # The finally clause will ensure the Downloader lock is released.
                        finally:
                            Downloader.lock.release()

                        Downloader.eventMaxThreads.wait()
                        # We woke up, but we do not know if there are any open threads in the thread pool. Loop and check
                        # tList again.
                
                # Flush the change to disk.
                self.suTable.updateDB(self.sunums)
        except Exception as exc:
            if len(exc.args) != 2:
                raise

            type = exc[0]

            # Eventually I will figure out which exceptions to handle.
            raise
        finally:
            # Always release lock.
            if gotLock:
                self.suTable.releaseLock()

    @staticmethod
    def newThread(url, requestID, sunums, sus, reqTable, request, dbUser, binPath, arguments, log):
        poller = ProviderPoller(url, requestID, sunums, sus, reqTable, request, dbUser, binPath, arguments, log)
        poller.start()

def readTables(sus, requests, sites):
    if sus:
        sus.tryRead()

    if requests:
        requests.tryRead()
    
    if sites:
        sites.tryRead()

# Process the SUs for the source site represented by url.
# url - the base URL to the rs.sh cgi (e.g., http://jsoc.stanford.edu/cgi-bin) from which SU paths can be obtained.
# sunums - a list of sorted SUNUMs to download.
# sus - the SU table object that represents the SU database table.
# request - ReqTable::dict[requestidStr] object.
# binPath - the local path to the binaries needed to ingest the downloaded SU into SUMS. This is mostly likely the path to
#           the DRMS binaries (one binary needed is vso_sum_alloc)
# log - the log to write messages to.
# reprocess - the SUs identified are all being reprocessed. They all have a status of 'P' in the SU table. There was
#             some interruption that caused the download to be lost.
# reset - reset the processing start time for each SU. Ignored, unless reprocess is true
def processSUs(url, sunums, sus, reqTable, request, dbUser, binPath, arguments, log, reprocess=False, reset=False):
    # Get path to SUs by calling the rs.sh cgi at the owning remote site (url identifies the remote site).
    # Create the sunum= argument.

    # Skip any SUs that have already been processed. Pending SUs are OK to restart, however. It may be the case
    # that rsumds.py was interrupted during a download, in which case, we want to re-download the SU.
    workingSunums = []
    for asunum in sunums:
        try:
            su = sus.get([ asunum ])[0]
            if su.status == 'P':
                if not reprocess:
                    # Accidental attempt to reprocess an SU whose processing has already started.
                    raise Exception('accidentalRepro', 'An accidental attempt to reprocess pending SU ' + str(asunum) + ' occurred.')
                workingSunums.append(asunum)
                # Reset start time.
                if reset:
                    sus.setStarttime([ asunum ], datetime.now())
        except Exception as exc:
            if len(exc.args) != 2:
                raise # Re-raise

            etype = exc.args[0]

            if etype != 'unknownSunum':
                raise

            workingSunums.append(asunum)
            # Create a new SU table record for this SU.
            log.write(['Inserting a new SU table record for ' + str(asunum)])
            sus.insert([ asunum ])

    sunumLst = ','.join(str(asunum) for asunum in workingSunums)
    values = {'requestid' : 'none', 'sunums' : sunumLst}
    data = urllib.urlencode(values)
    log.write(['Requesting paths for SUNUMs ' + sunumLst + '. URL is ' + url + '/rs.sh' + '?' + data])
    req = urllib2.Request(url + '/rs.sh', data)
    response = urllib2.urlopen(req)
    dlInfoStr = response.read()
    dlInfo = json.loads(dlInfoStr)

    if dlInfo['status'] == 'complete':
        # All of the requested SUs are online at the providing site.
        paths = dlInfo['paths']

        # Start a download for each SU. If we cannot start the download for any reason, then set the SU status to 'E'.
        retentions = {}
        for (asunum, path, series, suSize) in paths:
            if not path:
                # A path of None means that the SUNUM was invalid. We want to set the SU status to 'E'.
                sus.setStatus([ asunum ], 'E', 'SU ' + str(asunum) + ' is not valid at the providing site.')
                continue
            elif path == '':
                # An empty-string path means that the SUNUM was valid, but that the SU referred to was offline, and could not
                # be placed back online - it is not archived. 
                # ART - I need to figure out how to place the SUNUM in SUMS so that its archive flag is N (not archived).
                sus.setStatus([ asunum ], 'C', 'SU ' + str(asunum) + ' refers to an offline SU valid at the providing site that was not archived. It cannot be downloaded.')
                continue
                
            if suSize is None:
                suSize = 0

            # Get retention, if it hasn't been gotten yet. If we are re-processing SUs, then the retention has already been determined and saved.
            if reprocess:
                # We know this won't raise, because we already obtained the su object at the beginning of this method.
                su = sus.get([ asunum ])[0]
                retention = su.retention
            else:
                if series in retentions:
                    retention = retentions[series]
                else:
                    # request provides the host, port, and dbname to use with jsoc_info to fetch the retention value.
                    retention = ReqTable.getRetention(series, request, dbUser, binPath, log)
                    retentions[series] = retention
                    
                # Save series and retention.
                sus.setSeries([ asunum ], series)
                sus.setRetention([ asunum ], retention)

            while True:
                Downloader.lock.acquire()
                try:
                    if len(Downloader.tList) < Downloader.maxThreads:
                        log.write(['Instantiating a Downloader for SU ' + str(asunum) + '.'])
                        Downloader.newThread(asunum, path, series, suSize, retention, sus, dlInfo['scpUser'], dlInfo['scpHost'], dlInfo['scpPort'], binPath, arguments, log)
                        break # The finally clause will ensure the Downloader lock is released.
                finally:
                    Downloader.lock.release()

                # We are holding the suTable lock. The new downloader threads have to acquire this lock because they change the
                # status of the suTable rows. If we do not releaes the lock, then we could deadlock here.
                sus.releaseLock()

                Downloader.eventMaxThreads.wait()

                sus.acquireLock()
                # We woke up, but we do not know if there are any open threads in the thread pool. Loop and check
                # tList again.

        # For each SU that was requested, but for which no path was given in the response, update its SU-table record with an error status.
        pathInResp = dict([ (str(asunum), True) for (asunum, path, series, suSize) in paths ])
        for asunum in workingSunums:
            if str(asunum) not in pathInResp:
                sus.setStatus([asunum], 'E', 'Providing site cannot provide a path for SU ' + str(asunum) + '.')
    elif dlInfo['status'] == 'pending':
        # One or more of the requested SUs is offline. Poll until they are ready. Ideally we wouldn't block the main
        # thread here, waiting for the SUs to be available. We could spawn a thread to poll, freeing up the main
        # thread to continue with other requests. But in the interest of time, just poll for now. Must acquire
        # su-table lock when it is finally time to start downloads.
        log.write(['Request includes one or more SUs that are offline at the providing site. Waiting for providing site to put them online.'])

        ProviderPoller.newThread(url, dlInfo['requestid'], workingSunums, sus, reqTable, request, dbUser, binPath, arguments, log)
    else:
        # Error of some kind.
        # Update the SU-table status of the SUs to 'E'.
        sus.setStatus(workingSunums, 'E', 'Unable to obtain paths from providing site.\n' + dlInfo['statusMsg'] + '.')

# Global - set True when main thread calls sys.exit(). Threads must check this variable if they can loop indefinitely.
# There is no need to use a lock when accessing this variable - only the main thread will write to it.
gShutDown = False

rv = RET_SUCCESS

if __name__ == "__main__":
    try:
        sumsDrmsParams = SumsDrmsParams()
        if sumsDrmsParams is None:
            raise Exception('drmsParams', 'Unable to locate DRMS parameters file (drmsparams.py).')
            
        parser = CmdlParser(usage='%(prog)s [ -h ] [ sutable=<storage unit table> ] [ reqtable=<request table> ] [ --dbname=<db name> ] [ --dbhost=<db host> ] [ --dbport=<db port> ] [ --binpath=<executable path> ] [ --logfile=<base log-file name> ]')
    
        # Optional parameters - no default argument is provided, so the default is None, which will trigger the use of what exists in the configuration file
        # (which is drmsparams.py).
        parser.add_argument('r', '--reqtable', help='The database table that contains records of the SU-request being processed. If provided, overrides default specified in configuration file.', metavar='<request unit table>', dest='reqtable', default=sumsDrmsParams.get('RS_REQUEST_TABLE'))
        parser.add_argument('s', '--sutable', help='The database table that contains records of the storage units being processed. If provided, overrides default specified in configuration file.', metavar='<storage unit table>', dest='sutable', default=sumsDrmsParams.get('RS_SU_TABLE'))
        parser.add_argument('n', '--nworkers', help='The number of scp worker threads.', metavar='<number of worker threads>', dest='nWorkers', default=sumsDrmsParams.get('RS_N_WORKERS'))
        parser.add_argument('t', '--tmpdir', help='The temporary directory to use for scp downloads.', metavar='<temporary directory>', dest='tmpdir', default=sumsDrmsParams.get('RS_TMPDIR'))
        parser.add_argument('-N', '--dbname', help='The name of the database that contains the series table from which records are to be deleted.', metavar='<db name>', dest='dbname', default=sumsDrmsParams.get('RS_DBNAME'))
        parser.add_argument('-U', '--dbuser', help='The name of the database user account.', metavar='<db user>', dest='dbuser', default=sumsDrmsParams.get('RS_DBUSER'))
        parser.add_argument('-H', '--dbhost', help='The host machine of the database that contains the series table from which records are to be deleted.', metavar='<db host machine>', dest='dbhost', default=sumsDrmsParams.get('RS_DBHOST'))
        parser.add_argument('-P', '--dbport', help='The port on the host machine that is accepting connections for the database that contains the series table from which records are to be deleted.', metavar='<db host port>', dest='dbport', default=int(sumsDrmsParams.get('RS_DBPORT')))
        parser.add_argument('-b', '--binpath', help='The path to executables run by this daemon (e.g., vso_sum_alloc, vso_sum_put).', metavar='<executable path>', dest='binpath', default=sumsDrmsParams.get('RS_BINPATH'))
        parser.add_argument('-l', '--logfile', help='The file to which logging is written.', metavar='<file name>', dest='logfile', default=os.path.join(sumsDrmsParams.get('RS_LOGDIR'), LOG_FILE_BASE_NAME + '_' + datetime.now().strftime('%Y%m%d') + '.txt'))
        
                
        arguments = Arguments(parser)
        
        arguments.setArg('lockfile', sumsDrmsParams.get('RS_LOCKFILE'))
        arguments.setArg('dltimeout', int(sumsDrmsParams.get('RS_DLTIMEOUT')))
        arguments.setArg('reqtimeout', int(sumsDrmsParams.get('RS_REQTIMEOUT')))
        arguments.setArg('maxthreads', int(sumsDrmsParams.get('RS_MAXTHREADS')))
        arguments.setArg('logdir', sumsDrmsParams.get('RS_LOGDIR'))
        
        arguments.setArg('sumsdbname', sumsDrmsParams.get('DBNAME') + '_sums')
        arguments.setArg('sumsdbuser', sumsDrmsParams.get('SUMS_MANAGER'))
        arguments.setArg('sumsdbhost', sumsDrmsParams.get('SUMS_DB_HOST'))
        arguments.setArg('sumsdbport', int(sumsDrmsParams.get('SUMPGPORT')))
        
        arguments.setArg('tapesysexists', int(sumsDrmsParams.get('SUMS_TAPE_AVAILABLE')) == 1)
        
        pid = os.getpid()
            
        with DrmsLock(arguments.lockfile, str(pid)) as lock:
            rslog = Log(arguments.logfile)

            rslog.write([ 'Starting up daemon.' ]) 
            rslog.write([ 'Obtained script file lock.' ])
            # Connect to the database
            try:
                # The connection is NOT in autocommit mode. If changes need to be saved, then conn.commit() must be called.
                with psycopg2.connect(database=arguments.dbname, user=arguments.dbuser, host=arguments.dbhost, port=arguments.dbport) as conn:
                    rslog.write(['Connected to database ' + arguments.dbname + ' on ' + arguments.dbhost + ':' + str(arguments.dbport) + ' as user ' + arguments.dbuser])
                    with conn.cursor() as cursor:
                        suTable = arguments.sutable
                        reqTable = arguments.reqtable

                        sus = None
                        requests = None
                        sites = None

                        # Read the storage-unit and request tables. Do this only once per daemon run. However, we save this table
                        # information every iteration of the daemon loop, just in case a crash happens. After a crash, when
                        # the daemon starts up, it will retrieve the latest saved information so the disruption will be minimal.
                        # We will have to clean up any pending downloads since the threads managing those downloads will have
                        # been lost, and we cannot trust that the downloads completed successfully (although they might have).
                        # A fancier implementation would be some kind of download manager that can recover partially downloaded
                        # storage units, but who has the time :)
                        rslog.write([ 'Setting download-timeout to ' + str(arguments.dltimeout) + ' seconds.' ])
                        sus = SuTable(suTable, timedelta(seconds=arguments.dltimeout), rslog)
                        rslog.write([ 'Setting request-timeout to ' + str(arguments.reqtimeout) + ' minutes.' ])
                        requests = ReqTable(reqTable, timedelta(minutes=arguments.reqtimeout), rslog)
                        sites = SiteTable(rslog)
                        
                        SuTable.setCursor(cursor)
                        ReqTable.setCursor(cursor)

                        # This function will try to read each table 10 times before giving up (and raising an exception).
                        readTables(sus, requests, sites)

                        # Set max number of threads we can process at once.
                        Downloader.setMaxThreads(arguments.maxthreads)

                        # Recover pending downloads that got disrupted from a daemon crash. All SU downloads that are in the pending
                        # state at the time the tables are read were disrupted. There are no other threads running at this point.
                        susPending = sus.getPending()
                        
                        siteSunums = {}
                        for asu in susPending:
                            timeNow = datetime.now(asu.starttime.tzinfo)
                            if timeNow > asu.starttime + sus.getTimeout():
                                # Set SU status to 'E'.
                                rslog.write(['Download of SUNUM ' + str(asu.sunum) + ' timed-out.'])
                                sus.setStatus([ asu.sunum ], 'E', 'Download timed-out.')
                                continue

                            rslog.write(['Recovering interrupted download for SUNUM ' + str(asu.sunum) + '.'])
                            siteURL = sites.getURL(asu.sunum)
                            
                            if siteURL not in siteSunums:
                                siteSunums[siteURL] = []

                            siteSunums[siteURL].append(asu.sunum)

                        sus.updateDB()

                        # There is no need to acquire the SU-table lock. processSUs() will start new threads that can modify the
                        # SU-record statuses, but by the time that happens, the main thread will be done reading those statuses.
                        for url in siteSunums.iterkeys():
                            if len(siteSunums[url]) > 0:
                                # Chunk is a list of SUNUMs (up to 64 of them).
                                siteSunums[url].sort()
                                chunker = Chunker(siteSunums[url], 64)
                                for chunk in chunker:
                                    # processSUs(..., reprocess=False) would attempt to insert a new record in the sus table for each SUNUM. By
                                    # setting the last reprocess argument to True, we do not insert a new record, but instead continue to use
                                    # the existing record. 
                                    try:
                                        processSUs(url, chunk, sus, requests, None, arguments.dbuser, arguments.binpath, arguments, rslog, True)

                                    except Exception as exc:
                                        if len(exc.args) != 2:
                                            raise # Re-raise

                                        etype = exc.args[0]
                                        msg = exc.args[1]

                                        # Do not die - just reject reprocess attempt of the request. Eventually, the downloads will time-out, and the 
                                        # status of the request will be marked 'E'.
                                        rslog.write(['Failed to reprocess SUs ' + ','.join([ str(asunum) for asunum in chunk ]) + '.'])
                    
                        # I think this is how you make it possible to pass arguments to your signal handler - define the function
                        # in the scope where the variables you want to use are visible. terminator is a closure where
                        # requests, sus, and cursor are defined.
                        shutDown = False
                        def terminator(*args):
                            global shutDown
                            global rslog
                            
                            rslog.write(['Termination signal handler called. Saving the db-table caches.'])
                            shutDown = True

                        signal.signal(signal.SIGINT, terminator)
                        signal.signal(signal.SIGTERM, terminator)

                        # Make N threads that handle scp commands. Each of the worker threads will use one of these threads to perform
                        # the actual scp command.
                        for iter in range(1, arguments.nWorkers):
                            ScpWorker.lock.acquire()
                            try:
                                if len(ScpWorker.tList) < ScpWorker.maxThreads:
                                    self.log.write([ 'Instantiating ScpWorker ' + str(iter) + '.' ])
                                    ScpWorker.newThread(asunum, path, series, retention, self.suTable, dlInfo['scpUser'], dlInfo['scpHost'], dlInfo['scpPort'], self.binPath, self.arguments, self.log)
                                else:
                                    break # The finally clause will ensure the ScpWorker lock is released.
                            finally:
                                ScpWorker.lock.release()                        
                        
                        # Start of main loop.
                        loopN = 0
                        while True and not shutDown:
                            gotLock = False
                            try:
                                # Always lock the SU table first and do all processing that requires this lock first.
                                gotLock = sus.acquireLock()
                                rslog.write([ '(Main) Class Downloader acquired SU-table lock.' ])

                                # For each P SU in the SU table, see if it is time to time-out. susPending are ordered by SUNUM.
                                # I guess we could process more than one SuTable, but for now, let's assume there is only one such
                                # table. The Downloader thread normally handles time-outs, but if the thread croaks and leaves 
                                # the SU pending, then the following code handles the time-out.
                                susPending = sus.getPending()
                                for asu in susPending:
                                    timeNow = datetime.now(asu.starttime.tzinfo)

                                    if timeNow > asu.starttime + sus.getTimeout():
                                        rslog.write(['Download of SUNUM ' + str(asu.sunum) + ' timed-out.'])
                                        # Kill the Downloader thread (if it exists).
                                        sus.stopWorker(asu.sunum)
                                        sus.setStatus([ asu.sunum ], 'E', 'Download timed-out.')
                           
                                try: 
                                    cursor.execute('BEGIN')
                                    sus.updateDB()
                                    cursor.execute('END')
                                except psycopg2.Error as exc:
                                    # Handle database-command errors.
                                    raise Exception('dbUpdate', exc.diag.message_primary)

                                # Ignore SUs in the other states (C or E). These will be checked in other parts of the code.
                                
                                # For each 'P' request in the request table, check to see if the requested downloads have completed yet.

                                # Log every 5 seconds.
                                timeToLog = (loopN % 5 == 0)

                                reqsPending = requests.getPending()
                                for arequest in reqsPending:
                                    done = True
                                    reqError = False
                                    sunums = arequest['sunums']
                                    errMsg = ''
                                    processing = {}

                                    for asunum in sunums:
                                        asu = sus.get([ asunum ])[0]

                                        if str(asunum) in processing:
                                            # Skip duplicates.
                                            rslog.write(['Skipping pending request for SU ' + str(asunum) + ' - this is a duplicate SU.'])
                                            continue
                                        else:
                                            processing[str(asunum)] = True                                        
                                        
                                        if asu.status == 'P':
                                            rslog.write(['Download of SU ' + str(asu.sunum)  + ' is pending.'])
                                            done = False
                                        elif asu.status == 'E':
                                            rslog.write(['Download of SU ' + str(asu.sunum)  + ' has errored-out.'])
                                            errMsg = asu.errmsg
                                            reqError = True
                                        else:
                                            rslog.write(['Download of SU ' + str(asu.sunum)  + ' has completed.'])
                                    
                                    if done:
                                        # There are no pending downloads for this request. Set this request's status to 'C' or 'E', and decrement
                                        # refcount on each SU.
                                        if reqError:
                                            rslog.write(['Request number ' + str(arequest['requestid']) + ' for SUNUM(s) ' + ','.join([ str(asunum) for asunum in arequest['sunums'] ]) + ' errored-out.'])
                                            requests.setStatus([ arequest['requestid'] ], 'E', errMsg)
                                        else:
                                            rslog.write(['Request number ' + str(arequest['requestid']) + ' for SUNUM(s) ' + ','.join([ str(asunum) for asunum in arequest['sunums'] ]) + ' completed successfully.'])
                                            requests.setStatus([ arequest['requestid'] ], 'C')

                                        # These next two calls can modify the db state! Put them in a transaction so that they form an atomic
                                        # operation. We do not want an interruption to cause the first to happen, but not the second.
                                        try:
                                            cursor.execute('BEGIN')

                                            # Update the statuses in the requests db table.
                                            requests.updateDB([arequest['requestid']])
                                            
                                            # Remove duplicates from list first. We do not need to preserve the order of the SUNUMs
                                            # before calling decrementRefcount() since that function uses a hash lookup on the SUNUM
                                            # to find the associated refcount. decrementRefcount() does not modify the SU db table.
                                            # Call updateDB() to do that.
                                            sus.decrementRefcount(list(set(sunums)))
                                            sus.updateDB()

                                            cursor.execute('END') # Same as cursor.execute('COMMIT')
                                        except psycopg2.Error as exc:
                                            # Handle database-command errors.
                                            raise Exception('dbUpdate', exc.diag.message_primary)
                                            
                                # Right here is where we can find orphaned sus records and delete them. We have a list of all reachable SUs now that we've 
                                # iterated through the pending requests. We now iterate through ALL sus records and delete any that are not reachable.
                                # xxx
                
                                # For each 'N' request in the request table, start a new set of downloads (if there is no download currently running -
                                # i.e., no SU record) or increment the refcounts on the downloads (if there are downloads currently running - i.e.,
                                # an SU record exists). But Before starting a new download, make sure that requested SU is not already online.
                                # Due to race conditions, a request could have caused a download to occur needed by another request whose state is 'N'.
                                requests.refresh() # Clients may have added requests to the queue.
                                reqsNew = requests.getNew()

                                for arequest in reqsNew:
                                    timeNow = datetime.now(arequest['starttime'].tzinfo)
                                    if timeNow > arequest['starttime'] + requests.getTimeout():
                                        rslog.write(['Request number ' + str(arequest['requestid']) + ' timed-out.'])
                                        requests.setStatus([arequest['requestid']], 'E', 'Request timed-out.')
                                        try:
                                            cursor.execute('BEGIN')
                                            requests.updateDB([arequest['requestid']])
                                            cursor.execute('END')
                                        except psycopg2.Error as exc:
                                            # Handle database-command errors.
                                            raise Exception('dbUpdate', exc.diag.message_primary)
                                        
                                        continue
                                    
                                    sunums = arequest['sunums']
                                    rslog.write(['Found a new download request, id ' + str(arequest['requestid']) + ' for SUNUMs ' + ','.join([str(asunum) for asunum in sunums]) + '.'])
                                    
                                    # Get all SU records for which a download is already in progress.
                                    unknown = [] # An SU that is not being processed
                                    known = [] # An SU that is being processed.
                                    processing = {} # The SUs in sunums that are currently being processed. Use this to avoid duplicate SUs.
                                    skipRequest = False

                                    for asunum in sunums:
                                        if str(asunum) in processing:
                                            # Skip duplicates.
                                            rslog.write(['Skipping request for SU ' + str(asunum) + ' - this is a duplicate SU.'])
                                            continue
                                        else:
                                            processing[str(asunum)] = True
                                            
                                        try:
                                            asu = sus.get([ asunum ])[0] # Will raise if asunum is unknown.
                                            
                                            # If we get here, then the SU is already being processed as part of a previous request.
                                            # If the SU is in the 'E' or 'C' state, then we cannot process this request. We must wait until
                                            # this SU has been cleared out of the sus table when the pending requests are processed.
                                            if asu.status != 'P':
                                                rslog.write(['Deferring request, id ' + str(arequest['requestid']) + '. At least one previous request for this SU must be completed first.'])
                                                skipRequest = True
                                                break
                                            
                                            rslog.write(['A download for SU ' + str(asunum)+ ' is already in progress.'])
                                            known.append(asunum)
                                        except Exception as exc:
                                            if len(exc.args) != 2:
                                                raise # Re-raise
                                                
                                            etype = exc.args[0]
                                            msg = exc.args[1]
                    
                                            if etype == 'unknownSunum':
                                                unknown.append(asunum)
                                                
                                    if skipRequest:
                                        continue
 
                                    # Increment the refcount on all SU records for the SUs being requested by the new request. This modifies the
                                    # sus object.
                                    sus.incrementRefcount(known)
                                    
                                    offlineSunums = SuTable.offline(unknown, arguments.binpath, rslog)
                                    offlineSunumsDict = dict([ (str(asunum), True) for asunum in offlineSunums ])
                                    
                                    dlsToStart = []
                                    toComplete = []
                                    for asunum in unknown:
                                        if str(asunum) in offlineSunumsDict:
                                            rslog.write(['SU ' + str(asunum) + ' is offline - will start a download'])
                                            dlsToStart.append(asunum)
                                        else:
                                            rslog.write(['SU ' + str(asunum) + ' is online already - will NOT start a download.'])
                                            toComplete.append(asunum)
                                    
                                    # Insert a new SU record for all unknown SUs that are already online. These calls modify the sus object.
                                    sus.insert(toComplete)
                                    sus.setStatus(toComplete, 'C')
                                        
                                    # Start downloads for all unknown, offline SUs
                                    siteSunums = {}
                                    for asunum in dlsToStart:
                                        siteURL = sites.getURL(asunum)
                                        
                                        if siteURL not in siteSunums:
                                            siteSunums[siteURL] = []
                                        
                                        siteSunums[siteURL].append(asunum)
                                        
                                    for url in siteSunums.iterkeys():
                                        if len(siteSunums[url]) > 0:
                                            # Chunk is a list of SUNUMs (up to 64 of them).
                                            siteSunums[url].sort()
                                            chunker = Chunker(siteSunums[url], 64)
                                            for chunk in chunker:
                                                # We want to always insert a record for each SU into the SU table. Do not provide the insertRec
                                                # argument to do so. This call creates new SU-table records, so it modifies the sus object.
                                                processSUs(url, chunk, sus, requests, arequest, arguments.dbuser, arguments.binpath, arguments, rslog)
                                    
                                    # The new request has been fully processed. Change its status from 'N' to 'P'.
                                    # This call modifies the requests object.
                                    requests.setStatus([arequest['requestid']], 'P')

                                    # At this point, both the requests and sus object have been modified, but have not been flushed to disk.
                                    # Flush them, but do this inside a transaction so that the first does not happen without the second.
                                    try:
                                        cursor.execute('BEGIN')
                                        sus.updateDB(sunums)
                                        requests.updateDB([arequest['requestid']])
                                        cursor.execute('END')
                                    except psycopg2.Error as exc:
                                        # Handle database-command errors.
                                        raise Exception('dbUpdate', exc.diag.message_primary)

                                # Delete all request-table records whose state is 'D'. It doesn't matter if this operation gets interrupted. If
                                # that happens, then these delete-pending records will be deleted the next time this code runs uninterrupted.
                                reqsToDelete = requests.getDelete()
                                requests.deleteDB(reqsToDelete)
                            finally:
                                # Always release the lock, even if an unhandled exception crops up.
                                if gotLock:
                                    sus.releaseLock()
                                    rslog.write([ '(Main) Class Downloader released SU-table lock.' ])
                           
                            # Must poll for new requests to appear in requests table.
                            time.sleep(1)
                            loopN += 1
                            # I wonder if Python throws an exception if loopN rolls-over. Does it roll-over at the 32-bit or 64-bit boundary? 
                            if loopN >= 0x7FFFFFFF:
                                loopN = 0
                            # End of main loop.
                        
                        # Save the db state when exiting.
                        rslog.write(['Remote-sums daemon is exiting. Saving database tables.'])
                        try:
                            cursor.execute('BEGIN')
                            sus.updateDB()
                            requests.updateDB()
                            cursor.execute('END')
                        except psycopg2.Error as exc:
                            # Handle database-command errors.
                            raise Exception('dbUpdate', exc.diag.message_primary)

            except psycopg2.DatabaseError as exc:
                # Closes the cursor and connection
                
                # Man, there is no way to get an error message from any exception object that will provide any information why
                # the connection failed.
                print('Unable to connect to the database (no, I do not know why).', file=sys.stderr)
    
                # No need to close cursor - leaving the with block does that.
                rv = RET_DBCONNECT

            except psycopg2.Error as exc:
                # Handle database-command errors.
                print(exc.diag.message_primary, file=sys.stderr)
                rv = RET_DBCOMMAND

        # Lock was released
    except Exception as exc:
        if len(exc.args) != 2:
            raise # Re-raise
        
        etype = exc.args[0]
        msg = exc.args[1]
       
        if etype == 'drmsLock':
            rslog.write(['Error locking file: ' + lockFile + '\n' + msg])
            rv = RET_LOCK
        elif etype == 'CmdlParser-ArgUnrecognized' or etype == 'CmdlParser-ArgBadformat':
            rslog.write([msg])
            rv = RET_INVALIDARGS
        elif etype == 'invalidArg':
            rslog.write([msg])
            rv = RET_INVALIDARGS
        elif etype == 'sitetableRead':
            rslog.write(['Unable to load site table: ' + msg])
            rv = RET_SITETABLE_LOAD
        elif etype == 'sutableRead':
            rslog.write(['Unable to read from storage-unit table: ' + msg])
            rv = RET_SUTABLE_READ
        elif etype == 'sutableWrite':
            rslog.write(['Unable to write to storage-unit table: ' + msg])
            rv = RET_SUTABLE_WRITE
        elif etype == 'reqtableRead':
            rslog.write(['Unable to read from storage-unit-request table: ' + msg])
            rv = RET_REQTABLE_READ
        elif etype == 'reqtableWrite':
            rslog.write(['Unable to write to storage-unit-request table: ' + msg])
            rv = RET_REQTABLE_WRITE
        elif etype == 'badLogfile':
            print('Cannot access log file: ' + msg, file=sys.stderr)
            rv = RET_LOGFILE
        elif etype == 'badLogwrite':
            print('Cannot access log file: ' + msg, file=sys.stderr)
            rv = RET_LOGFILE
        elif etype == 'findOffline':
            rslog.write(['Cannot determine online disposition: ' + msg])
            rv = RET_OFFLINE
        elif etype == 'getRetention':
            rslog.write(['Cannot obtain retention value: ' + msg])
            rv = RET_GETRETENTION
        elif etype == 'unknownRequestid':
            rslog.write(['Oops! ' + msg])
            rv = RET_UKNOWNREQUEST
        elif etype == 'unknownSunum':
            rslog.write(['Oops! ' + msg])
            rv = RET_UKNOWNSU
        elif etype == 'workerRef':
            rslog.write([ msg ])
            rv = RET_WORKERREF
        elif etype == 'noReference':
            rslog.write([msg])
            rv = RET_UKNOWNSU
        elif etype == 'unknownSitecode':
            rslog.write([msg])
            rv = RET_UKNOWNSITECODE
        elif etype == 'knownSunum':
            rslog.write([msg])
            rv = RET_DUPLICATESUNUM
        elif etype == 'dbUpdate':
            rslog.write([msg])
            rv = RET_DBUPDATE
        elif etype == 'sumsAPI':
            rslog.write([msg])
            rv = RET_SUMS
        else:
            rslog.write(['Unhandled exception. Remote-sums daemon is exiting. Rolling back uncommitted database changes. '])
            raise # Re-raise

if rslog:
    rslog.write(['Exiting with return status ' + str(rv)])
# Will not exit process if threads are still running. Set global shutdown flag that threads are monitoring. When they see the flag, they
# will terminate too.
gShutDown = True
sys.exit(rv)
